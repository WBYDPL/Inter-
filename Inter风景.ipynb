{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "num_classes = 4\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置训练集测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((200,200)),\n",
    "#         transforms.RandomSizedCrop(180),\n",
    "        transforms.RandomResizedCrop(180, (0.8, 1)),\n",
    "\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "train_dataset = datasets.ImageFolder(root='data/aaa/train',\n",
    "                                           transform=data_transform)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='data/aaa/test',\n",
    "                                           transform=data_transform)\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(6272, 200)\n",
    "        self.fc2 = nn.Linear(200, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "model = ConvNet(num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/147], Loss: 1.3821\n",
      "Epoch [1/10], Step [2/147], Loss: 1.3829\n",
      "Epoch [1/10], Step [3/147], Loss: 1.3868\n",
      "Epoch [1/10], Step [4/147], Loss: 1.3695\n",
      "Epoch [1/10], Step [5/147], Loss: 1.3568\n",
      "Epoch [1/10], Step [6/147], Loss: 1.3365\n",
      "Epoch [1/10], Step [7/147], Loss: 1.3175\n",
      "Epoch [1/10], Step [8/147], Loss: 1.2965\n",
      "Epoch [1/10], Step [9/147], Loss: 1.3079\n",
      "Epoch [1/10], Step [10/147], Loss: 1.2699\n",
      "Epoch [1/10], Step [11/147], Loss: 1.2022\n",
      "Epoch [1/10], Step [12/147], Loss: 1.0951\n",
      "Epoch [1/10], Step [13/147], Loss: 1.1553\n",
      "Epoch [1/10], Step [14/147], Loss: 0.9799\n",
      "Epoch [1/10], Step [15/147], Loss: 0.9511\n",
      "Epoch [1/10], Step [16/147], Loss: 0.9484\n",
      "Epoch [1/10], Step [17/147], Loss: 0.9020\n",
      "Epoch [1/10], Step [18/147], Loss: 0.9103\n",
      "Epoch [1/10], Step [19/147], Loss: 0.9853\n",
      "Epoch [1/10], Step [20/147], Loss: 0.8281\n",
      "Epoch [1/10], Step [21/147], Loss: 0.8759\n",
      "Epoch [1/10], Step [22/147], Loss: 0.9520\n",
      "Epoch [1/10], Step [23/147], Loss: 0.9821\n",
      "Epoch [1/10], Step [24/147], Loss: 0.9208\n",
      "Epoch [1/10], Step [25/147], Loss: 0.6562\n",
      "Epoch [1/10], Step [26/147], Loss: 0.7994\n",
      "Epoch [1/10], Step [27/147], Loss: 1.0354\n",
      "Epoch [1/10], Step [28/147], Loss: 0.6217\n",
      "Epoch [1/10], Step [29/147], Loss: 0.7044\n",
      "Epoch [1/10], Step [30/147], Loss: 0.7642\n",
      "Epoch [1/10], Step [31/147], Loss: 0.6998\n",
      "Epoch [1/10], Step [32/147], Loss: 0.6653\n",
      "Epoch [1/10], Step [33/147], Loss: 0.7478\n",
      "Epoch [1/10], Step [34/147], Loss: 0.6735\n",
      "Epoch [1/10], Step [35/147], Loss: 0.7257\n",
      "Epoch [1/10], Step [36/147], Loss: 0.8790\n",
      "Epoch [1/10], Step [37/147], Loss: 0.7647\n",
      "Epoch [1/10], Step [38/147], Loss: 0.7851\n",
      "Epoch [1/10], Step [39/147], Loss: 0.7722\n",
      "Epoch [1/10], Step [40/147], Loss: 0.7441\n",
      "Epoch [1/10], Step [41/147], Loss: 0.7478\n",
      "Epoch [1/10], Step [42/147], Loss: 0.7544\n",
      "Epoch [1/10], Step [43/147], Loss: 0.6697\n",
      "Epoch [1/10], Step [44/147], Loss: 0.6498\n",
      "Epoch [1/10], Step [45/147], Loss: 0.6621\n",
      "Epoch [1/10], Step [46/147], Loss: 0.8297\n",
      "Epoch [1/10], Step [47/147], Loss: 0.5253\n",
      "Epoch [1/10], Step [48/147], Loss: 0.7897\n",
      "Epoch [1/10], Step [49/147], Loss: 0.5344\n",
      "Epoch [1/10], Step [50/147], Loss: 0.7235\n",
      "Epoch [1/10], Step [51/147], Loss: 0.6401\n",
      "Epoch [1/10], Step [52/147], Loss: 0.7902\n",
      "Epoch [1/10], Step [53/147], Loss: 0.7336\n",
      "Epoch [1/10], Step [54/147], Loss: 0.8110\n",
      "Epoch [1/10], Step [55/147], Loss: 0.7024\n",
      "Epoch [1/10], Step [56/147], Loss: 0.7672\n",
      "Epoch [1/10], Step [57/147], Loss: 0.8509\n",
      "Epoch [1/10], Step [58/147], Loss: 0.7483\n",
      "Epoch [1/10], Step [59/147], Loss: 0.6625\n",
      "Epoch [1/10], Step [60/147], Loss: 0.6790\n",
      "Epoch [1/10], Step [61/147], Loss: 0.6030\n",
      "Epoch [1/10], Step [62/147], Loss: 0.6337\n",
      "Epoch [1/10], Step [63/147], Loss: 0.5570\n",
      "Epoch [1/10], Step [64/147], Loss: 0.5644\n",
      "Epoch [1/10], Step [65/147], Loss: 0.7460\n",
      "Epoch [1/10], Step [66/147], Loss: 0.5596\n",
      "Epoch [1/10], Step [67/147], Loss: 0.7859\n",
      "Epoch [1/10], Step [68/147], Loss: 0.6351\n",
      "Epoch [1/10], Step [69/147], Loss: 0.6615\n",
      "Epoch [1/10], Step [70/147], Loss: 0.5576\n",
      "Epoch [1/10], Step [71/147], Loss: 0.6154\n",
      "Epoch [1/10], Step [72/147], Loss: 0.7403\n",
      "Epoch [1/10], Step [73/147], Loss: 0.6347\n",
      "Epoch [1/10], Step [74/147], Loss: 0.6978\n",
      "Epoch [1/10], Step [75/147], Loss: 0.7515\n",
      "Epoch [1/10], Step [76/147], Loss: 0.6394\n",
      "Epoch [1/10], Step [77/147], Loss: 0.8169\n",
      "Epoch [1/10], Step [78/147], Loss: 0.7717\n",
      "Epoch [1/10], Step [79/147], Loss: 0.7388\n",
      "Epoch [1/10], Step [80/147], Loss: 0.6275\n",
      "Epoch [1/10], Step [81/147], Loss: 0.6790\n",
      "Epoch [1/10], Step [82/147], Loss: 0.9888\n",
      "Epoch [1/10], Step [83/147], Loss: 0.5298\n",
      "Epoch [1/10], Step [84/147], Loss: 0.4642\n",
      "Epoch [1/10], Step [85/147], Loss: 0.8307\n",
      "Epoch [1/10], Step [86/147], Loss: 0.6383\n",
      "Epoch [1/10], Step [87/147], Loss: 0.5924\n",
      "Epoch [1/10], Step [88/147], Loss: 0.6892\n",
      "Epoch [1/10], Step [89/147], Loss: 0.7093\n",
      "Epoch [1/10], Step [90/147], Loss: 0.5162\n",
      "Epoch [1/10], Step [91/147], Loss: 0.5309\n",
      "Epoch [1/10], Step [92/147], Loss: 0.4592\n",
      "Epoch [1/10], Step [93/147], Loss: 0.5806\n",
      "Epoch [1/10], Step [94/147], Loss: 0.7384\n",
      "Epoch [1/10], Step [95/147], Loss: 0.6968\n",
      "Epoch [1/10], Step [96/147], Loss: 0.5554\n",
      "Epoch [1/10], Step [97/147], Loss: 0.5696\n",
      "Epoch [1/10], Step [98/147], Loss: 0.5020\n",
      "Epoch [1/10], Step [99/147], Loss: 0.6376\n",
      "Epoch [1/10], Step [100/147], Loss: 0.5186\n",
      "Epoch [1/10], Step [101/147], Loss: 0.6099\n",
      "Epoch [1/10], Step [102/147], Loss: 0.5658\n",
      "Epoch [1/10], Step [103/147], Loss: 0.5986\n",
      "Epoch [1/10], Step [104/147], Loss: 0.5698\n",
      "Epoch [1/10], Step [105/147], Loss: 0.7866\n",
      "Epoch [1/10], Step [106/147], Loss: 0.7188\n",
      "Epoch [1/10], Step [107/147], Loss: 0.7062\n",
      "Epoch [1/10], Step [108/147], Loss: 0.6868\n",
      "Epoch [1/10], Step [109/147], Loss: 0.7622\n",
      "Epoch [1/10], Step [110/147], Loss: 0.6696\n",
      "Epoch [1/10], Step [111/147], Loss: 0.6134\n",
      "Epoch [1/10], Step [112/147], Loss: 0.6766\n",
      "Epoch [1/10], Step [113/147], Loss: 0.7370\n",
      "Epoch [1/10], Step [114/147], Loss: 0.7639\n",
      "Epoch [1/10], Step [115/147], Loss: 0.6163\n",
      "Epoch [1/10], Step [116/147], Loss: 0.5548\n",
      "Epoch [1/10], Step [117/147], Loss: 0.5688\n",
      "Epoch [1/10], Step [118/147], Loss: 0.5944\n",
      "Epoch [1/10], Step [119/147], Loss: 0.5413\n",
      "Epoch [1/10], Step [120/147], Loss: 0.6112\n",
      "Epoch [1/10], Step [121/147], Loss: 0.6140\n",
      "Epoch [1/10], Step [122/147], Loss: 0.6828\n",
      "Epoch [1/10], Step [123/147], Loss: 0.5958\n",
      "Epoch [1/10], Step [124/147], Loss: 0.6215\n",
      "Epoch [1/10], Step [125/147], Loss: 0.5432\n",
      "Epoch [1/10], Step [126/147], Loss: 0.5916\n",
      "Epoch [1/10], Step [127/147], Loss: 0.5360\n",
      "Epoch [1/10], Step [128/147], Loss: 0.5902\n",
      "Epoch [1/10], Step [129/147], Loss: 0.5540\n",
      "Epoch [1/10], Step [130/147], Loss: 0.5984\n",
      "Epoch [1/10], Step [131/147], Loss: 0.6364\n",
      "Epoch [1/10], Step [132/147], Loss: 0.6174\n",
      "Epoch [1/10], Step [133/147], Loss: 0.7752\n",
      "Epoch [1/10], Step [134/147], Loss: 0.4278\n",
      "Epoch [1/10], Step [135/147], Loss: 0.5338\n",
      "Epoch [1/10], Step [136/147], Loss: 0.7395\n",
      "Epoch [1/10], Step [137/147], Loss: 0.6870\n",
      "Epoch [1/10], Step [138/147], Loss: 0.5680\n",
      "Epoch [1/10], Step [139/147], Loss: 0.7895\n",
      "Epoch [1/10], Step [140/147], Loss: 0.6350\n",
      "Epoch [1/10], Step [141/147], Loss: 0.4063\n",
      "Epoch [1/10], Step [142/147], Loss: 0.6174\n",
      "Epoch [1/10], Step [143/147], Loss: 0.4392\n",
      "Epoch [1/10], Step [144/147], Loss: 0.6405\n",
      "Epoch [1/10], Step [145/147], Loss: 0.6533\n",
      "Epoch [1/10], Step [146/147], Loss: 0.4694\n",
      "Epoch [1/10], Step [147/147], Loss: 0.6734\n",
      "Epoch [2/10], Step [1/147], Loss: 0.4619\n",
      "Epoch [2/10], Step [2/147], Loss: 0.4068\n",
      "Epoch [2/10], Step [3/147], Loss: 0.4952\n",
      "Epoch [2/10], Step [4/147], Loss: 0.5775\n",
      "Epoch [2/10], Step [5/147], Loss: 0.4253\n",
      "Epoch [2/10], Step [6/147], Loss: 0.5846\n",
      "Epoch [2/10], Step [7/147], Loss: 0.7304\n",
      "Epoch [2/10], Step [8/147], Loss: 0.5554\n",
      "Epoch [2/10], Step [9/147], Loss: 0.5824\n",
      "Epoch [2/10], Step [10/147], Loss: 0.4843\n",
      "Epoch [2/10], Step [11/147], Loss: 0.6394\n",
      "Epoch [2/10], Step [12/147], Loss: 0.7819\n",
      "Epoch [2/10], Step [13/147], Loss: 0.7258\n",
      "Epoch [2/10], Step [14/147], Loss: 0.6900\n",
      "Epoch [2/10], Step [15/147], Loss: 0.3015\n",
      "Epoch [2/10], Step [16/147], Loss: 0.7744\n",
      "Epoch [2/10], Step [17/147], Loss: 0.4525\n",
      "Epoch [2/10], Step [18/147], Loss: 0.4928\n",
      "Epoch [2/10], Step [19/147], Loss: 0.5683\n",
      "Epoch [2/10], Step [20/147], Loss: 0.6996\n",
      "Epoch [2/10], Step [21/147], Loss: 0.4519\n",
      "Epoch [2/10], Step [22/147], Loss: 0.5795\n",
      "Epoch [2/10], Step [23/147], Loss: 0.6018\n",
      "Epoch [2/10], Step [24/147], Loss: 0.4980\n",
      "Epoch [2/10], Step [25/147], Loss: 0.5010\n",
      "Epoch [2/10], Step [26/147], Loss: 0.5237\n",
      "Epoch [2/10], Step [27/147], Loss: 0.4530\n",
      "Epoch [2/10], Step [28/147], Loss: 0.4475\n",
      "Epoch [2/10], Step [29/147], Loss: 0.5476\n",
      "Epoch [2/10], Step [30/147], Loss: 0.4549\n",
      "Epoch [2/10], Step [31/147], Loss: 0.5901\n",
      "Epoch [2/10], Step [32/147], Loss: 0.4228\n",
      "Epoch [2/10], Step [33/147], Loss: 0.5480\n",
      "Epoch [2/10], Step [34/147], Loss: 0.4292\n",
      "Epoch [2/10], Step [35/147], Loss: 0.3599\n",
      "Epoch [2/10], Step [36/147], Loss: 0.5391\n",
      "Epoch [2/10], Step [37/147], Loss: 0.4121\n",
      "Epoch [2/10], Step [38/147], Loss: 0.6148\n",
      "Epoch [2/10], Step [39/147], Loss: 0.4589\n",
      "Epoch [2/10], Step [40/147], Loss: 0.3371\n",
      "Epoch [2/10], Step [41/147], Loss: 0.5931\n",
      "Epoch [2/10], Step [42/147], Loss: 0.5844\n",
      "Epoch [2/10], Step [43/147], Loss: 0.5798\n",
      "Epoch [2/10], Step [44/147], Loss: 0.5934\n",
      "Epoch [2/10], Step [45/147], Loss: 0.5193\n",
      "Epoch [2/10], Step [46/147], Loss: 0.3771\n",
      "Epoch [2/10], Step [47/147], Loss: 0.4563\n",
      "Epoch [2/10], Step [48/147], Loss: 0.5489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [49/147], Loss: 0.5762\n",
      "Epoch [2/10], Step [50/147], Loss: 0.5836\n",
      "Epoch [2/10], Step [51/147], Loss: 0.4796\n",
      "Epoch [2/10], Step [52/147], Loss: 0.4322\n",
      "Epoch [2/10], Step [53/147], Loss: 0.3839\n",
      "Epoch [2/10], Step [54/147], Loss: 0.5230\n",
      "Epoch [2/10], Step [55/147], Loss: 0.4655\n",
      "Epoch [2/10], Step [56/147], Loss: 0.4799\n",
      "Epoch [2/10], Step [57/147], Loss: 0.5141\n",
      "Epoch [2/10], Step [58/147], Loss: 0.4507\n",
      "Epoch [2/10], Step [59/147], Loss: 0.5583\n",
      "Epoch [2/10], Step [60/147], Loss: 0.4686\n",
      "Epoch [2/10], Step [61/147], Loss: 0.3466\n",
      "Epoch [2/10], Step [62/147], Loss: 0.5462\n",
      "Epoch [2/10], Step [63/147], Loss: 0.6877\n",
      "Epoch [2/10], Step [64/147], Loss: 0.5621\n",
      "Epoch [2/10], Step [65/147], Loss: 0.6398\n",
      "Epoch [2/10], Step [66/147], Loss: 0.3784\n",
      "Epoch [2/10], Step [67/147], Loss: 0.5660\n",
      "Epoch [2/10], Step [68/147], Loss: 0.3747\n",
      "Epoch [2/10], Step [69/147], Loss: 0.5787\n",
      "Epoch [2/10], Step [70/147], Loss: 0.6216\n",
      "Epoch [2/10], Step [71/147], Loss: 0.5295\n",
      "Epoch [2/10], Step [72/147], Loss: 0.5322\n",
      "Epoch [2/10], Step [73/147], Loss: 0.6600\n",
      "Epoch [2/10], Step [74/147], Loss: 0.4229\n",
      "Epoch [2/10], Step [75/147], Loss: 0.5852\n",
      "Epoch [2/10], Step [76/147], Loss: 0.2908\n",
      "Epoch [2/10], Step [77/147], Loss: 0.5162\n",
      "Epoch [2/10], Step [78/147], Loss: 0.8432\n",
      "Epoch [2/10], Step [79/147], Loss: 0.5839\n",
      "Epoch [2/10], Step [80/147], Loss: 0.5966\n",
      "Epoch [2/10], Step [81/147], Loss: 0.6758\n",
      "Epoch [2/10], Step [82/147], Loss: 0.4139\n",
      "Epoch [2/10], Step [83/147], Loss: 0.6499\n",
      "Epoch [2/10], Step [84/147], Loss: 0.4813\n",
      "Epoch [2/10], Step [85/147], Loss: 0.5780\n",
      "Epoch [2/10], Step [86/147], Loss: 0.4829\n",
      "Epoch [2/10], Step [87/147], Loss: 0.5261\n",
      "Epoch [2/10], Step [88/147], Loss: 0.5002\n",
      "Epoch [2/10], Step [89/147], Loss: 0.4196\n",
      "Epoch [2/10], Step [90/147], Loss: 0.5021\n",
      "Epoch [2/10], Step [91/147], Loss: 0.5852\n",
      "Epoch [2/10], Step [92/147], Loss: 0.6934\n",
      "Epoch [2/10], Step [93/147], Loss: 0.5638\n",
      "Epoch [2/10], Step [94/147], Loss: 0.4465\n",
      "Epoch [2/10], Step [95/147], Loss: 0.3216\n",
      "Epoch [2/10], Step [96/147], Loss: 0.6122\n",
      "Epoch [2/10], Step [97/147], Loss: 0.4761\n",
      "Epoch [2/10], Step [98/147], Loss: 0.6505\n",
      "Epoch [2/10], Step [99/147], Loss: 0.6571\n",
      "Epoch [2/10], Step [100/147], Loss: 0.5629\n",
      "Epoch [2/10], Step [101/147], Loss: 0.3679\n",
      "Epoch [2/10], Step [102/147], Loss: 0.4396\n",
      "Epoch [2/10], Step [103/147], Loss: 0.4124\n",
      "Epoch [2/10], Step [104/147], Loss: 0.4268\n",
      "Epoch [2/10], Step [105/147], Loss: 0.5583\n",
      "Epoch [2/10], Step [106/147], Loss: 0.3330\n",
      "Epoch [2/10], Step [107/147], Loss: 0.5604\n",
      "Epoch [2/10], Step [108/147], Loss: 0.3762\n",
      "Epoch [2/10], Step [109/147], Loss: 0.4199\n",
      "Epoch [2/10], Step [110/147], Loss: 0.3056\n",
      "Epoch [2/10], Step [111/147], Loss: 0.3128\n",
      "Epoch [2/10], Step [112/147], Loss: 0.5505\n",
      "Epoch [2/10], Step [113/147], Loss: 0.3035\n",
      "Epoch [2/10], Step [114/147], Loss: 0.5764\n",
      "Epoch [2/10], Step [115/147], Loss: 0.6336\n",
      "Epoch [2/10], Step [116/147], Loss: 0.4511\n",
      "Epoch [2/10], Step [117/147], Loss: 0.3542\n",
      "Epoch [2/10], Step [118/147], Loss: 0.3698\n",
      "Epoch [2/10], Step [119/147], Loss: 0.4282\n",
      "Epoch [2/10], Step [120/147], Loss: 0.6437\n",
      "Epoch [2/10], Step [121/147], Loss: 0.4474\n",
      "Epoch [2/10], Step [122/147], Loss: 0.5341\n",
      "Epoch [2/10], Step [123/147], Loss: 0.3994\n",
      "Epoch [2/10], Step [124/147], Loss: 0.4528\n",
      "Epoch [2/10], Step [125/147], Loss: 0.5273\n",
      "Epoch [2/10], Step [126/147], Loss: 0.5930\n",
      "Epoch [2/10], Step [127/147], Loss: 0.4260\n",
      "Epoch [2/10], Step [128/147], Loss: 0.6645\n",
      "Epoch [2/10], Step [129/147], Loss: 0.6307\n",
      "Epoch [2/10], Step [130/147], Loss: 0.3569\n",
      "Epoch [2/10], Step [131/147], Loss: 0.5806\n",
      "Epoch [2/10], Step [132/147], Loss: 0.5498\n",
      "Epoch [2/10], Step [133/147], Loss: 0.6032\n",
      "Epoch [2/10], Step [134/147], Loss: 0.6873\n",
      "Epoch [2/10], Step [135/147], Loss: 0.4182\n",
      "Epoch [2/10], Step [136/147], Loss: 0.4605\n",
      "Epoch [2/10], Step [137/147], Loss: 0.4697\n",
      "Epoch [2/10], Step [138/147], Loss: 0.4311\n",
      "Epoch [2/10], Step [139/147], Loss: 0.3936\n",
      "Epoch [2/10], Step [140/147], Loss: 0.5024\n",
      "Epoch [2/10], Step [141/147], Loss: 0.3622\n",
      "Epoch [2/10], Step [142/147], Loss: 0.3007\n",
      "Epoch [2/10], Step [143/147], Loss: 0.6461\n",
      "Epoch [2/10], Step [144/147], Loss: 0.4366\n",
      "Epoch [2/10], Step [145/147], Loss: 0.4707\n",
      "Epoch [2/10], Step [146/147], Loss: 0.3665\n",
      "Epoch [2/10], Step [147/147], Loss: 0.5973\n",
      "Epoch [3/10], Step [1/147], Loss: 0.2862\n",
      "Epoch [3/10], Step [2/147], Loss: 0.5032\n",
      "Epoch [3/10], Step [3/147], Loss: 0.3812\n",
      "Epoch [3/10], Step [4/147], Loss: 0.5511\n",
      "Epoch [3/10], Step [5/147], Loss: 0.5339\n",
      "Epoch [3/10], Step [6/147], Loss: 0.4847\n",
      "Epoch [3/10], Step [7/147], Loss: 0.4966\n",
      "Epoch [3/10], Step [8/147], Loss: 0.5221\n",
      "Epoch [3/10], Step [9/147], Loss: 0.5716\n",
      "Epoch [3/10], Step [10/147], Loss: 0.4503\n",
      "Epoch [3/10], Step [11/147], Loss: 0.5326\n",
      "Epoch [3/10], Step [12/147], Loss: 0.5226\n",
      "Epoch [3/10], Step [13/147], Loss: 0.4310\n",
      "Epoch [3/10], Step [14/147], Loss: 0.4968\n",
      "Epoch [3/10], Step [15/147], Loss: 0.3272\n",
      "Epoch [3/10], Step [16/147], Loss: 0.4624\n",
      "Epoch [3/10], Step [17/147], Loss: 0.6474\n",
      "Epoch [3/10], Step [18/147], Loss: 0.4279\n",
      "Epoch [3/10], Step [19/147], Loss: 0.4597\n",
      "Epoch [3/10], Step [20/147], Loss: 0.4262\n",
      "Epoch [3/10], Step [21/147], Loss: 0.4332\n",
      "Epoch [3/10], Step [22/147], Loss: 0.4321\n",
      "Epoch [3/10], Step [23/147], Loss: 0.2938\n",
      "Epoch [3/10], Step [24/147], Loss: 0.3721\n",
      "Epoch [3/10], Step [25/147], Loss: 0.4954\n",
      "Epoch [3/10], Step [26/147], Loss: 0.5298\n",
      "Epoch [3/10], Step [27/147], Loss: 0.4506\n",
      "Epoch [3/10], Step [28/147], Loss: 0.3111\n",
      "Epoch [3/10], Step [29/147], Loss: 0.3264\n",
      "Epoch [3/10], Step [30/147], Loss: 0.3822\n",
      "Epoch [3/10], Step [31/147], Loss: 0.3120\n",
      "Epoch [3/10], Step [32/147], Loss: 0.5552\n",
      "Epoch [3/10], Step [33/147], Loss: 0.4035\n",
      "Epoch [3/10], Step [34/147], Loss: 0.4584\n",
      "Epoch [3/10], Step [35/147], Loss: 0.5051\n",
      "Epoch [3/10], Step [36/147], Loss: 0.3146\n",
      "Epoch [3/10], Step [37/147], Loss: 0.4452\n",
      "Epoch [3/10], Step [38/147], Loss: 0.5463\n",
      "Epoch [3/10], Step [39/147], Loss: 0.3039\n",
      "Epoch [3/10], Step [40/147], Loss: 0.4520\n",
      "Epoch [3/10], Step [41/147], Loss: 0.4606\n",
      "Epoch [3/10], Step [42/147], Loss: 0.2502\n",
      "Epoch [3/10], Step [43/147], Loss: 0.4013\n",
      "Epoch [3/10], Step [44/147], Loss: 0.7002\n",
      "Epoch [3/10], Step [45/147], Loss: 0.4158\n",
      "Epoch [3/10], Step [46/147], Loss: 0.3947\n",
      "Epoch [3/10], Step [47/147], Loss: 0.5442\n",
      "Epoch [3/10], Step [48/147], Loss: 0.2993\n",
      "Epoch [3/10], Step [49/147], Loss: 0.2612\n",
      "Epoch [3/10], Step [50/147], Loss: 0.5659\n",
      "Epoch [3/10], Step [51/147], Loss: 0.4008\n",
      "Epoch [3/10], Step [52/147], Loss: 0.3511\n",
      "Epoch [3/10], Step [53/147], Loss: 0.3411\n",
      "Epoch [3/10], Step [54/147], Loss: 0.3106\n",
      "Epoch [3/10], Step [55/147], Loss: 0.4231\n",
      "Epoch [3/10], Step [56/147], Loss: 0.3304\n",
      "Epoch [3/10], Step [57/147], Loss: 0.2613\n",
      "Epoch [3/10], Step [58/147], Loss: 0.3880\n",
      "Epoch [3/10], Step [59/147], Loss: 0.5913\n",
      "Epoch [3/10], Step [60/147], Loss: 0.5510\n",
      "Epoch [3/10], Step [61/147], Loss: 0.3179\n",
      "Epoch [3/10], Step [62/147], Loss: 0.2638\n",
      "Epoch [3/10], Step [63/147], Loss: 0.6572\n",
      "Epoch [3/10], Step [64/147], Loss: 0.3250\n",
      "Epoch [3/10], Step [65/147], Loss: 0.5011\n",
      "Epoch [3/10], Step [66/147], Loss: 0.3650\n",
      "Epoch [3/10], Step [67/147], Loss: 0.3695\n",
      "Epoch [3/10], Step [68/147], Loss: 0.6660\n",
      "Epoch [3/10], Step [69/147], Loss: 0.3636\n",
      "Epoch [3/10], Step [70/147], Loss: 0.3393\n",
      "Epoch [3/10], Step [71/147], Loss: 0.3060\n",
      "Epoch [3/10], Step [72/147], Loss: 0.5519\n",
      "Epoch [3/10], Step [73/147], Loss: 0.4471\n",
      "Epoch [3/10], Step [74/147], Loss: 0.4779\n",
      "Epoch [3/10], Step [75/147], Loss: 0.4697\n",
      "Epoch [3/10], Step [76/147], Loss: 0.4750\n",
      "Epoch [3/10], Step [77/147], Loss: 0.5795\n",
      "Epoch [3/10], Step [78/147], Loss: 0.3589\n",
      "Epoch [3/10], Step [79/147], Loss: 0.3882\n",
      "Epoch [3/10], Step [80/147], Loss: 0.2177\n",
      "Epoch [3/10], Step [81/147], Loss: 0.4318\n",
      "Epoch [3/10], Step [82/147], Loss: 0.3520\n",
      "Epoch [3/10], Step [83/147], Loss: 0.3727\n",
      "Epoch [3/10], Step [84/147], Loss: 0.4014\n",
      "Epoch [3/10], Step [85/147], Loss: 0.5440\n",
      "Epoch [3/10], Step [86/147], Loss: 0.3175\n",
      "Epoch [3/10], Step [87/147], Loss: 0.3666\n",
      "Epoch [3/10], Step [88/147], Loss: 0.4209\n",
      "Epoch [3/10], Step [89/147], Loss: 0.3676\n",
      "Epoch [3/10], Step [90/147], Loss: 0.5305\n",
      "Epoch [3/10], Step [91/147], Loss: 0.3789\n",
      "Epoch [3/10], Step [92/147], Loss: 0.5759\n",
      "Epoch [3/10], Step [93/147], Loss: 0.4539\n",
      "Epoch [3/10], Step [94/147], Loss: 0.4603\n",
      "Epoch [3/10], Step [95/147], Loss: 0.4158\n",
      "Epoch [3/10], Step [96/147], Loss: 0.3825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [97/147], Loss: 0.5962\n",
      "Epoch [3/10], Step [98/147], Loss: 0.6797\n",
      "Epoch [3/10], Step [99/147], Loss: 0.6249\n",
      "Epoch [3/10], Step [100/147], Loss: 0.4946\n",
      "Epoch [3/10], Step [101/147], Loss: 0.5388\n",
      "Epoch [3/10], Step [102/147], Loss: 0.3855\n",
      "Epoch [3/10], Step [103/147], Loss: 0.3591\n",
      "Epoch [3/10], Step [104/147], Loss: 0.4111\n",
      "Epoch [3/10], Step [105/147], Loss: 0.5003\n",
      "Epoch [3/10], Step [106/147], Loss: 0.3943\n",
      "Epoch [3/10], Step [107/147], Loss: 0.3185\n",
      "Epoch [3/10], Step [108/147], Loss: 0.5478\n",
      "Epoch [3/10], Step [109/147], Loss: 0.3022\n",
      "Epoch [3/10], Step [110/147], Loss: 0.4416\n",
      "Epoch [3/10], Step [111/147], Loss: 0.4331\n",
      "Epoch [3/10], Step [112/147], Loss: 0.4054\n",
      "Epoch [3/10], Step [113/147], Loss: 0.4051\n",
      "Epoch [3/10], Step [114/147], Loss: 0.4319\n",
      "Epoch [3/10], Step [115/147], Loss: 0.3778\n",
      "Epoch [3/10], Step [116/147], Loss: 0.3258\n",
      "Epoch [3/10], Step [117/147], Loss: 0.2577\n",
      "Epoch [3/10], Step [118/147], Loss: 0.4899\n",
      "Epoch [3/10], Step [119/147], Loss: 0.3113\n",
      "Epoch [3/10], Step [120/147], Loss: 0.3810\n",
      "Epoch [3/10], Step [121/147], Loss: 0.3794\n",
      "Epoch [3/10], Step [122/147], Loss: 0.2602\n",
      "Epoch [3/10], Step [123/147], Loss: 0.4325\n",
      "Epoch [3/10], Step [124/147], Loss: 0.3150\n",
      "Epoch [3/10], Step [125/147], Loss: 0.2785\n",
      "Epoch [3/10], Step [126/147], Loss: 0.2517\n",
      "Epoch [3/10], Step [127/147], Loss: 0.2679\n",
      "Epoch [3/10], Step [128/147], Loss: 0.3189\n",
      "Epoch [3/10], Step [129/147], Loss: 0.3251\n",
      "Epoch [3/10], Step [130/147], Loss: 0.4769\n",
      "Epoch [3/10], Step [131/147], Loss: 0.4785\n",
      "Epoch [3/10], Step [132/147], Loss: 0.2541\n",
      "Epoch [3/10], Step [133/147], Loss: 0.2035\n",
      "Epoch [3/10], Step [134/147], Loss: 0.5070\n",
      "Epoch [3/10], Step [135/147], Loss: 0.5683\n",
      "Epoch [3/10], Step [136/147], Loss: 0.3482\n",
      "Epoch [3/10], Step [137/147], Loss: 0.4687\n",
      "Epoch [3/10], Step [138/147], Loss: 0.2478\n",
      "Epoch [3/10], Step [139/147], Loss: 0.5550\n",
      "Epoch [3/10], Step [140/147], Loss: 0.3585\n",
      "Epoch [3/10], Step [141/147], Loss: 0.6317\n",
      "Epoch [3/10], Step [142/147], Loss: 0.3927\n",
      "Epoch [3/10], Step [143/147], Loss: 0.6645\n",
      "Epoch [3/10], Step [144/147], Loss: 0.4370\n",
      "Epoch [3/10], Step [145/147], Loss: 0.3940\n",
      "Epoch [3/10], Step [146/147], Loss: 0.3314\n",
      "Epoch [3/10], Step [147/147], Loss: 0.3952\n",
      "Epoch [4/10], Step [1/147], Loss: 0.4349\n",
      "Epoch [4/10], Step [2/147], Loss: 0.3853\n",
      "Epoch [4/10], Step [3/147], Loss: 0.4956\n",
      "Epoch [4/10], Step [4/147], Loss: 0.4159\n",
      "Epoch [4/10], Step [5/147], Loss: 0.4066\n",
      "Epoch [4/10], Step [6/147], Loss: 0.2502\n",
      "Epoch [4/10], Step [7/147], Loss: 0.3386\n",
      "Epoch [4/10], Step [8/147], Loss: 0.4087\n",
      "Epoch [4/10], Step [9/147], Loss: 0.3974\n",
      "Epoch [4/10], Step [10/147], Loss: 0.2972\n",
      "Epoch [4/10], Step [11/147], Loss: 0.2978\n",
      "Epoch [4/10], Step [12/147], Loss: 0.5327\n",
      "Epoch [4/10], Step [13/147], Loss: 0.3946\n",
      "Epoch [4/10], Step [14/147], Loss: 0.3017\n",
      "Epoch [4/10], Step [15/147], Loss: 0.3541\n",
      "Epoch [4/10], Step [16/147], Loss: 0.3925\n",
      "Epoch [4/10], Step [17/147], Loss: 0.3451\n",
      "Epoch [4/10], Step [18/147], Loss: 0.3966\n",
      "Epoch [4/10], Step [19/147], Loss: 0.3125\n",
      "Epoch [4/10], Step [20/147], Loss: 0.4615\n",
      "Epoch [4/10], Step [21/147], Loss: 0.3727\n",
      "Epoch [4/10], Step [22/147], Loss: 0.3827\n",
      "Epoch [4/10], Step [23/147], Loss: 0.5416\n",
      "Epoch [4/10], Step [24/147], Loss: 0.4073\n",
      "Epoch [4/10], Step [25/147], Loss: 0.6035\n",
      "Epoch [4/10], Step [26/147], Loss: 0.5015\n",
      "Epoch [4/10], Step [27/147], Loss: 0.3361\n",
      "Epoch [4/10], Step [28/147], Loss: 0.4011\n",
      "Epoch [4/10], Step [29/147], Loss: 0.3504\n",
      "Epoch [4/10], Step [30/147], Loss: 0.3816\n",
      "Epoch [4/10], Step [31/147], Loss: 0.4126\n",
      "Epoch [4/10], Step [32/147], Loss: 0.3661\n",
      "Epoch [4/10], Step [33/147], Loss: 0.3705\n",
      "Epoch [4/10], Step [34/147], Loss: 0.3565\n",
      "Epoch [4/10], Step [35/147], Loss: 0.4083\n",
      "Epoch [4/10], Step [36/147], Loss: 0.3663\n",
      "Epoch [4/10], Step [37/147], Loss: 0.4211\n",
      "Epoch [4/10], Step [38/147], Loss: 0.4091\n",
      "Epoch [4/10], Step [39/147], Loss: 0.4233\n",
      "Epoch [4/10], Step [40/147], Loss: 0.3272\n",
      "Epoch [4/10], Step [41/147], Loss: 0.4812\n",
      "Epoch [4/10], Step [42/147], Loss: 0.4310\n",
      "Epoch [4/10], Step [43/147], Loss: 0.4073\n",
      "Epoch [4/10], Step [44/147], Loss: 0.2511\n",
      "Epoch [4/10], Step [45/147], Loss: 0.4177\n",
      "Epoch [4/10], Step [46/147], Loss: 0.3636\n",
      "Epoch [4/10], Step [47/147], Loss: 0.3215\n",
      "Epoch [4/10], Step [48/147], Loss: 0.5278\n",
      "Epoch [4/10], Step [49/147], Loss: 0.3197\n",
      "Epoch [4/10], Step [50/147], Loss: 0.2616\n",
      "Epoch [4/10], Step [51/147], Loss: 0.3219\n",
      "Epoch [4/10], Step [52/147], Loss: 0.2259\n",
      "Epoch [4/10], Step [53/147], Loss: 0.5324\n",
      "Epoch [4/10], Step [54/147], Loss: 0.3481\n",
      "Epoch [4/10], Step [55/147], Loss: 0.1917\n",
      "Epoch [4/10], Step [56/147], Loss: 0.4323\n",
      "Epoch [4/10], Step [57/147], Loss: 0.3711\n",
      "Epoch [4/10], Step [58/147], Loss: 0.2597\n",
      "Epoch [4/10], Step [59/147], Loss: 0.3373\n",
      "Epoch [4/10], Step [60/147], Loss: 0.3721\n",
      "Epoch [4/10], Step [61/147], Loss: 0.3206\n",
      "Epoch [4/10], Step [62/147], Loss: 0.5820\n",
      "Epoch [4/10], Step [63/147], Loss: 0.4101\n",
      "Epoch [4/10], Step [64/147], Loss: 0.3613\n",
      "Epoch [4/10], Step [65/147], Loss: 0.3303\n",
      "Epoch [4/10], Step [66/147], Loss: 0.6140\n",
      "Epoch [4/10], Step [67/147], Loss: 0.4465\n",
      "Epoch [4/10], Step [68/147], Loss: 0.3457\n",
      "Epoch [4/10], Step [69/147], Loss: 0.2948\n",
      "Epoch [4/10], Step [70/147], Loss: 0.3944\n",
      "Epoch [4/10], Step [71/147], Loss: 0.4707\n",
      "Epoch [4/10], Step [72/147], Loss: 0.3731\n",
      "Epoch [4/10], Step [73/147], Loss: 0.3573\n",
      "Epoch [4/10], Step [74/147], Loss: 0.3778\n",
      "Epoch [4/10], Step [75/147], Loss: 0.4195\n",
      "Epoch [4/10], Step [76/147], Loss: 0.4202\n",
      "Epoch [4/10], Step [77/147], Loss: 0.3101\n",
      "Epoch [4/10], Step [78/147], Loss: 0.4668\n",
      "Epoch [4/10], Step [79/147], Loss: 0.5032\n",
      "Epoch [4/10], Step [80/147], Loss: 0.3404\n",
      "Epoch [4/10], Step [81/147], Loss: 0.3804\n",
      "Epoch [4/10], Step [82/147], Loss: 0.3369\n",
      "Epoch [4/10], Step [83/147], Loss: 0.5026\n",
      "Epoch [4/10], Step [84/147], Loss: 0.5834\n",
      "Epoch [4/10], Step [85/147], Loss: 0.3318\n",
      "Epoch [4/10], Step [86/147], Loss: 0.3843\n",
      "Epoch [4/10], Step [87/147], Loss: 0.2966\n",
      "Epoch [4/10], Step [88/147], Loss: 0.2580\n",
      "Epoch [4/10], Step [89/147], Loss: 0.3443\n",
      "Epoch [4/10], Step [90/147], Loss: 0.4566\n",
      "Epoch [4/10], Step [91/147], Loss: 0.3055\n",
      "Epoch [4/10], Step [92/147], Loss: 0.3450\n",
      "Epoch [4/10], Step [93/147], Loss: 0.3801\n",
      "Epoch [4/10], Step [94/147], Loss: 0.4883\n",
      "Epoch [4/10], Step [95/147], Loss: 0.4184\n",
      "Epoch [4/10], Step [96/147], Loss: 0.4049\n",
      "Epoch [4/10], Step [97/147], Loss: 0.6085\n",
      "Epoch [4/10], Step [98/147], Loss: 0.4476\n",
      "Epoch [4/10], Step [99/147], Loss: 0.2955\n",
      "Epoch [4/10], Step [100/147], Loss: 0.2816\n",
      "Epoch [4/10], Step [101/147], Loss: 0.3952\n",
      "Epoch [4/10], Step [102/147], Loss: 0.4426\n",
      "Epoch [4/10], Step [103/147], Loss: 0.3491\n",
      "Epoch [4/10], Step [104/147], Loss: 0.2854\n",
      "Epoch [4/10], Step [105/147], Loss: 0.4426\n",
      "Epoch [4/10], Step [106/147], Loss: 0.3277\n",
      "Epoch [4/10], Step [107/147], Loss: 0.3056\n",
      "Epoch [4/10], Step [108/147], Loss: 0.3967\n",
      "Epoch [4/10], Step [109/147], Loss: 0.2939\n",
      "Epoch [4/10], Step [110/147], Loss: 0.2963\n",
      "Epoch [4/10], Step [111/147], Loss: 0.3535\n",
      "Epoch [4/10], Step [112/147], Loss: 0.4464\n",
      "Epoch [4/10], Step [113/147], Loss: 0.3915\n",
      "Epoch [4/10], Step [114/147], Loss: 0.1975\n",
      "Epoch [4/10], Step [115/147], Loss: 0.3243\n",
      "Epoch [4/10], Step [116/147], Loss: 0.3869\n",
      "Epoch [4/10], Step [117/147], Loss: 0.3784\n",
      "Epoch [4/10], Step [118/147], Loss: 0.3530\n",
      "Epoch [4/10], Step [119/147], Loss: 0.6406\n",
      "Epoch [4/10], Step [120/147], Loss: 0.2125\n",
      "Epoch [4/10], Step [121/147], Loss: 0.4643\n",
      "Epoch [4/10], Step [122/147], Loss: 0.4352\n",
      "Epoch [4/10], Step [123/147], Loss: 0.5043\n",
      "Epoch [4/10], Step [124/147], Loss: 0.4316\n",
      "Epoch [4/10], Step [125/147], Loss: 0.2787\n",
      "Epoch [4/10], Step [126/147], Loss: 0.4451\n",
      "Epoch [4/10], Step [127/147], Loss: 0.5739\n",
      "Epoch [4/10], Step [128/147], Loss: 0.3870\n",
      "Epoch [4/10], Step [129/147], Loss: 0.3148\n",
      "Epoch [4/10], Step [130/147], Loss: 0.2781\n",
      "Epoch [4/10], Step [131/147], Loss: 0.3284\n",
      "Epoch [4/10], Step [132/147], Loss: 0.3926\n",
      "Epoch [4/10], Step [133/147], Loss: 0.3627\n",
      "Epoch [4/10], Step [134/147], Loss: 0.4106\n",
      "Epoch [4/10], Step [135/147], Loss: 0.3010\n",
      "Epoch [4/10], Step [136/147], Loss: 0.4236\n",
      "Epoch [4/10], Step [137/147], Loss: 0.3290\n",
      "Epoch [4/10], Step [138/147], Loss: 0.3585\n",
      "Epoch [4/10], Step [139/147], Loss: 0.3023\n",
      "Epoch [4/10], Step [140/147], Loss: 0.2675\n",
      "Epoch [4/10], Step [141/147], Loss: 0.3871\n",
      "Epoch [4/10], Step [142/147], Loss: 0.2680\n",
      "Epoch [4/10], Step [143/147], Loss: 0.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [144/147], Loss: 0.5940\n",
      "Epoch [4/10], Step [145/147], Loss: 0.3983\n",
      "Epoch [4/10], Step [146/147], Loss: 0.3252\n",
      "Epoch [4/10], Step [147/147], Loss: 0.2753\n",
      "Epoch [5/10], Step [1/147], Loss: 0.3075\n",
      "Epoch [5/10], Step [2/147], Loss: 0.3636\n",
      "Epoch [5/10], Step [3/147], Loss: 0.4336\n",
      "Epoch [5/10], Step [4/147], Loss: 0.4574\n",
      "Epoch [5/10], Step [5/147], Loss: 0.4770\n",
      "Epoch [5/10], Step [6/147], Loss: 0.3582\n",
      "Epoch [5/10], Step [7/147], Loss: 0.2714\n",
      "Epoch [5/10], Step [8/147], Loss: 0.5538\n",
      "Epoch [5/10], Step [9/147], Loss: 0.4027\n",
      "Epoch [5/10], Step [10/147], Loss: 0.3136\n",
      "Epoch [5/10], Step [11/147], Loss: 0.2632\n",
      "Epoch [5/10], Step [12/147], Loss: 0.2123\n",
      "Epoch [5/10], Step [13/147], Loss: 0.3060\n",
      "Epoch [5/10], Step [14/147], Loss: 0.3164\n",
      "Epoch [5/10], Step [15/147], Loss: 0.4878\n",
      "Epoch [5/10], Step [16/147], Loss: 0.4585\n",
      "Epoch [5/10], Step [17/147], Loss: 0.3153\n",
      "Epoch [5/10], Step [18/147], Loss: 0.2187\n",
      "Epoch [5/10], Step [19/147], Loss: 0.5027\n",
      "Epoch [5/10], Step [20/147], Loss: 0.4550\n",
      "Epoch [5/10], Step [21/147], Loss: 0.2888\n",
      "Epoch [5/10], Step [22/147], Loss: 0.3551\n",
      "Epoch [5/10], Step [23/147], Loss: 0.3121\n",
      "Epoch [5/10], Step [24/147], Loss: 0.3909\n",
      "Epoch [5/10], Step [25/147], Loss: 0.2867\n",
      "Epoch [5/10], Step [26/147], Loss: 0.2481\n",
      "Epoch [5/10], Step [27/147], Loss: 0.2549\n",
      "Epoch [5/10], Step [28/147], Loss: 0.3648\n",
      "Epoch [5/10], Step [29/147], Loss: 0.3185\n",
      "Epoch [5/10], Step [30/147], Loss: 0.3889\n",
      "Epoch [5/10], Step [31/147], Loss: 0.4110\n",
      "Epoch [5/10], Step [32/147], Loss: 0.3190\n",
      "Epoch [5/10], Step [33/147], Loss: 0.2669\n",
      "Epoch [5/10], Step [34/147], Loss: 0.3496\n",
      "Epoch [5/10], Step [35/147], Loss: 0.2468\n",
      "Epoch [5/10], Step [36/147], Loss: 0.2456\n",
      "Epoch [5/10], Step [37/147], Loss: 0.3406\n",
      "Epoch [5/10], Step [38/147], Loss: 0.2995\n",
      "Epoch [5/10], Step [39/147], Loss: 0.2983\n",
      "Epoch [5/10], Step [40/147], Loss: 0.2290\n",
      "Epoch [5/10], Step [41/147], Loss: 0.3132\n",
      "Epoch [5/10], Step [42/147], Loss: 0.5294\n",
      "Epoch [5/10], Step [43/147], Loss: 0.4990\n",
      "Epoch [5/10], Step [44/147], Loss: 0.2551\n",
      "Epoch [5/10], Step [45/147], Loss: 0.4824\n",
      "Epoch [5/10], Step [46/147], Loss: 0.3719\n",
      "Epoch [5/10], Step [47/147], Loss: 0.2719\n",
      "Epoch [5/10], Step [48/147], Loss: 0.3305\n",
      "Epoch [5/10], Step [49/147], Loss: 0.2253\n",
      "Epoch [5/10], Step [50/147], Loss: 0.4205\n",
      "Epoch [5/10], Step [51/147], Loss: 0.3131\n",
      "Epoch [5/10], Step [52/147], Loss: 0.4229\n",
      "Epoch [5/10], Step [53/147], Loss: 0.4091\n",
      "Epoch [5/10], Step [54/147], Loss: 0.2868\n",
      "Epoch [5/10], Step [55/147], Loss: 0.3647\n",
      "Epoch [5/10], Step [56/147], Loss: 0.4515\n",
      "Epoch [5/10], Step [57/147], Loss: 0.3624\n",
      "Epoch [5/10], Step [58/147], Loss: 0.4233\n",
      "Epoch [5/10], Step [59/147], Loss: 0.3598\n",
      "Epoch [5/10], Step [60/147], Loss: 0.5183\n",
      "Epoch [5/10], Step [61/147], Loss: 0.3247\n",
      "Epoch [5/10], Step [62/147], Loss: 0.4543\n",
      "Epoch [5/10], Step [63/147], Loss: 0.3107\n",
      "Epoch [5/10], Step [64/147], Loss: 0.5932\n",
      "Epoch [5/10], Step [65/147], Loss: 0.4087\n",
      "Epoch [5/10], Step [66/147], Loss: 0.4315\n",
      "Epoch [5/10], Step [67/147], Loss: 0.3405\n",
      "Epoch [5/10], Step [68/147], Loss: 0.2660\n",
      "Epoch [5/10], Step [69/147], Loss: 0.3584\n",
      "Epoch [5/10], Step [70/147], Loss: 0.4115\n",
      "Epoch [5/10], Step [71/147], Loss: 0.2424\n",
      "Epoch [5/10], Step [72/147], Loss: 0.5452\n",
      "Epoch [5/10], Step [73/147], Loss: 0.4417\n",
      "Epoch [5/10], Step [74/147], Loss: 0.4186\n",
      "Epoch [5/10], Step [75/147], Loss: 0.3490\n",
      "Epoch [5/10], Step [76/147], Loss: 0.2413\n",
      "Epoch [5/10], Step [77/147], Loss: 0.3738\n",
      "Epoch [5/10], Step [78/147], Loss: 0.3652\n",
      "Epoch [5/10], Step [79/147], Loss: 0.2596\n",
      "Epoch [5/10], Step [80/147], Loss: 0.4304\n",
      "Epoch [5/10], Step [81/147], Loss: 0.3764\n",
      "Epoch [5/10], Step [82/147], Loss: 0.3715\n",
      "Epoch [5/10], Step [83/147], Loss: 0.3933\n",
      "Epoch [5/10], Step [84/147], Loss: 0.5055\n",
      "Epoch [5/10], Step [85/147], Loss: 0.3772\n",
      "Epoch [5/10], Step [86/147], Loss: 0.2422\n",
      "Epoch [5/10], Step [87/147], Loss: 0.3451\n",
      "Epoch [5/10], Step [88/147], Loss: 0.2833\n",
      "Epoch [5/10], Step [89/147], Loss: 0.3370\n",
      "Epoch [5/10], Step [90/147], Loss: 0.2916\n",
      "Epoch [5/10], Step [91/147], Loss: 0.2204\n",
      "Epoch [5/10], Step [92/147], Loss: 0.3506\n",
      "Epoch [5/10], Step [93/147], Loss: 0.4024\n",
      "Epoch [5/10], Step [94/147], Loss: 0.4471\n",
      "Epoch [5/10], Step [95/147], Loss: 0.2599\n",
      "Epoch [5/10], Step [96/147], Loss: 0.3350\n",
      "Epoch [5/10], Step [97/147], Loss: 0.4069\n",
      "Epoch [5/10], Step [98/147], Loss: 0.3566\n",
      "Epoch [5/10], Step [99/147], Loss: 0.3874\n",
      "Epoch [5/10], Step [100/147], Loss: 0.5529\n",
      "Epoch [5/10], Step [101/147], Loss: 0.3052\n",
      "Epoch [5/10], Step [102/147], Loss: 0.3107\n",
      "Epoch [5/10], Step [103/147], Loss: 0.2600\n",
      "Epoch [5/10], Step [104/147], Loss: 0.4013\n",
      "Epoch [5/10], Step [105/147], Loss: 0.3437\n",
      "Epoch [5/10], Step [106/147], Loss: 0.2516\n",
      "Epoch [5/10], Step [107/147], Loss: 0.3628\n",
      "Epoch [5/10], Step [108/147], Loss: 0.3578\n",
      "Epoch [5/10], Step [109/147], Loss: 0.2208\n",
      "Epoch [5/10], Step [110/147], Loss: 0.3896\n",
      "Epoch [5/10], Step [111/147], Loss: 0.3380\n",
      "Epoch [5/10], Step [112/147], Loss: 0.3592\n",
      "Epoch [5/10], Step [113/147], Loss: 0.6157\n",
      "Epoch [5/10], Step [114/147], Loss: 0.3950\n",
      "Epoch [5/10], Step [115/147], Loss: 0.4413\n",
      "Epoch [5/10], Step [116/147], Loss: 0.3895\n",
      "Epoch [5/10], Step [117/147], Loss: 0.2897\n",
      "Epoch [5/10], Step [118/147], Loss: 0.2944\n",
      "Epoch [5/10], Step [119/147], Loss: 0.3875\n",
      "Epoch [5/10], Step [120/147], Loss: 0.2042\n",
      "Epoch [5/10], Step [121/147], Loss: 0.4795\n",
      "Epoch [5/10], Step [122/147], Loss: 0.2101\n",
      "Epoch [5/10], Step [123/147], Loss: 0.3115\n",
      "Epoch [5/10], Step [124/147], Loss: 0.5364\n",
      "Epoch [5/10], Step [125/147], Loss: 0.2245\n",
      "Epoch [5/10], Step [126/147], Loss: 0.3749\n",
      "Epoch [5/10], Step [127/147], Loss: 0.3522\n",
      "Epoch [5/10], Step [128/147], Loss: 0.3906\n",
      "Epoch [5/10], Step [129/147], Loss: 0.3175\n",
      "Epoch [5/10], Step [130/147], Loss: 0.4033\n",
      "Epoch [5/10], Step [131/147], Loss: 0.3433\n",
      "Epoch [5/10], Step [132/147], Loss: 0.3882\n",
      "Epoch [5/10], Step [133/147], Loss: 0.3341\n",
      "Epoch [5/10], Step [134/147], Loss: 0.3279\n",
      "Epoch [5/10], Step [135/147], Loss: 0.3052\n",
      "Epoch [5/10], Step [136/147], Loss: 0.2686\n",
      "Epoch [5/10], Step [137/147], Loss: 0.3064\n",
      "Epoch [5/10], Step [138/147], Loss: 0.3690\n",
      "Epoch [5/10], Step [139/147], Loss: 0.3231\n",
      "Epoch [5/10], Step [140/147], Loss: 0.2172\n",
      "Epoch [5/10], Step [141/147], Loss: 0.2534\n",
      "Epoch [5/10], Step [142/147], Loss: 0.3423\n",
      "Epoch [5/10], Step [143/147], Loss: 0.3448\n",
      "Epoch [5/10], Step [144/147], Loss: 0.3731\n",
      "Epoch [5/10], Step [145/147], Loss: 0.2497\n",
      "Epoch [5/10], Step [146/147], Loss: 0.4225\n",
      "Epoch [5/10], Step [147/147], Loss: 0.4619\n",
      "Epoch [6/10], Step [1/147], Loss: 0.3514\n",
      "Epoch [6/10], Step [2/147], Loss: 0.4027\n",
      "Epoch [6/10], Step [3/147], Loss: 0.2471\n",
      "Epoch [6/10], Step [4/147], Loss: 0.4146\n",
      "Epoch [6/10], Step [5/147], Loss: 0.4275\n",
      "Epoch [6/10], Step [6/147], Loss: 0.4304\n",
      "Epoch [6/10], Step [7/147], Loss: 0.2707\n",
      "Epoch [6/10], Step [8/147], Loss: 0.2870\n",
      "Epoch [6/10], Step [9/147], Loss: 0.4429\n",
      "Epoch [6/10], Step [10/147], Loss: 0.5409\n",
      "Epoch [6/10], Step [11/147], Loss: 0.3116\n",
      "Epoch [6/10], Step [12/147], Loss: 0.2629\n",
      "Epoch [6/10], Step [13/147], Loss: 0.2223\n",
      "Epoch [6/10], Step [14/147], Loss: 0.3718\n",
      "Epoch [6/10], Step [15/147], Loss: 0.6540\n",
      "Epoch [6/10], Step [16/147], Loss: 0.2555\n",
      "Epoch [6/10], Step [17/147], Loss: 0.3015\n",
      "Epoch [6/10], Step [18/147], Loss: 0.3220\n",
      "Epoch [6/10], Step [19/147], Loss: 0.3192\n",
      "Epoch [6/10], Step [20/147], Loss: 0.2855\n",
      "Epoch [6/10], Step [21/147], Loss: 0.2075\n",
      "Epoch [6/10], Step [22/147], Loss: 0.1993\n",
      "Epoch [6/10], Step [23/147], Loss: 0.3336\n",
      "Epoch [6/10], Step [24/147], Loss: 0.4085\n",
      "Epoch [6/10], Step [25/147], Loss: 0.3203\n",
      "Epoch [6/10], Step [26/147], Loss: 0.4022\n",
      "Epoch [6/10], Step [27/147], Loss: 0.2419\n",
      "Epoch [6/10], Step [28/147], Loss: 0.2748\n",
      "Epoch [6/10], Step [29/147], Loss: 0.4158\n",
      "Epoch [6/10], Step [30/147], Loss: 0.1884\n",
      "Epoch [6/10], Step [31/147], Loss: 0.2326\n",
      "Epoch [6/10], Step [32/147], Loss: 0.2959\n",
      "Epoch [6/10], Step [33/147], Loss: 0.3439\n",
      "Epoch [6/10], Step [34/147], Loss: 0.3181\n",
      "Epoch [6/10], Step [35/147], Loss: 0.3819\n",
      "Epoch [6/10], Step [36/147], Loss: 0.3785\n",
      "Epoch [6/10], Step [37/147], Loss: 0.3804\n",
      "Epoch [6/10], Step [38/147], Loss: 0.3955\n",
      "Epoch [6/10], Step [39/147], Loss: 0.3490\n",
      "Epoch [6/10], Step [40/147], Loss: 0.4367\n",
      "Epoch [6/10], Step [41/147], Loss: 0.2599\n",
      "Epoch [6/10], Step [42/147], Loss: 0.4842\n",
      "Epoch [6/10], Step [43/147], Loss: 0.3988\n",
      "Epoch [6/10], Step [44/147], Loss: 0.2862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [45/147], Loss: 0.2405\n",
      "Epoch [6/10], Step [46/147], Loss: 0.5319\n",
      "Epoch [6/10], Step [47/147], Loss: 0.2508\n",
      "Epoch [6/10], Step [48/147], Loss: 0.2768\n",
      "Epoch [6/10], Step [49/147], Loss: 0.2534\n",
      "Epoch [6/10], Step [50/147], Loss: 0.4990\n",
      "Epoch [6/10], Step [51/147], Loss: 0.4824\n",
      "Epoch [6/10], Step [52/147], Loss: 0.3691\n",
      "Epoch [6/10], Step [53/147], Loss: 0.3120\n",
      "Epoch [6/10], Step [54/147], Loss: 0.4884\n",
      "Epoch [6/10], Step [55/147], Loss: 0.2600\n",
      "Epoch [6/10], Step [56/147], Loss: 0.2772\n",
      "Epoch [6/10], Step [57/147], Loss: 0.2630\n",
      "Epoch [6/10], Step [58/147], Loss: 0.3258\n",
      "Epoch [6/10], Step [59/147], Loss: 0.3084\n",
      "Epoch [6/10], Step [60/147], Loss: 0.4418\n",
      "Epoch [6/10], Step [61/147], Loss: 0.3283\n",
      "Epoch [6/10], Step [62/147], Loss: 0.2618\n",
      "Epoch [6/10], Step [63/147], Loss: 0.4244\n",
      "Epoch [6/10], Step [64/147], Loss: 0.2697\n",
      "Epoch [6/10], Step [65/147], Loss: 0.3916\n",
      "Epoch [6/10], Step [66/147], Loss: 0.4225\n",
      "Epoch [6/10], Step [67/147], Loss: 0.3096\n",
      "Epoch [6/10], Step [68/147], Loss: 0.4093\n",
      "Epoch [6/10], Step [69/147], Loss: 0.4667\n",
      "Epoch [6/10], Step [70/147], Loss: 0.3369\n",
      "Epoch [6/10], Step [71/147], Loss: 0.2714\n",
      "Epoch [6/10], Step [72/147], Loss: 0.1881\n",
      "Epoch [6/10], Step [73/147], Loss: 0.2873\n",
      "Epoch [6/10], Step [74/147], Loss: 0.2896\n",
      "Epoch [6/10], Step [75/147], Loss: 0.4918\n",
      "Epoch [6/10], Step [76/147], Loss: 0.3276\n",
      "Epoch [6/10], Step [77/147], Loss: 0.3352\n",
      "Epoch [6/10], Step [78/147], Loss: 0.3888\n",
      "Epoch [6/10], Step [79/147], Loss: 0.2068\n",
      "Epoch [6/10], Step [80/147], Loss: 0.3093\n",
      "Epoch [6/10], Step [81/147], Loss: 0.3814\n",
      "Epoch [6/10], Step [82/147], Loss: 0.3384\n",
      "Epoch [6/10], Step [83/147], Loss: 0.2339\n",
      "Epoch [6/10], Step [84/147], Loss: 0.3259\n",
      "Epoch [6/10], Step [85/147], Loss: 0.2217\n",
      "Epoch [6/10], Step [86/147], Loss: 0.2066\n",
      "Epoch [6/10], Step [87/147], Loss: 0.4171\n",
      "Epoch [6/10], Step [88/147], Loss: 0.4363\n",
      "Epoch [6/10], Step [89/147], Loss: 0.2772\n",
      "Epoch [6/10], Step [90/147], Loss: 0.3885\n",
      "Epoch [6/10], Step [91/147], Loss: 0.3281\n",
      "Epoch [6/10], Step [92/147], Loss: 0.3912\n",
      "Epoch [6/10], Step [93/147], Loss: 0.4594\n",
      "Epoch [6/10], Step [94/147], Loss: 0.5595\n",
      "Epoch [6/10], Step [95/147], Loss: 0.3676\n",
      "Epoch [6/10], Step [96/147], Loss: 0.2988\n",
      "Epoch [6/10], Step [97/147], Loss: 0.3697\n",
      "Epoch [6/10], Step [98/147], Loss: 0.2852\n",
      "Epoch [6/10], Step [99/147], Loss: 0.3977\n",
      "Epoch [6/10], Step [100/147], Loss: 0.2725\n",
      "Epoch [6/10], Step [101/147], Loss: 0.2861\n",
      "Epoch [6/10], Step [102/147], Loss: 0.3929\n",
      "Epoch [6/10], Step [103/147], Loss: 0.2430\n",
      "Epoch [6/10], Step [104/147], Loss: 0.3050\n",
      "Epoch [6/10], Step [105/147], Loss: 0.3930\n",
      "Epoch [6/10], Step [106/147], Loss: 0.5118\n",
      "Epoch [6/10], Step [107/147], Loss: 0.3400\n",
      "Epoch [6/10], Step [108/147], Loss: 0.3819\n",
      "Epoch [6/10], Step [109/147], Loss: 0.7349\n",
      "Epoch [6/10], Step [110/147], Loss: 0.2320\n",
      "Epoch [6/10], Step [111/147], Loss: 0.3810\n",
      "Epoch [6/10], Step [112/147], Loss: 0.6173\n",
      "Epoch [6/10], Step [113/147], Loss: 0.2375\n",
      "Epoch [6/10], Step [114/147], Loss: 0.4272\n",
      "Epoch [6/10], Step [115/147], Loss: 0.4705\n",
      "Epoch [6/10], Step [116/147], Loss: 0.4130\n",
      "Epoch [6/10], Step [117/147], Loss: 0.4754\n",
      "Epoch [6/10], Step [118/147], Loss: 0.3675\n",
      "Epoch [6/10], Step [119/147], Loss: 0.1918\n",
      "Epoch [6/10], Step [120/147], Loss: 0.4828\n",
      "Epoch [6/10], Step [121/147], Loss: 0.3711\n",
      "Epoch [6/10], Step [122/147], Loss: 0.2783\n",
      "Epoch [6/10], Step [123/147], Loss: 0.3330\n",
      "Epoch [6/10], Step [124/147], Loss: 0.3394\n",
      "Epoch [6/10], Step [125/147], Loss: 0.3119\n",
      "Epoch [6/10], Step [126/147], Loss: 0.2684\n",
      "Epoch [6/10], Step [127/147], Loss: 0.3231\n",
      "Epoch [6/10], Step [128/147], Loss: 0.3323\n",
      "Epoch [6/10], Step [129/147], Loss: 0.3865\n",
      "Epoch [6/10], Step [130/147], Loss: 0.2779\n",
      "Epoch [6/10], Step [131/147], Loss: 0.3176\n",
      "Epoch [6/10], Step [132/147], Loss: 0.2568\n",
      "Epoch [6/10], Step [133/147], Loss: 0.3626\n",
      "Epoch [6/10], Step [134/147], Loss: 0.4610\n",
      "Epoch [6/10], Step [135/147], Loss: 0.4772\n",
      "Epoch [6/10], Step [136/147], Loss: 0.3868\n",
      "Epoch [6/10], Step [137/147], Loss: 0.2803\n",
      "Epoch [6/10], Step [138/147], Loss: 0.2929\n",
      "Epoch [6/10], Step [139/147], Loss: 0.3591\n",
      "Epoch [6/10], Step [140/147], Loss: 0.2514\n",
      "Epoch [6/10], Step [141/147], Loss: 0.3299\n",
      "Epoch [6/10], Step [142/147], Loss: 0.4202\n",
      "Epoch [6/10], Step [143/147], Loss: 0.2686\n",
      "Epoch [6/10], Step [144/147], Loss: 0.2648\n",
      "Epoch [6/10], Step [145/147], Loss: 0.1988\n",
      "Epoch [6/10], Step [146/147], Loss: 0.3939\n",
      "Epoch [6/10], Step [147/147], Loss: 0.2236\n",
      "Epoch [7/10], Step [1/147], Loss: 0.3172\n",
      "Epoch [7/10], Step [2/147], Loss: 0.3104\n",
      "Epoch [7/10], Step [3/147], Loss: 0.2677\n",
      "Epoch [7/10], Step [4/147], Loss: 0.2598\n",
      "Epoch [7/10], Step [5/147], Loss: 0.2618\n",
      "Epoch [7/10], Step [6/147], Loss: 0.3811\n",
      "Epoch [7/10], Step [7/147], Loss: 0.2951\n",
      "Epoch [7/10], Step [8/147], Loss: 0.3673\n",
      "Epoch [7/10], Step [9/147], Loss: 0.2993\n",
      "Epoch [7/10], Step [10/147], Loss: 0.3833\n",
      "Epoch [7/10], Step [11/147], Loss: 0.2641\n",
      "Epoch [7/10], Step [12/147], Loss: 0.2476\n",
      "Epoch [7/10], Step [13/147], Loss: 0.3190\n",
      "Epoch [7/10], Step [14/147], Loss: 0.2499\n",
      "Epoch [7/10], Step [15/147], Loss: 0.3389\n",
      "Epoch [7/10], Step [16/147], Loss: 0.4577\n",
      "Epoch [7/10], Step [17/147], Loss: 0.2544\n",
      "Epoch [7/10], Step [18/147], Loss: 0.3847\n",
      "Epoch [7/10], Step [19/147], Loss: 0.5423\n",
      "Epoch [7/10], Step [20/147], Loss: 0.2673\n",
      "Epoch [7/10], Step [21/147], Loss: 0.2038\n",
      "Epoch [7/10], Step [22/147], Loss: 0.1922\n",
      "Epoch [7/10], Step [23/147], Loss: 0.2545\n",
      "Epoch [7/10], Step [24/147], Loss: 0.3580\n",
      "Epoch [7/10], Step [25/147], Loss: 0.3329\n",
      "Epoch [7/10], Step [26/147], Loss: 0.2078\n",
      "Epoch [7/10], Step [27/147], Loss: 0.5102\n",
      "Epoch [7/10], Step [28/147], Loss: 0.2350\n",
      "Epoch [7/10], Step [29/147], Loss: 0.2527\n",
      "Epoch [7/10], Step [30/147], Loss: 0.1933\n",
      "Epoch [7/10], Step [31/147], Loss: 0.1995\n",
      "Epoch [7/10], Step [32/147], Loss: 0.3402\n",
      "Epoch [7/10], Step [33/147], Loss: 0.3776\n",
      "Epoch [7/10], Step [34/147], Loss: 0.3388\n",
      "Epoch [7/10], Step [35/147], Loss: 0.3256\n",
      "Epoch [7/10], Step [36/147], Loss: 0.3162\n",
      "Epoch [7/10], Step [37/147], Loss: 0.2175\n",
      "Epoch [7/10], Step [38/147], Loss: 0.5226\n",
      "Epoch [7/10], Step [39/147], Loss: 0.3408\n",
      "Epoch [7/10], Step [40/147], Loss: 0.2104\n",
      "Epoch [7/10], Step [41/147], Loss: 0.3818\n",
      "Epoch [7/10], Step [42/147], Loss: 0.3676\n",
      "Epoch [7/10], Step [43/147], Loss: 0.4336\n",
      "Epoch [7/10], Step [44/147], Loss: 0.3171\n",
      "Epoch [7/10], Step [45/147], Loss: 0.2010\n",
      "Epoch [7/10], Step [46/147], Loss: 0.2888\n",
      "Epoch [7/10], Step [47/147], Loss: 0.2921\n",
      "Epoch [7/10], Step [48/147], Loss: 0.3606\n",
      "Epoch [7/10], Step [49/147], Loss: 0.2629\n",
      "Epoch [7/10], Step [50/147], Loss: 0.4073\n",
      "Epoch [7/10], Step [51/147], Loss: 0.2075\n",
      "Epoch [7/10], Step [52/147], Loss: 0.3329\n",
      "Epoch [7/10], Step [53/147], Loss: 0.2683\n",
      "Epoch [7/10], Step [54/147], Loss: 0.3126\n",
      "Epoch [7/10], Step [55/147], Loss: 0.4164\n",
      "Epoch [7/10], Step [56/147], Loss: 0.4082\n",
      "Epoch [7/10], Step [57/147], Loss: 0.2891\n",
      "Epoch [7/10], Step [58/147], Loss: 0.2261\n",
      "Epoch [7/10], Step [59/147], Loss: 0.4595\n",
      "Epoch [7/10], Step [60/147], Loss: 0.1875\n",
      "Epoch [7/10], Step [61/147], Loss: 0.3829\n",
      "Epoch [7/10], Step [62/147], Loss: 0.1823\n",
      "Epoch [7/10], Step [63/147], Loss: 0.2428\n",
      "Epoch [7/10], Step [64/147], Loss: 0.2344\n",
      "Epoch [7/10], Step [65/147], Loss: 0.4344\n",
      "Epoch [7/10], Step [66/147], Loss: 0.3214\n",
      "Epoch [7/10], Step [67/147], Loss: 0.2454\n",
      "Epoch [7/10], Step [68/147], Loss: 0.3125\n",
      "Epoch [7/10], Step [69/147], Loss: 0.2235\n",
      "Epoch [7/10], Step [70/147], Loss: 0.3296\n",
      "Epoch [7/10], Step [71/147], Loss: 0.1820\n",
      "Epoch [7/10], Step [72/147], Loss: 0.4316\n",
      "Epoch [7/10], Step [73/147], Loss: 0.3731\n",
      "Epoch [7/10], Step [74/147], Loss: 0.2543\n",
      "Epoch [7/10], Step [75/147], Loss: 0.3435\n",
      "Epoch [7/10], Step [76/147], Loss: 0.1733\n",
      "Epoch [7/10], Step [77/147], Loss: 0.3037\n",
      "Epoch [7/10], Step [78/147], Loss: 0.2906\n",
      "Epoch [7/10], Step [79/147], Loss: 0.4807\n",
      "Epoch [7/10], Step [80/147], Loss: 0.3432\n",
      "Epoch [7/10], Step [81/147], Loss: 0.3483\n",
      "Epoch [7/10], Step [82/147], Loss: 0.3128\n",
      "Epoch [7/10], Step [83/147], Loss: 0.3035\n",
      "Epoch [7/10], Step [84/147], Loss: 0.3924\n",
      "Epoch [7/10], Step [85/147], Loss: 0.3070\n",
      "Epoch [7/10], Step [86/147], Loss: 0.2952\n",
      "Epoch [7/10], Step [87/147], Loss: 0.3333\n",
      "Epoch [7/10], Step [88/147], Loss: 0.2148\n",
      "Epoch [7/10], Step [89/147], Loss: 0.1953\n",
      "Epoch [7/10], Step [90/147], Loss: 0.3018\n",
      "Epoch [7/10], Step [91/147], Loss: 0.2658\n",
      "Epoch [7/10], Step [92/147], Loss: 0.3084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [93/147], Loss: 0.3107\n",
      "Epoch [7/10], Step [94/147], Loss: 0.4178\n",
      "Epoch [7/10], Step [95/147], Loss: 0.3768\n",
      "Epoch [7/10], Step [96/147], Loss: 0.3535\n",
      "Epoch [7/10], Step [97/147], Loss: 0.2713\n",
      "Epoch [7/10], Step [98/147], Loss: 0.2203\n",
      "Epoch [7/10], Step [99/147], Loss: 0.2758\n",
      "Epoch [7/10], Step [100/147], Loss: 0.4228\n",
      "Epoch [7/10], Step [101/147], Loss: 0.2810\n",
      "Epoch [7/10], Step [102/147], Loss: 0.4188\n",
      "Epoch [7/10], Step [103/147], Loss: 0.3164\n",
      "Epoch [7/10], Step [104/147], Loss: 0.3195\n",
      "Epoch [7/10], Step [105/147], Loss: 0.4098\n",
      "Epoch [7/10], Step [106/147], Loss: 0.1980\n",
      "Epoch [7/10], Step [107/147], Loss: 0.3295\n",
      "Epoch [7/10], Step [108/147], Loss: 0.2354\n",
      "Epoch [7/10], Step [109/147], Loss: 0.3246\n",
      "Epoch [7/10], Step [110/147], Loss: 0.2714\n",
      "Epoch [7/10], Step [111/147], Loss: 0.5791\n",
      "Epoch [7/10], Step [112/147], Loss: 0.2850\n",
      "Epoch [7/10], Step [113/147], Loss: 0.3575\n",
      "Epoch [7/10], Step [114/147], Loss: 0.2122\n",
      "Epoch [7/10], Step [115/147], Loss: 0.1526\n",
      "Epoch [7/10], Step [116/147], Loss: 0.4800\n",
      "Epoch [7/10], Step [117/147], Loss: 0.2155\n",
      "Epoch [7/10], Step [118/147], Loss: 0.3968\n",
      "Epoch [7/10], Step [119/147], Loss: 0.3302\n",
      "Epoch [7/10], Step [120/147], Loss: 0.3242\n",
      "Epoch [7/10], Step [121/147], Loss: 0.2824\n",
      "Epoch [7/10], Step [122/147], Loss: 0.3515\n",
      "Epoch [7/10], Step [123/147], Loss: 0.4363\n",
      "Epoch [7/10], Step [124/147], Loss: 0.4178\n",
      "Epoch [7/10], Step [125/147], Loss: 0.3184\n",
      "Epoch [7/10], Step [126/147], Loss: 0.3212\n",
      "Epoch [7/10], Step [127/147], Loss: 0.4831\n",
      "Epoch [7/10], Step [128/147], Loss: 0.4204\n",
      "Epoch [7/10], Step [129/147], Loss: 0.4065\n",
      "Epoch [7/10], Step [130/147], Loss: 0.4068\n",
      "Epoch [7/10], Step [131/147], Loss: 0.4954\n",
      "Epoch [7/10], Step [132/147], Loss: 0.3873\n",
      "Epoch [7/10], Step [133/147], Loss: 0.2208\n",
      "Epoch [7/10], Step [134/147], Loss: 0.2689\n",
      "Epoch [7/10], Step [135/147], Loss: 0.1862\n",
      "Epoch [7/10], Step [136/147], Loss: 0.4027\n",
      "Epoch [7/10], Step [137/147], Loss: 0.3536\n",
      "Epoch [7/10], Step [138/147], Loss: 0.4106\n",
      "Epoch [7/10], Step [139/147], Loss: 0.3384\n",
      "Epoch [7/10], Step [140/147], Loss: 0.2466\n",
      "Epoch [7/10], Step [141/147], Loss: 0.2937\n",
      "Epoch [7/10], Step [142/147], Loss: 0.4032\n",
      "Epoch [7/10], Step [143/147], Loss: 0.3630\n",
      "Epoch [7/10], Step [144/147], Loss: 0.4536\n",
      "Epoch [7/10], Step [145/147], Loss: 0.2887\n",
      "Epoch [7/10], Step [146/147], Loss: 0.3736\n",
      "Epoch [7/10], Step [147/147], Loss: 0.3872\n",
      "Epoch [8/10], Step [1/147], Loss: 0.4489\n",
      "Epoch [8/10], Step [2/147], Loss: 0.2411\n",
      "Epoch [8/10], Step [3/147], Loss: 0.2759\n",
      "Epoch [8/10], Step [4/147], Loss: 0.2305\n",
      "Epoch [8/10], Step [5/147], Loss: 0.3076\n",
      "Epoch [8/10], Step [6/147], Loss: 0.3346\n",
      "Epoch [8/10], Step [7/147], Loss: 0.3776\n",
      "Epoch [8/10], Step [8/147], Loss: 0.2330\n",
      "Epoch [8/10], Step [9/147], Loss: 0.2366\n",
      "Epoch [8/10], Step [10/147], Loss: 0.2649\n",
      "Epoch [8/10], Step [11/147], Loss: 0.3858\n",
      "Epoch [8/10], Step [12/147], Loss: 0.3987\n",
      "Epoch [8/10], Step [13/147], Loss: 0.3517\n",
      "Epoch [8/10], Step [14/147], Loss: 0.1835\n",
      "Epoch [8/10], Step [15/147], Loss: 0.2899\n",
      "Epoch [8/10], Step [16/147], Loss: 0.1353\n",
      "Epoch [8/10], Step [17/147], Loss: 0.2374\n",
      "Epoch [8/10], Step [18/147], Loss: 0.4506\n",
      "Epoch [8/10], Step [19/147], Loss: 0.2553\n",
      "Epoch [8/10], Step [20/147], Loss: 0.2638\n",
      "Epoch [8/10], Step [21/147], Loss: 0.2599\n",
      "Epoch [8/10], Step [22/147], Loss: 0.2224\n",
      "Epoch [8/10], Step [23/147], Loss: 0.2392\n",
      "Epoch [8/10], Step [24/147], Loss: 0.3823\n",
      "Epoch [8/10], Step [25/147], Loss: 0.3684\n",
      "Epoch [8/10], Step [26/147], Loss: 0.2443\n",
      "Epoch [8/10], Step [27/147], Loss: 0.2481\n",
      "Epoch [8/10], Step [28/147], Loss: 0.4370\n",
      "Epoch [8/10], Step [29/147], Loss: 0.2887\n",
      "Epoch [8/10], Step [30/147], Loss: 0.1478\n",
      "Epoch [8/10], Step [31/147], Loss: 0.5279\n",
      "Epoch [8/10], Step [32/147], Loss: 0.2382\n",
      "Epoch [8/10], Step [33/147], Loss: 0.1604\n",
      "Epoch [8/10], Step [34/147], Loss: 0.3602\n",
      "Epoch [8/10], Step [35/147], Loss: 0.2751\n",
      "Epoch [8/10], Step [36/147], Loss: 0.3997\n",
      "Epoch [8/10], Step [37/147], Loss: 0.2785\n",
      "Epoch [8/10], Step [38/147], Loss: 0.2083\n",
      "Epoch [8/10], Step [39/147], Loss: 0.3928\n",
      "Epoch [8/10], Step [40/147], Loss: 0.2755\n",
      "Epoch [8/10], Step [41/147], Loss: 0.2691\n",
      "Epoch [8/10], Step [42/147], Loss: 0.2419\n",
      "Epoch [8/10], Step [43/147], Loss: 0.3697\n",
      "Epoch [8/10], Step [44/147], Loss: 0.3012\n",
      "Epoch [8/10], Step [45/147], Loss: 0.3237\n",
      "Epoch [8/10], Step [46/147], Loss: 0.3310\n",
      "Epoch [8/10], Step [47/147], Loss: 0.2492\n",
      "Epoch [8/10], Step [48/147], Loss: 0.2972\n",
      "Epoch [8/10], Step [49/147], Loss: 0.3764\n",
      "Epoch [8/10], Step [50/147], Loss: 0.3250\n",
      "Epoch [8/10], Step [51/147], Loss: 0.2728\n",
      "Epoch [8/10], Step [52/147], Loss: 0.3290\n",
      "Epoch [8/10], Step [53/147], Loss: 0.5698\n",
      "Epoch [8/10], Step [54/147], Loss: 0.4313\n",
      "Epoch [8/10], Step [55/147], Loss: 0.3696\n",
      "Epoch [8/10], Step [56/147], Loss: 0.3151\n",
      "Epoch [8/10], Step [57/147], Loss: 0.2863\n",
      "Epoch [8/10], Step [58/147], Loss: 0.2196\n",
      "Epoch [8/10], Step [59/147], Loss: 0.3769\n",
      "Epoch [8/10], Step [60/147], Loss: 0.1923\n",
      "Epoch [8/10], Step [61/147], Loss: 0.4325\n",
      "Epoch [8/10], Step [62/147], Loss: 0.4365\n",
      "Epoch [8/10], Step [63/147], Loss: 0.1439\n",
      "Epoch [8/10], Step [64/147], Loss: 0.3276\n",
      "Epoch [8/10], Step [65/147], Loss: 0.2995\n",
      "Epoch [8/10], Step [66/147], Loss: 0.3436\n",
      "Epoch [8/10], Step [67/147], Loss: 0.2038\n",
      "Epoch [8/10], Step [68/147], Loss: 0.3229\n",
      "Epoch [8/10], Step [69/147], Loss: 0.2376\n",
      "Epoch [8/10], Step [70/147], Loss: 0.3329\n",
      "Epoch [8/10], Step [71/147], Loss: 0.2577\n",
      "Epoch [8/10], Step [72/147], Loss: 0.4037\n",
      "Epoch [8/10], Step [73/147], Loss: 0.1630\n",
      "Epoch [8/10], Step [74/147], Loss: 0.3370\n",
      "Epoch [8/10], Step [75/147], Loss: 0.2774\n",
      "Epoch [8/10], Step [76/147], Loss: 0.2124\n",
      "Epoch [8/10], Step [77/147], Loss: 0.2575\n",
      "Epoch [8/10], Step [78/147], Loss: 0.2195\n",
      "Epoch [8/10], Step [79/147], Loss: 0.2587\n",
      "Epoch [8/10], Step [80/147], Loss: 0.2870\n",
      "Epoch [8/10], Step [81/147], Loss: 0.3093\n",
      "Epoch [8/10], Step [82/147], Loss: 0.4272\n",
      "Epoch [8/10], Step [83/147], Loss: 0.2377\n",
      "Epoch [8/10], Step [84/147], Loss: 0.2189\n",
      "Epoch [8/10], Step [85/147], Loss: 0.2441\n",
      "Epoch [8/10], Step [86/147], Loss: 0.2997\n",
      "Epoch [8/10], Step [87/147], Loss: 0.4231\n",
      "Epoch [8/10], Step [88/147], Loss: 0.2754\n",
      "Epoch [8/10], Step [89/147], Loss: 0.2526\n",
      "Epoch [8/10], Step [90/147], Loss: 0.2678\n",
      "Epoch [8/10], Step [91/147], Loss: 0.3428\n",
      "Epoch [8/10], Step [92/147], Loss: 0.5017\n",
      "Epoch [8/10], Step [93/147], Loss: 0.2979\n",
      "Epoch [8/10], Step [94/147], Loss: 0.1942\n",
      "Epoch [8/10], Step [95/147], Loss: 0.5923\n",
      "Epoch [8/10], Step [96/147], Loss: 0.5396\n",
      "Epoch [8/10], Step [97/147], Loss: 0.3319\n",
      "Epoch [8/10], Step [98/147], Loss: 0.2282\n",
      "Epoch [8/10], Step [99/147], Loss: 0.2042\n",
      "Epoch [8/10], Step [100/147], Loss: 0.3203\n",
      "Epoch [8/10], Step [101/147], Loss: 0.3179\n",
      "Epoch [8/10], Step [102/147], Loss: 0.2270\n",
      "Epoch [8/10], Step [103/147], Loss: 0.3740\n",
      "Epoch [8/10], Step [104/147], Loss: 0.3552\n",
      "Epoch [8/10], Step [105/147], Loss: 0.2970\n",
      "Epoch [8/10], Step [106/147], Loss: 0.2099\n",
      "Epoch [8/10], Step [107/147], Loss: 0.3740\n",
      "Epoch [8/10], Step [108/147], Loss: 0.2688\n",
      "Epoch [8/10], Step [109/147], Loss: 0.4521\n",
      "Epoch [8/10], Step [110/147], Loss: 0.3758\n",
      "Epoch [8/10], Step [111/147], Loss: 0.2737\n",
      "Epoch [8/10], Step [112/147], Loss: 0.4044\n",
      "Epoch [8/10], Step [113/147], Loss: 0.4130\n",
      "Epoch [8/10], Step [114/147], Loss: 0.2140\n",
      "Epoch [8/10], Step [115/147], Loss: 0.3020\n",
      "Epoch [8/10], Step [116/147], Loss: 0.4766\n",
      "Epoch [8/10], Step [117/147], Loss: 0.3381\n",
      "Epoch [8/10], Step [118/147], Loss: 0.1648\n",
      "Epoch [8/10], Step [119/147], Loss: 0.3168\n",
      "Epoch [8/10], Step [120/147], Loss: 0.3917\n",
      "Epoch [8/10], Step [121/147], Loss: 0.3569\n",
      "Epoch [8/10], Step [122/147], Loss: 0.2195\n",
      "Epoch [8/10], Step [123/147], Loss: 0.3131\n",
      "Epoch [8/10], Step [124/147], Loss: 0.4239\n",
      "Epoch [8/10], Step [125/147], Loss: 0.3890\n",
      "Epoch [8/10], Step [126/147], Loss: 0.5175\n",
      "Epoch [8/10], Step [127/147], Loss: 0.4003\n",
      "Epoch [8/10], Step [128/147], Loss: 0.2577\n",
      "Epoch [8/10], Step [129/147], Loss: 0.3346\n",
      "Epoch [8/10], Step [130/147], Loss: 0.3492\n",
      "Epoch [8/10], Step [131/147], Loss: 0.4682\n",
      "Epoch [8/10], Step [132/147], Loss: 0.4952\n",
      "Epoch [8/10], Step [133/147], Loss: 0.4052\n",
      "Epoch [8/10], Step [134/147], Loss: 0.3693\n",
      "Epoch [8/10], Step [135/147], Loss: 0.2572\n",
      "Epoch [8/10], Step [136/147], Loss: 0.3553\n",
      "Epoch [8/10], Step [137/147], Loss: 0.3629\n",
      "Epoch [8/10], Step [138/147], Loss: 0.3413\n",
      "Epoch [8/10], Step [139/147], Loss: 0.1274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [140/147], Loss: 0.3481\n",
      "Epoch [8/10], Step [141/147], Loss: 0.4096\n",
      "Epoch [8/10], Step [142/147], Loss: 0.2700\n",
      "Epoch [8/10], Step [143/147], Loss: 0.2835\n",
      "Epoch [8/10], Step [144/147], Loss: 0.2650\n",
      "Epoch [8/10], Step [145/147], Loss: 0.3626\n",
      "Epoch [8/10], Step [146/147], Loss: 0.3126\n",
      "Epoch [8/10], Step [147/147], Loss: 0.1437\n",
      "Epoch [9/10], Step [1/147], Loss: 0.2293\n",
      "Epoch [9/10], Step [2/147], Loss: 0.2213\n",
      "Epoch [9/10], Step [3/147], Loss: 0.2712\n",
      "Epoch [9/10], Step [4/147], Loss: 0.2134\n",
      "Epoch [9/10], Step [5/147], Loss: 0.3837\n",
      "Epoch [9/10], Step [6/147], Loss: 0.5159\n",
      "Epoch [9/10], Step [7/147], Loss: 0.1234\n",
      "Epoch [9/10], Step [8/147], Loss: 0.2375\n",
      "Epoch [9/10], Step [9/147], Loss: 0.3687\n",
      "Epoch [9/10], Step [10/147], Loss: 0.2597\n",
      "Epoch [9/10], Step [11/147], Loss: 0.3152\n",
      "Epoch [9/10], Step [12/147], Loss: 0.3868\n",
      "Epoch [9/10], Step [13/147], Loss: 0.3646\n",
      "Epoch [9/10], Step [14/147], Loss: 0.2891\n",
      "Epoch [9/10], Step [15/147], Loss: 0.2371\n",
      "Epoch [9/10], Step [16/147], Loss: 0.2999\n",
      "Epoch [9/10], Step [17/147], Loss: 0.1663\n",
      "Epoch [9/10], Step [18/147], Loss: 0.1527\n",
      "Epoch [9/10], Step [19/147], Loss: 0.1863\n",
      "Epoch [9/10], Step [20/147], Loss: 0.2948\n",
      "Epoch [9/10], Step [21/147], Loss: 0.2825\n",
      "Epoch [9/10], Step [22/147], Loss: 0.1957\n",
      "Epoch [9/10], Step [23/147], Loss: 0.1513\n",
      "Epoch [9/10], Step [24/147], Loss: 0.3538\n",
      "Epoch [9/10], Step [25/147], Loss: 0.3868\n",
      "Epoch [9/10], Step [26/147], Loss: 0.2154\n",
      "Epoch [9/10], Step [27/147], Loss: 0.2581\n",
      "Epoch [9/10], Step [28/147], Loss: 0.4151\n",
      "Epoch [9/10], Step [29/147], Loss: 0.1813\n",
      "Epoch [9/10], Step [30/147], Loss: 0.2425\n",
      "Epoch [9/10], Step [31/147], Loss: 0.2258\n",
      "Epoch [9/10], Step [32/147], Loss: 0.3020\n",
      "Epoch [9/10], Step [33/147], Loss: 0.2969\n",
      "Epoch [9/10], Step [34/147], Loss: 0.3328\n",
      "Epoch [9/10], Step [35/147], Loss: 0.3175\n",
      "Epoch [9/10], Step [36/147], Loss: 0.2634\n",
      "Epoch [9/10], Step [37/147], Loss: 0.2684\n",
      "Epoch [9/10], Step [38/147], Loss: 0.3030\n",
      "Epoch [9/10], Step [39/147], Loss: 0.2420\n",
      "Epoch [9/10], Step [40/147], Loss: 0.3685\n",
      "Epoch [9/10], Step [41/147], Loss: 0.3715\n",
      "Epoch [9/10], Step [42/147], Loss: 0.2258\n",
      "Epoch [9/10], Step [43/147], Loss: 0.3484\n",
      "Epoch [9/10], Step [44/147], Loss: 0.4989\n",
      "Epoch [9/10], Step [45/147], Loss: 0.2387\n",
      "Epoch [9/10], Step [46/147], Loss: 0.1218\n",
      "Epoch [9/10], Step [47/147], Loss: 0.3294\n",
      "Epoch [9/10], Step [48/147], Loss: 0.2166\n",
      "Epoch [9/10], Step [49/147], Loss: 0.2748\n",
      "Epoch [9/10], Step [50/147], Loss: 0.1943\n",
      "Epoch [9/10], Step [51/147], Loss: 0.4943\n",
      "Epoch [9/10], Step [52/147], Loss: 0.3899\n",
      "Epoch [9/10], Step [53/147], Loss: 0.3229\n",
      "Epoch [9/10], Step [54/147], Loss: 0.3232\n",
      "Epoch [9/10], Step [55/147], Loss: 0.4603\n",
      "Epoch [9/10], Step [56/147], Loss: 0.3141\n",
      "Epoch [9/10], Step [57/147], Loss: 0.2867\n",
      "Epoch [9/10], Step [58/147], Loss: 0.2529\n",
      "Epoch [9/10], Step [59/147], Loss: 0.2866\n",
      "Epoch [9/10], Step [60/147], Loss: 0.2440\n",
      "Epoch [9/10], Step [61/147], Loss: 0.2893\n",
      "Epoch [9/10], Step [62/147], Loss: 0.2892\n",
      "Epoch [9/10], Step [63/147], Loss: 0.2896\n",
      "Epoch [9/10], Step [64/147], Loss: 0.2413\n",
      "Epoch [9/10], Step [65/147], Loss: 0.2616\n",
      "Epoch [9/10], Step [66/147], Loss: 0.4329\n",
      "Epoch [9/10], Step [67/147], Loss: 0.3879\n",
      "Epoch [9/10], Step [68/147], Loss: 0.3075\n",
      "Epoch [9/10], Step [69/147], Loss: 0.3174\n",
      "Epoch [9/10], Step [70/147], Loss: 0.2714\n",
      "Epoch [9/10], Step [71/147], Loss: 0.1332\n",
      "Epoch [9/10], Step [72/147], Loss: 0.2927\n",
      "Epoch [9/10], Step [73/147], Loss: 0.2683\n",
      "Epoch [9/10], Step [74/147], Loss: 0.3542\n",
      "Epoch [9/10], Step [75/147], Loss: 0.2846\n",
      "Epoch [9/10], Step [76/147], Loss: 0.2773\n",
      "Epoch [9/10], Step [77/147], Loss: 0.2878\n",
      "Epoch [9/10], Step [78/147], Loss: 0.3748\n",
      "Epoch [9/10], Step [79/147], Loss: 0.3753\n",
      "Epoch [9/10], Step [80/147], Loss: 0.2038\n",
      "Epoch [9/10], Step [81/147], Loss: 0.2986\n",
      "Epoch [9/10], Step [82/147], Loss: 0.3605\n",
      "Epoch [9/10], Step [83/147], Loss: 0.1956\n",
      "Epoch [9/10], Step [84/147], Loss: 0.3508\n",
      "Epoch [9/10], Step [85/147], Loss: 0.5675\n",
      "Epoch [9/10], Step [86/147], Loss: 0.2020\n",
      "Epoch [9/10], Step [87/147], Loss: 0.3437\n",
      "Epoch [9/10], Step [88/147], Loss: 0.3452\n",
      "Epoch [9/10], Step [89/147], Loss: 0.2662\n",
      "Epoch [9/10], Step [90/147], Loss: 0.2908\n",
      "Epoch [9/10], Step [91/147], Loss: 0.2270\n",
      "Epoch [9/10], Step [92/147], Loss: 0.2680\n",
      "Epoch [9/10], Step [93/147], Loss: 0.2372\n",
      "Epoch [9/10], Step [94/147], Loss: 0.2527\n",
      "Epoch [9/10], Step [95/147], Loss: 0.3856\n",
      "Epoch [9/10], Step [96/147], Loss: 0.3146\n",
      "Epoch [9/10], Step [97/147], Loss: 0.2224\n",
      "Epoch [9/10], Step [98/147], Loss: 0.3236\n",
      "Epoch [9/10], Step [99/147], Loss: 0.2239\n",
      "Epoch [9/10], Step [100/147], Loss: 0.2383\n",
      "Epoch [9/10], Step [101/147], Loss: 0.1944\n",
      "Epoch [9/10], Step [102/147], Loss: 0.1958\n",
      "Epoch [9/10], Step [103/147], Loss: 0.3156\n",
      "Epoch [9/10], Step [104/147], Loss: 0.1979\n",
      "Epoch [9/10], Step [105/147], Loss: 0.2650\n",
      "Epoch [9/10], Step [106/147], Loss: 0.3105\n",
      "Epoch [9/10], Step [107/147], Loss: 0.1928\n",
      "Epoch [9/10], Step [108/147], Loss: 0.1988\n",
      "Epoch [9/10], Step [109/147], Loss: 0.3784\n",
      "Epoch [9/10], Step [110/147], Loss: 0.2577\n",
      "Epoch [9/10], Step [111/147], Loss: 0.2251\n",
      "Epoch [9/10], Step [112/147], Loss: 0.2910\n",
      "Epoch [9/10], Step [113/147], Loss: 0.2767\n",
      "Epoch [9/10], Step [114/147], Loss: 0.3012\n",
      "Epoch [9/10], Step [115/147], Loss: 0.2874\n",
      "Epoch [9/10], Step [116/147], Loss: 0.2467\n",
      "Epoch [9/10], Step [117/147], Loss: 0.4180\n",
      "Epoch [9/10], Step [118/147], Loss: 0.2574\n",
      "Epoch [9/10], Step [119/147], Loss: 0.4248\n",
      "Epoch [9/10], Step [120/147], Loss: 0.1776\n",
      "Epoch [9/10], Step [121/147], Loss: 0.3613\n",
      "Epoch [9/10], Step [122/147], Loss: 0.1737\n",
      "Epoch [9/10], Step [123/147], Loss: 0.3314\n",
      "Epoch [9/10], Step [124/147], Loss: 0.2208\n",
      "Epoch [9/10], Step [125/147], Loss: 0.3239\n",
      "Epoch [9/10], Step [126/147], Loss: 0.3548\n",
      "Epoch [9/10], Step [127/147], Loss: 0.4826\n",
      "Epoch [9/10], Step [128/147], Loss: 0.2649\n",
      "Epoch [9/10], Step [129/147], Loss: 0.3001\n",
      "Epoch [9/10], Step [130/147], Loss: 0.2630\n",
      "Epoch [9/10], Step [131/147], Loss: 0.3045\n",
      "Epoch [9/10], Step [132/147], Loss: 0.2701\n",
      "Epoch [9/10], Step [133/147], Loss: 0.3358\n",
      "Epoch [9/10], Step [134/147], Loss: 0.4141\n",
      "Epoch [9/10], Step [135/147], Loss: 0.2084\n",
      "Epoch [9/10], Step [136/147], Loss: 0.2753\n",
      "Epoch [9/10], Step [137/147], Loss: 0.3383\n",
      "Epoch [9/10], Step [138/147], Loss: 0.2196\n",
      "Epoch [9/10], Step [139/147], Loss: 0.3128\n",
      "Epoch [9/10], Step [140/147], Loss: 0.1970\n",
      "Epoch [9/10], Step [141/147], Loss: 0.3512\n",
      "Epoch [9/10], Step [142/147], Loss: 0.2775\n",
      "Epoch [9/10], Step [143/147], Loss: 0.3165\n",
      "Epoch [9/10], Step [144/147], Loss: 0.3572\n",
      "Epoch [9/10], Step [145/147], Loss: 0.2485\n",
      "Epoch [9/10], Step [146/147], Loss: 0.3022\n",
      "Epoch [9/10], Step [147/147], Loss: 0.3596\n",
      "Epoch [10/10], Step [1/147], Loss: 0.6044\n",
      "Epoch [10/10], Step [2/147], Loss: 0.2546\n",
      "Epoch [10/10], Step [3/147], Loss: 0.3028\n",
      "Epoch [10/10], Step [4/147], Loss: 0.2878\n",
      "Epoch [10/10], Step [5/147], Loss: 0.2391\n",
      "Epoch [10/10], Step [6/147], Loss: 0.2275\n",
      "Epoch [10/10], Step [7/147], Loss: 0.3366\n",
      "Epoch [10/10], Step [8/147], Loss: 0.3732\n",
      "Epoch [10/10], Step [9/147], Loss: 0.2826\n",
      "Epoch [10/10], Step [10/147], Loss: 0.2935\n",
      "Epoch [10/10], Step [11/147], Loss: 0.3452\n",
      "Epoch [10/10], Step [12/147], Loss: 0.3748\n",
      "Epoch [10/10], Step [13/147], Loss: 0.2187\n",
      "Epoch [10/10], Step [14/147], Loss: 0.2586\n",
      "Epoch [10/10], Step [15/147], Loss: 0.2892\n",
      "Epoch [10/10], Step [16/147], Loss: 0.1845\n",
      "Epoch [10/10], Step [17/147], Loss: 0.3200\n",
      "Epoch [10/10], Step [18/147], Loss: 0.3073\n",
      "Epoch [10/10], Step [19/147], Loss: 0.4232\n",
      "Epoch [10/10], Step [20/147], Loss: 0.1704\n",
      "Epoch [10/10], Step [21/147], Loss: 0.2770\n",
      "Epoch [10/10], Step [22/147], Loss: 0.3737\n",
      "Epoch [10/10], Step [23/147], Loss: 0.4725\n",
      "Epoch [10/10], Step [24/147], Loss: 0.2592\n",
      "Epoch [10/10], Step [25/147], Loss: 0.1967\n",
      "Epoch [10/10], Step [26/147], Loss: 0.2385\n",
      "Epoch [10/10], Step [27/147], Loss: 0.3760\n",
      "Epoch [10/10], Step [28/147], Loss: 0.2308\n",
      "Epoch [10/10], Step [29/147], Loss: 0.2348\n",
      "Epoch [10/10], Step [30/147], Loss: 0.3068\n",
      "Epoch [10/10], Step [31/147], Loss: 0.2624\n",
      "Epoch [10/10], Step [32/147], Loss: 0.1666\n",
      "Epoch [10/10], Step [33/147], Loss: 0.2456\n",
      "Epoch [10/10], Step [34/147], Loss: 0.3144\n",
      "Epoch [10/10], Step [35/147], Loss: 0.2890\n",
      "Epoch [10/10], Step [36/147], Loss: 0.2457\n",
      "Epoch [10/10], Step [37/147], Loss: 0.1508\n",
      "Epoch [10/10], Step [38/147], Loss: 0.2572\n",
      "Epoch [10/10], Step [39/147], Loss: 0.2464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [40/147], Loss: 0.2278\n",
      "Epoch [10/10], Step [41/147], Loss: 0.1515\n",
      "Epoch [10/10], Step [42/147], Loss: 0.3661\n",
      "Epoch [10/10], Step [43/147], Loss: 0.1914\n",
      "Epoch [10/10], Step [44/147], Loss: 0.3416\n",
      "Epoch [10/10], Step [45/147], Loss: 0.0915\n",
      "Epoch [10/10], Step [46/147], Loss: 0.3056\n",
      "Epoch [10/10], Step [47/147], Loss: 0.2849\n",
      "Epoch [10/10], Step [48/147], Loss: 0.2995\n",
      "Epoch [10/10], Step [49/147], Loss: 0.2099\n",
      "Epoch [10/10], Step [50/147], Loss: 0.1830\n",
      "Epoch [10/10], Step [51/147], Loss: 0.2573\n",
      "Epoch [10/10], Step [52/147], Loss: 0.2101\n",
      "Epoch [10/10], Step [53/147], Loss: 0.4184\n",
      "Epoch [10/10], Step [54/147], Loss: 0.1624\n",
      "Epoch [10/10], Step [55/147], Loss: 0.1997\n",
      "Epoch [10/10], Step [56/147], Loss: 0.2790\n",
      "Epoch [10/10], Step [57/147], Loss: 0.3829\n",
      "Epoch [10/10], Step [58/147], Loss: 0.3639\n",
      "Epoch [10/10], Step [59/147], Loss: 0.2418\n",
      "Epoch [10/10], Step [60/147], Loss: 0.3023\n",
      "Epoch [10/10], Step [61/147], Loss: 0.2195\n",
      "Epoch [10/10], Step [62/147], Loss: 0.2724\n",
      "Epoch [10/10], Step [63/147], Loss: 0.3939\n",
      "Epoch [10/10], Step [64/147], Loss: 0.2987\n",
      "Epoch [10/10], Step [65/147], Loss: 0.3227\n",
      "Epoch [10/10], Step [66/147], Loss: 0.3294\n",
      "Epoch [10/10], Step [67/147], Loss: 0.2861\n",
      "Epoch [10/10], Step [68/147], Loss: 0.2415\n",
      "Epoch [10/10], Step [69/147], Loss: 0.3016\n",
      "Epoch [10/10], Step [70/147], Loss: 0.2079\n",
      "Epoch [10/10], Step [71/147], Loss: 0.3320\n",
      "Epoch [10/10], Step [72/147], Loss: 0.4591\n",
      "Epoch [10/10], Step [73/147], Loss: 0.1794\n",
      "Epoch [10/10], Step [74/147], Loss: 0.2357\n",
      "Epoch [10/10], Step [75/147], Loss: 0.3039\n",
      "Epoch [10/10], Step [76/147], Loss: 0.2041\n",
      "Epoch [10/10], Step [77/147], Loss: 0.3824\n",
      "Epoch [10/10], Step [78/147], Loss: 0.2173\n",
      "Epoch [10/10], Step [79/147], Loss: 0.3179\n",
      "Epoch [10/10], Step [80/147], Loss: 0.2714\n",
      "Epoch [10/10], Step [81/147], Loss: 0.3027\n",
      "Epoch [10/10], Step [82/147], Loss: 0.4167\n",
      "Epoch [10/10], Step [83/147], Loss: 0.3142\n",
      "Epoch [10/10], Step [84/147], Loss: 0.1395\n",
      "Epoch [10/10], Step [85/147], Loss: 0.3967\n",
      "Epoch [10/10], Step [86/147], Loss: 0.2064\n",
      "Epoch [10/10], Step [87/147], Loss: 0.3242\n",
      "Epoch [10/10], Step [88/147], Loss: 0.2430\n",
      "Epoch [10/10], Step [89/147], Loss: 0.3398\n",
      "Epoch [10/10], Step [90/147], Loss: 0.2993\n",
      "Epoch [10/10], Step [91/147], Loss: 0.3859\n",
      "Epoch [10/10], Step [92/147], Loss: 0.2647\n",
      "Epoch [10/10], Step [93/147], Loss: 0.3002\n",
      "Epoch [10/10], Step [94/147], Loss: 0.2018\n",
      "Epoch [10/10], Step [95/147], Loss: 0.2165\n",
      "Epoch [10/10], Step [96/147], Loss: 0.3432\n",
      "Epoch [10/10], Step [97/147], Loss: 0.1716\n",
      "Epoch [10/10], Step [98/147], Loss: 0.2406\n",
      "Epoch [10/10], Step [99/147], Loss: 0.2541\n",
      "Epoch [10/10], Step [100/147], Loss: 0.2737\n",
      "Epoch [10/10], Step [101/147], Loss: 0.2445\n",
      "Epoch [10/10], Step [102/147], Loss: 0.1680\n",
      "Epoch [10/10], Step [103/147], Loss: 0.2122\n",
      "Epoch [10/10], Step [104/147], Loss: 0.4090\n",
      "Epoch [10/10], Step [105/147], Loss: 0.2721\n",
      "Epoch [10/10], Step [106/147], Loss: 0.4866\n",
      "Epoch [10/10], Step [107/147], Loss: 0.2431\n",
      "Epoch [10/10], Step [108/147], Loss: 0.2307\n",
      "Epoch [10/10], Step [109/147], Loss: 0.4471\n",
      "Epoch [10/10], Step [110/147], Loss: 0.3424\n",
      "Epoch [10/10], Step [111/147], Loss: 0.3634\n",
      "Epoch [10/10], Step [112/147], Loss: 0.2121\n",
      "Epoch [10/10], Step [113/147], Loss: 0.2136\n",
      "Epoch [10/10], Step [114/147], Loss: 0.3061\n",
      "Epoch [10/10], Step [115/147], Loss: 0.3305\n",
      "Epoch [10/10], Step [116/147], Loss: 0.2817\n",
      "Epoch [10/10], Step [117/147], Loss: 0.3140\n",
      "Epoch [10/10], Step [118/147], Loss: 0.3472\n",
      "Epoch [10/10], Step [119/147], Loss: 0.2304\n",
      "Epoch [10/10], Step [120/147], Loss: 0.3016\n",
      "Epoch [10/10], Step [121/147], Loss: 0.3989\n",
      "Epoch [10/10], Step [122/147], Loss: 0.2265\n",
      "Epoch [10/10], Step [123/147], Loss: 0.3914\n",
      "Epoch [10/10], Step [124/147], Loss: 0.2759\n",
      "Epoch [10/10], Step [125/147], Loss: 0.2090\n",
      "Epoch [10/10], Step [126/147], Loss: 0.2746\n",
      "Epoch [10/10], Step [127/147], Loss: 0.3800\n",
      "Epoch [10/10], Step [128/147], Loss: 0.2499\n",
      "Epoch [10/10], Step [129/147], Loss: 0.3981\n",
      "Epoch [10/10], Step [130/147], Loss: 0.3331\n",
      "Epoch [10/10], Step [131/147], Loss: 0.2233\n",
      "Epoch [10/10], Step [132/147], Loss: 0.5429\n",
      "Epoch [10/10], Step [133/147], Loss: 0.4642\n",
      "Epoch [10/10], Step [134/147], Loss: 0.3551\n",
      "Epoch [10/10], Step [135/147], Loss: 0.2767\n",
      "Epoch [10/10], Step [136/147], Loss: 0.3919\n",
      "Epoch [10/10], Step [137/147], Loss: 0.3886\n",
      "Epoch [10/10], Step [138/147], Loss: 0.4228\n",
      "Epoch [10/10], Step [139/147], Loss: 0.2549\n",
      "Epoch [10/10], Step [140/147], Loss: 0.4129\n",
      "Epoch [10/10], Step [141/147], Loss: 0.2311\n",
      "Epoch [10/10], Step [142/147], Loss: 0.3377\n",
      "Epoch [10/10], Step [143/147], Loss: 0.4235\n",
      "Epoch [10/10], Step [144/147], Loss: 0.2059\n",
      "Epoch [10/10], Step [145/147], Loss: 0.3100\n",
      "Epoch [10/10], Step [146/147], Loss: 0.2130\n",
      "Epoch [10/10], Step [147/147], Loss: 0.3097\n"
     ]
    }
   ],
   "source": [
    "loss_all=[]\n",
    "total_step = len(train_loader)\n",
    "for learning_rate in [0.1, 0.01,0.001,0.0001]:\n",
    "    loss_item=[]\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 1== 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "            loss_item.append(loss.item())\n",
    "loss_all.append(loss_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1470, 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0ef59c813d12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_all_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_all_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_all_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.01'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_all_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.001'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_all_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.0001'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "loss_all_array=np.array(loss_all)\n",
    "print(loss_all_array.T.shape)\n",
    "plt.plot(loss_all_array.T[:,0],label='0.1')\n",
    "plt.plot(loss_all_array.T[:,1],label='0.01')\n",
    "plt.plot(loss_all_array.T[:,2],label='0.001')\n",
    "plt.plot(loss_all_array.T[:,3],label='0.0001')\n",
    "plt.ylabel('loss',fontsize=16)\n",
    "plt.xlabel('iter',fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 89.19054801407742 %\n"
     ]
    }
   ],
   "source": [
    "# Test the modely_test=labels_all\n",
    "\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_all=[]\n",
    "    labels_all=[]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels_all=labels_all+labels.cpu().numpy().tolist()\n",
    "        predicted_all=predicted_all+predicted.cpu().numpy().tolist()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = labels_all\n",
    "y_pred = predicted_all\n",
    "predicted_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "con_mat = confusion_matrix(y_test, y_pred)\n",
    "con_mat_norm = con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis]     # 归一化\n",
    "con_mat_norm = np.around(con_mat_norm, decimals=2)\n",
    "# === plot ===\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(con_mat_norm, annot=True, cmap='Blues')\n",
    "plt.ylim(0,4)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: tensorboardX in d:\\anaconda\\lib\\site-packages (2.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in d:\\anaconda\\lib\\site-packages (from tensorboardX) (3.20.1)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from tensorboardX) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\anaconda\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from tensorboardX) (1.21.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --coding:utf-8--\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from tensorboardX import SummaryWriter\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "# 冻结卷积层\n",
    "for name, par in model_ft.named_parameters():\n",
    "    if 'features' in name:\n",
    "        par.requires_grad = False\n",
    "\n",
    "#修改几层\n",
    "model_ft.classifier[6] = nn.Linear(4096, 4)\n",
    "model_ft = model_ft.to(device)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "optimizer_ft = optim.SGD(lr=learning_rate, params=filter(lambda p: p.requires_grad, model_ft.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/147], Loss: 1.4379\n",
      "Epoch [1/10], Step [2/147], Loss: 1.3997\n",
      "Epoch [1/10], Step [3/147], Loss: 1.4790\n",
      "Epoch [1/10], Step [4/147], Loss: 1.4695\n",
      "Epoch [1/10], Step [5/147], Loss: 1.4643\n",
      "Epoch [1/10], Step [6/147], Loss: 1.4831\n",
      "Epoch [1/10], Step [7/147], Loss: 1.4472\n",
      "Epoch [1/10], Step [8/147], Loss: 1.4348\n",
      "Epoch [1/10], Step [9/147], Loss: 1.4378\n",
      "Epoch [1/10], Step [10/147], Loss: 1.4314\n",
      "Epoch [1/10], Step [11/147], Loss: 1.4795\n",
      "Epoch [1/10], Step [12/147], Loss: 1.3839\n",
      "Epoch [1/10], Step [13/147], Loss: 1.4646\n",
      "Epoch [1/10], Step [14/147], Loss: 1.4742\n",
      "Epoch [1/10], Step [15/147], Loss: 1.3748\n",
      "Epoch [1/10], Step [16/147], Loss: 1.4302\n",
      "Epoch [1/10], Step [17/147], Loss: 1.3835\n",
      "Epoch [1/10], Step [18/147], Loss: 1.3999\n",
      "Epoch [1/10], Step [19/147], Loss: 1.4350\n",
      "Epoch [1/10], Step [20/147], Loss: 1.4008\n",
      "Epoch [1/10], Step [21/147], Loss: 1.4115\n",
      "Epoch [1/10], Step [22/147], Loss: 1.3735\n",
      "Epoch [1/10], Step [23/147], Loss: 1.3876\n",
      "Epoch [1/10], Step [24/147], Loss: 1.4647\n",
      "Epoch [1/10], Step [25/147], Loss: 1.3822\n",
      "Epoch [1/10], Step [26/147], Loss: 1.4938\n",
      "Epoch [1/10], Step [27/147], Loss: 1.3707\n",
      "Epoch [1/10], Step [28/147], Loss: 1.3125\n",
      "Epoch [1/10], Step [29/147], Loss: 1.3865\n",
      "Epoch [1/10], Step [30/147], Loss: 1.4482\n",
      "Epoch [1/10], Step [31/147], Loss: 1.3533\n",
      "Epoch [1/10], Step [32/147], Loss: 1.2680\n",
      "Epoch [1/10], Step [33/147], Loss: 1.3623\n",
      "Epoch [1/10], Step [34/147], Loss: 1.4031\n",
      "Epoch [1/10], Step [35/147], Loss: 1.3443\n",
      "Epoch [1/10], Step [36/147], Loss: 1.4206\n",
      "Epoch [1/10], Step [37/147], Loss: 1.3897\n",
      "Epoch [1/10], Step [38/147], Loss: 1.3902\n",
      "Epoch [1/10], Step [39/147], Loss: 1.3603\n",
      "Epoch [1/10], Step [40/147], Loss: 1.3827\n",
      "Epoch [1/10], Step [41/147], Loss: 1.3140\n",
      "Epoch [1/10], Step [42/147], Loss: 1.2642\n",
      "Epoch [1/10], Step [43/147], Loss: 1.3157\n",
      "Epoch [1/10], Step [44/147], Loss: 1.3160\n",
      "Epoch [1/10], Step [45/147], Loss: 1.2344\n",
      "Epoch [1/10], Step [46/147], Loss: 1.3759\n",
      "Epoch [1/10], Step [47/147], Loss: 1.2476\n",
      "Epoch [1/10], Step [48/147], Loss: 1.3687\n",
      "Epoch [1/10], Step [49/147], Loss: 1.3378\n",
      "Epoch [1/10], Step [50/147], Loss: 1.2708\n",
      "Epoch [1/10], Step [51/147], Loss: 1.2584\n",
      "Epoch [1/10], Step [52/147], Loss: 1.3642\n",
      "Epoch [1/10], Step [53/147], Loss: 1.2954\n",
      "Epoch [1/10], Step [54/147], Loss: 1.3540\n",
      "Epoch [1/10], Step [55/147], Loss: 1.3184\n",
      "Epoch [1/10], Step [56/147], Loss: 1.2808\n",
      "Epoch [1/10], Step [57/147], Loss: 1.3153\n",
      "Epoch [1/10], Step [58/147], Loss: 1.3062\n",
      "Epoch [1/10], Step [59/147], Loss: 1.2917\n",
      "Epoch [1/10], Step [60/147], Loss: 1.2995\n",
      "Epoch [1/10], Step [61/147], Loss: 1.2272\n",
      "Epoch [1/10], Step [62/147], Loss: 1.3417\n",
      "Epoch [1/10], Step [63/147], Loss: 1.2546\n",
      "Epoch [1/10], Step [64/147], Loss: 1.2953\n",
      "Epoch [1/10], Step [65/147], Loss: 1.3203\n",
      "Epoch [1/10], Step [66/147], Loss: 1.2460\n",
      "Epoch [1/10], Step [67/147], Loss: 1.3450\n",
      "Epoch [1/10], Step [68/147], Loss: 1.3425\n",
      "Epoch [1/10], Step [69/147], Loss: 1.2556\n",
      "Epoch [1/10], Step [70/147], Loss: 1.2618\n",
      "Epoch [1/10], Step [71/147], Loss: 1.2880\n",
      "Epoch [1/10], Step [72/147], Loss: 1.2119\n",
      "Epoch [1/10], Step [73/147], Loss: 1.2114\n",
      "Epoch [1/10], Step [74/147], Loss: 1.3361\n",
      "Epoch [1/10], Step [75/147], Loss: 1.2817\n",
      "Epoch [1/10], Step [76/147], Loss: 1.2407\n",
      "Epoch [1/10], Step [77/147], Loss: 1.2708\n",
      "Epoch [1/10], Step [78/147], Loss: 1.2405\n",
      "Epoch [1/10], Step [79/147], Loss: 1.2124\n",
      "Epoch [1/10], Step [80/147], Loss: 1.2858\n",
      "Epoch [1/10], Step [81/147], Loss: 1.2388\n",
      "Epoch [1/10], Step [82/147], Loss: 1.2356\n",
      "Epoch [1/10], Step [83/147], Loss: 1.2335\n",
      "Epoch [1/10], Step [84/147], Loss: 1.2679\n",
      "Epoch [1/10], Step [85/147], Loss: 1.1771\n",
      "Epoch [1/10], Step [86/147], Loss: 1.2127\n",
      "Epoch [1/10], Step [87/147], Loss: 1.2062\n",
      "Epoch [1/10], Step [88/147], Loss: 1.2529\n",
      "Epoch [1/10], Step [89/147], Loss: 1.1934\n",
      "Epoch [1/10], Step [90/147], Loss: 1.2519\n",
      "Epoch [1/10], Step [91/147], Loss: 1.2682\n",
      "Epoch [1/10], Step [92/147], Loss: 1.2914\n",
      "Epoch [1/10], Step [93/147], Loss: 1.2519\n",
      "Epoch [1/10], Step [94/147], Loss: 1.1928\n",
      "Epoch [1/10], Step [95/147], Loss: 1.2312\n",
      "Epoch [1/10], Step [96/147], Loss: 1.2442\n",
      "Epoch [1/10], Step [97/147], Loss: 1.1723\n",
      "Epoch [1/10], Step [98/147], Loss: 1.1995\n",
      "Epoch [1/10], Step [99/147], Loss: 1.1974\n",
      "Epoch [1/10], Step [100/147], Loss: 1.2041\n",
      "Epoch [1/10], Step [101/147], Loss: 1.1885\n",
      "Epoch [1/10], Step [102/147], Loss: 1.1421\n",
      "Epoch [1/10], Step [103/147], Loss: 1.1979\n",
      "Epoch [1/10], Step [104/147], Loss: 1.1853\n",
      "Epoch [1/10], Step [105/147], Loss: 1.1423\n",
      "Epoch [1/10], Step [106/147], Loss: 1.1866\n",
      "Epoch [1/10], Step [107/147], Loss: 1.1371\n",
      "Epoch [1/10], Step [108/147], Loss: 1.2398\n",
      "Epoch [1/10], Step [109/147], Loss: 1.1844\n",
      "Epoch [1/10], Step [110/147], Loss: 1.1889\n",
      "Epoch [1/10], Step [111/147], Loss: 1.1946\n",
      "Epoch [1/10], Step [112/147], Loss: 1.1647\n",
      "Epoch [1/10], Step [113/147], Loss: 1.1378\n",
      "Epoch [1/10], Step [114/147], Loss: 1.1536\n",
      "Epoch [1/10], Step [115/147], Loss: 1.2308\n",
      "Epoch [1/10], Step [116/147], Loss: 1.2193\n",
      "Epoch [1/10], Step [117/147], Loss: 1.1054\n",
      "Epoch [1/10], Step [118/147], Loss: 1.1525\n",
      "Epoch [1/10], Step [119/147], Loss: 1.1745\n",
      "Epoch [1/10], Step [120/147], Loss: 1.1406\n",
      "Epoch [1/10], Step [121/147], Loss: 1.1507\n",
      "Epoch [1/10], Step [122/147], Loss: 1.1378\n",
      "Epoch [1/10], Step [123/147], Loss: 1.1713\n",
      "Epoch [1/10], Step [124/147], Loss: 1.1512\n",
      "Epoch [1/10], Step [125/147], Loss: 1.1428\n",
      "Epoch [1/10], Step [126/147], Loss: 1.1369\n",
      "Epoch [1/10], Step [127/147], Loss: 1.1683\n",
      "Epoch [1/10], Step [128/147], Loss: 1.2079\n",
      "Epoch [1/10], Step [129/147], Loss: 1.1634\n",
      "Epoch [1/10], Step [130/147], Loss: 1.1008\n",
      "Epoch [1/10], Step [131/147], Loss: 1.1218\n",
      "Epoch [1/10], Step [132/147], Loss: 1.1472\n",
      "Epoch [1/10], Step [133/147], Loss: 1.1083\n",
      "Epoch [1/10], Step [134/147], Loss: 1.1424\n",
      "Epoch [1/10], Step [135/147], Loss: 1.1174\n",
      "Epoch [1/10], Step [136/147], Loss: 1.1416\n",
      "Epoch [1/10], Step [137/147], Loss: 1.1072\n",
      "Epoch [1/10], Step [138/147], Loss: 1.1107\n",
      "Epoch [1/10], Step [139/147], Loss: 1.0635\n",
      "Epoch [1/10], Step [140/147], Loss: 1.1005\n",
      "Epoch [1/10], Step [141/147], Loss: 1.1491\n",
      "Epoch [1/10], Step [142/147], Loss: 1.1069\n",
      "Epoch [1/10], Step [143/147], Loss: 1.0921\n",
      "Epoch [1/10], Step [144/147], Loss: 1.1454\n",
      "Epoch [1/10], Step [145/147], Loss: 1.0532\n",
      "Epoch [1/10], Step [146/147], Loss: 1.1452\n",
      "Epoch [1/10], Step [147/147], Loss: 1.1374\n",
      "Epoch [2/10], Step [1/147], Loss: 1.1628\n",
      "Epoch [2/10], Step [2/147], Loss: 1.0187\n",
      "Epoch [2/10], Step [3/147], Loss: 1.0914\n",
      "Epoch [2/10], Step [4/147], Loss: 1.1176\n",
      "Epoch [2/10], Step [5/147], Loss: 1.0478\n",
      "Epoch [2/10], Step [6/147], Loss: 1.1088\n",
      "Epoch [2/10], Step [7/147], Loss: 1.1233\n",
      "Epoch [2/10], Step [8/147], Loss: 1.0530\n",
      "Epoch [2/10], Step [9/147], Loss: 1.0022\n",
      "Epoch [2/10], Step [10/147], Loss: 1.0756\n",
      "Epoch [2/10], Step [11/147], Loss: 1.0663\n",
      "Epoch [2/10], Step [12/147], Loss: 1.0368\n",
      "Epoch [2/10], Step [13/147], Loss: 1.1012\n",
      "Epoch [2/10], Step [14/147], Loss: 1.0686\n",
      "Epoch [2/10], Step [15/147], Loss: 1.0637\n",
      "Epoch [2/10], Step [16/147], Loss: 1.0892\n",
      "Epoch [2/10], Step [17/147], Loss: 1.0758\n",
      "Epoch [2/10], Step [18/147], Loss: 1.0457\n",
      "Epoch [2/10], Step [19/147], Loss: 1.1600\n",
      "Epoch [2/10], Step [20/147], Loss: 1.0432\n",
      "Epoch [2/10], Step [21/147], Loss: 0.9904\n",
      "Epoch [2/10], Step [22/147], Loss: 1.0792\n",
      "Epoch [2/10], Step [23/147], Loss: 1.0475\n",
      "Epoch [2/10], Step [24/147], Loss: 1.1141\n",
      "Epoch [2/10], Step [25/147], Loss: 1.0695\n",
      "Epoch [2/10], Step [26/147], Loss: 1.0430\n",
      "Epoch [2/10], Step [27/147], Loss: 1.0849\n",
      "Epoch [2/10], Step [28/147], Loss: 1.0998\n",
      "Epoch [2/10], Step [29/147], Loss: 1.0195\n",
      "Epoch [2/10], Step [30/147], Loss: 1.0383\n",
      "Epoch [2/10], Step [31/147], Loss: 1.0072\n",
      "Epoch [2/10], Step [32/147], Loss: 1.1248\n",
      "Epoch [2/10], Step [33/147], Loss: 1.0529\n",
      "Epoch [2/10], Step [34/147], Loss: 1.0654\n",
      "Epoch [2/10], Step [35/147], Loss: 1.0243\n",
      "Epoch [2/10], Step [36/147], Loss: 1.0468\n",
      "Epoch [2/10], Step [37/147], Loss: 1.0312\n",
      "Epoch [2/10], Step [38/147], Loss: 1.0397\n",
      "Epoch [2/10], Step [39/147], Loss: 1.0510\n",
      "Epoch [2/10], Step [40/147], Loss: 1.0511\n",
      "Epoch [2/10], Step [41/147], Loss: 1.1018\n",
      "Epoch [2/10], Step [42/147], Loss: 0.9875\n",
      "Epoch [2/10], Step [43/147], Loss: 1.0696\n",
      "Epoch [2/10], Step [44/147], Loss: 1.0116\n",
      "Epoch [2/10], Step [45/147], Loss: 1.0622\n",
      "Epoch [2/10], Step [46/147], Loss: 0.9927\n",
      "Epoch [2/10], Step [47/147], Loss: 0.9933\n",
      "Epoch [2/10], Step [48/147], Loss: 1.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [49/147], Loss: 0.9149\n",
      "Epoch [2/10], Step [50/147], Loss: 1.0498\n",
      "Epoch [2/10], Step [51/147], Loss: 1.0218\n",
      "Epoch [2/10], Step [52/147], Loss: 1.0635\n",
      "Epoch [2/10], Step [53/147], Loss: 1.0249\n",
      "Epoch [2/10], Step [54/147], Loss: 1.0145\n",
      "Epoch [2/10], Step [55/147], Loss: 0.9946\n",
      "Epoch [2/10], Step [56/147], Loss: 1.0064\n",
      "Epoch [2/10], Step [57/147], Loss: 0.9871\n",
      "Epoch [2/10], Step [58/147], Loss: 1.0223\n",
      "Epoch [2/10], Step [59/147], Loss: 1.0054\n",
      "Epoch [2/10], Step [60/147], Loss: 1.0416\n",
      "Epoch [2/10], Step [61/147], Loss: 1.0150\n",
      "Epoch [2/10], Step [62/147], Loss: 0.9796\n",
      "Epoch [2/10], Step [63/147], Loss: 0.9928\n",
      "Epoch [2/10], Step [64/147], Loss: 1.0534\n",
      "Epoch [2/10], Step [65/147], Loss: 0.9405\n",
      "Epoch [2/10], Step [66/147], Loss: 0.9305\n",
      "Epoch [2/10], Step [67/147], Loss: 0.9470\n",
      "Epoch [2/10], Step [68/147], Loss: 0.9963\n",
      "Epoch [2/10], Step [69/147], Loss: 1.0163\n",
      "Epoch [2/10], Step [70/147], Loss: 0.9784\n",
      "Epoch [2/10], Step [71/147], Loss: 0.9735\n",
      "Epoch [2/10], Step [72/147], Loss: 0.9438\n",
      "Epoch [2/10], Step [73/147], Loss: 0.9694\n",
      "Epoch [2/10], Step [74/147], Loss: 0.9395\n",
      "Epoch [2/10], Step [75/147], Loss: 0.9049\n",
      "Epoch [2/10], Step [76/147], Loss: 0.9824\n",
      "Epoch [2/10], Step [77/147], Loss: 0.9381\n",
      "Epoch [2/10], Step [78/147], Loss: 0.9985\n",
      "Epoch [2/10], Step [79/147], Loss: 0.9785\n",
      "Epoch [2/10], Step [80/147], Loss: 1.0027\n",
      "Epoch [2/10], Step [81/147], Loss: 0.9318\n",
      "Epoch [2/10], Step [82/147], Loss: 1.0156\n",
      "Epoch [2/10], Step [83/147], Loss: 1.0299\n",
      "Epoch [2/10], Step [84/147], Loss: 1.0048\n",
      "Epoch [2/10], Step [85/147], Loss: 0.8915\n",
      "Epoch [2/10], Step [86/147], Loss: 0.9327\n",
      "Epoch [2/10], Step [87/147], Loss: 0.8728\n",
      "Epoch [2/10], Step [88/147], Loss: 0.9965\n",
      "Epoch [2/10], Step [89/147], Loss: 0.9075\n",
      "Epoch [2/10], Step [90/147], Loss: 0.8528\n",
      "Epoch [2/10], Step [91/147], Loss: 0.9979\n",
      "Epoch [2/10], Step [92/147], Loss: 0.8915\n",
      "Epoch [2/10], Step [93/147], Loss: 0.9630\n",
      "Epoch [2/10], Step [94/147], Loss: 0.9802\n",
      "Epoch [2/10], Step [95/147], Loss: 0.9870\n",
      "Epoch [2/10], Step [96/147], Loss: 0.9359\n",
      "Epoch [2/10], Step [97/147], Loss: 0.9706\n",
      "Epoch [2/10], Step [98/147], Loss: 0.9146\n",
      "Epoch [2/10], Step [99/147], Loss: 0.9696\n",
      "Epoch [2/10], Step [100/147], Loss: 0.9217\n",
      "Epoch [2/10], Step [101/147], Loss: 0.8875\n",
      "Epoch [2/10], Step [102/147], Loss: 0.8942\n",
      "Epoch [2/10], Step [103/147], Loss: 0.9510\n",
      "Epoch [2/10], Step [104/147], Loss: 0.9460\n",
      "Epoch [2/10], Step [105/147], Loss: 0.9410\n",
      "Epoch [2/10], Step [106/147], Loss: 0.8755\n",
      "Epoch [2/10], Step [107/147], Loss: 0.9015\n",
      "Epoch [2/10], Step [108/147], Loss: 0.9550\n",
      "Epoch [2/10], Step [109/147], Loss: 0.9626\n",
      "Epoch [2/10], Step [110/147], Loss: 0.9032\n",
      "Epoch [2/10], Step [111/147], Loss: 0.9139\n",
      "Epoch [2/10], Step [112/147], Loss: 0.9418\n",
      "Epoch [2/10], Step [113/147], Loss: 0.8756\n",
      "Epoch [2/10], Step [114/147], Loss: 0.9438\n",
      "Epoch [2/10], Step [115/147], Loss: 0.9272\n",
      "Epoch [2/10], Step [116/147], Loss: 0.8626\n",
      "Epoch [2/10], Step [117/147], Loss: 0.9086\n",
      "Epoch [2/10], Step [118/147], Loss: 0.9205\n",
      "Epoch [2/10], Step [119/147], Loss: 0.8454\n",
      "Epoch [2/10], Step [120/147], Loss: 0.8636\n",
      "Epoch [2/10], Step [121/147], Loss: 0.8883\n",
      "Epoch [2/10], Step [122/147], Loss: 0.9127\n",
      "Epoch [2/10], Step [123/147], Loss: 0.8718\n",
      "Epoch [2/10], Step [124/147], Loss: 0.8221\n",
      "Epoch [2/10], Step [125/147], Loss: 0.9000\n",
      "Epoch [2/10], Step [126/147], Loss: 0.8655\n",
      "Epoch [2/10], Step [127/147], Loss: 0.8957\n",
      "Epoch [2/10], Step [128/147], Loss: 0.9487\n",
      "Epoch [2/10], Step [129/147], Loss: 0.8683\n",
      "Epoch [2/10], Step [130/147], Loss: 0.9426\n",
      "Epoch [2/10], Step [131/147], Loss: 0.8524\n",
      "Epoch [2/10], Step [132/147], Loss: 0.8670\n",
      "Epoch [2/10], Step [133/147], Loss: 0.9448\n",
      "Epoch [2/10], Step [134/147], Loss: 0.8474\n",
      "Epoch [2/10], Step [135/147], Loss: 0.9101\n",
      "Epoch [2/10], Step [136/147], Loss: 0.8630\n",
      "Epoch [2/10], Step [137/147], Loss: 0.9149\n",
      "Epoch [2/10], Step [138/147], Loss: 0.9379\n",
      "Epoch [2/10], Step [139/147], Loss: 0.8531\n",
      "Epoch [2/10], Step [140/147], Loss: 0.8364\n",
      "Epoch [2/10], Step [141/147], Loss: 0.8792\n",
      "Epoch [2/10], Step [142/147], Loss: 0.9020\n",
      "Epoch [2/10], Step [143/147], Loss: 0.9290\n",
      "Epoch [2/10], Step [144/147], Loss: 0.8824\n",
      "Epoch [2/10], Step [145/147], Loss: 0.9163\n",
      "Epoch [2/10], Step [146/147], Loss: 0.8599\n",
      "Epoch [2/10], Step [147/147], Loss: 0.8676\n",
      "Epoch [3/10], Step [1/147], Loss: 0.8261\n",
      "Epoch [3/10], Step [2/147], Loss: 0.8317\n",
      "Epoch [3/10], Step [3/147], Loss: 0.8574\n",
      "Epoch [3/10], Step [4/147], Loss: 0.8658\n",
      "Epoch [3/10], Step [5/147], Loss: 0.8906\n",
      "Epoch [3/10], Step [6/147], Loss: 0.8320\n",
      "Epoch [3/10], Step [7/147], Loss: 0.8586\n",
      "Epoch [3/10], Step [8/147], Loss: 0.8631\n",
      "Epoch [3/10], Step [9/147], Loss: 0.8592\n",
      "Epoch [3/10], Step [10/147], Loss: 0.8101\n",
      "Epoch [3/10], Step [11/147], Loss: 0.8580\n",
      "Epoch [3/10], Step [12/147], Loss: 0.8418\n",
      "Epoch [3/10], Step [13/147], Loss: 0.8493\n",
      "Epoch [3/10], Step [14/147], Loss: 0.8717\n",
      "Epoch [3/10], Step [15/147], Loss: 0.7937\n",
      "Epoch [3/10], Step [16/147], Loss: 0.8957\n",
      "Epoch [3/10], Step [17/147], Loss: 0.8752\n",
      "Epoch [3/10], Step [18/147], Loss: 0.8273\n",
      "Epoch [3/10], Step [19/147], Loss: 0.8354\n",
      "Epoch [3/10], Step [20/147], Loss: 0.8344\n",
      "Epoch [3/10], Step [21/147], Loss: 0.8748\n",
      "Epoch [3/10], Step [22/147], Loss: 0.8376\n",
      "Epoch [3/10], Step [23/147], Loss: 0.8552\n",
      "Epoch [3/10], Step [24/147], Loss: 0.8360\n",
      "Epoch [3/10], Step [25/147], Loss: 0.8015\n",
      "Epoch [3/10], Step [26/147], Loss: 0.8645\n",
      "Epoch [3/10], Step [27/147], Loss: 0.8361\n",
      "Epoch [3/10], Step [28/147], Loss: 0.8331\n",
      "Epoch [3/10], Step [29/147], Loss: 0.8486\n",
      "Epoch [3/10], Step [30/147], Loss: 0.8158\n",
      "Epoch [3/10], Step [31/147], Loss: 0.8727\n",
      "Epoch [3/10], Step [32/147], Loss: 0.8633\n",
      "Epoch [3/10], Step [33/147], Loss: 0.8365\n",
      "Epoch [3/10], Step [34/147], Loss: 0.8817\n",
      "Epoch [3/10], Step [35/147], Loss: 0.8381\n",
      "Epoch [3/10], Step [36/147], Loss: 0.8678\n",
      "Epoch [3/10], Step [37/147], Loss: 0.8635\n",
      "Epoch [3/10], Step [38/147], Loss: 0.7870\n",
      "Epoch [3/10], Step [39/147], Loss: 0.8062\n",
      "Epoch [3/10], Step [40/147], Loss: 0.7360\n",
      "Epoch [3/10], Step [41/147], Loss: 0.8042\n",
      "Epoch [3/10], Step [42/147], Loss: 0.8303\n",
      "Epoch [3/10], Step [43/147], Loss: 0.7941\n",
      "Epoch [3/10], Step [44/147], Loss: 0.8009\n",
      "Epoch [3/10], Step [45/147], Loss: 0.7945\n",
      "Epoch [3/10], Step [46/147], Loss: 0.8725\n",
      "Epoch [3/10], Step [47/147], Loss: 0.8624\n",
      "Epoch [3/10], Step [48/147], Loss: 0.7668\n",
      "Epoch [3/10], Step [49/147], Loss: 0.8877\n",
      "Epoch [3/10], Step [50/147], Loss: 0.8482\n",
      "Epoch [3/10], Step [51/147], Loss: 0.7371\n",
      "Epoch [3/10], Step [52/147], Loss: 0.8427\n",
      "Epoch [3/10], Step [53/147], Loss: 0.7922\n",
      "Epoch [3/10], Step [54/147], Loss: 0.7766\n",
      "Epoch [3/10], Step [55/147], Loss: 0.7805\n",
      "Epoch [3/10], Step [56/147], Loss: 0.8169\n",
      "Epoch [3/10], Step [57/147], Loss: 0.7485\n",
      "Epoch [3/10], Step [58/147], Loss: 0.8010\n",
      "Epoch [3/10], Step [59/147], Loss: 0.8183\n",
      "Epoch [3/10], Step [60/147], Loss: 0.7928\n",
      "Epoch [3/10], Step [61/147], Loss: 0.7775\n",
      "Epoch [3/10], Step [62/147], Loss: 0.7860\n",
      "Epoch [3/10], Step [63/147], Loss: 0.7658\n",
      "Epoch [3/10], Step [64/147], Loss: 0.7659\n",
      "Epoch [3/10], Step [65/147], Loss: 0.8064\n",
      "Epoch [3/10], Step [66/147], Loss: 0.7645\n",
      "Epoch [3/10], Step [67/147], Loss: 0.8265\n",
      "Epoch [3/10], Step [68/147], Loss: 0.7689\n",
      "Epoch [3/10], Step [69/147], Loss: 0.8669\n",
      "Epoch [3/10], Step [70/147], Loss: 0.7719\n",
      "Epoch [3/10], Step [71/147], Loss: 0.7746\n",
      "Epoch [3/10], Step [72/147], Loss: 0.8528\n",
      "Epoch [3/10], Step [73/147], Loss: 0.7676\n",
      "Epoch [3/10], Step [74/147], Loss: 0.8219\n",
      "Epoch [3/10], Step [75/147], Loss: 0.8879\n",
      "Epoch [3/10], Step [76/147], Loss: 0.7754\n",
      "Epoch [3/10], Step [77/147], Loss: 0.8348\n",
      "Epoch [3/10], Step [78/147], Loss: 0.7915\n",
      "Epoch [3/10], Step [79/147], Loss: 0.7820\n",
      "Epoch [3/10], Step [80/147], Loss: 0.7624\n",
      "Epoch [3/10], Step [81/147], Loss: 0.8300\n",
      "Epoch [3/10], Step [82/147], Loss: 0.8204\n",
      "Epoch [3/10], Step [83/147], Loss: 0.7807\n",
      "Epoch [3/10], Step [84/147], Loss: 0.7750\n",
      "Epoch [3/10], Step [85/147], Loss: 0.7618\n",
      "Epoch [3/10], Step [86/147], Loss: 0.7316\n",
      "Epoch [3/10], Step [87/147], Loss: 0.8787\n",
      "Epoch [3/10], Step [88/147], Loss: 0.8124\n",
      "Epoch [3/10], Step [89/147], Loss: 0.8661\n",
      "Epoch [3/10], Step [90/147], Loss: 0.7614\n",
      "Epoch [3/10], Step [91/147], Loss: 0.7627\n",
      "Epoch [3/10], Step [92/147], Loss: 0.7500\n",
      "Epoch [3/10], Step [93/147], Loss: 0.7841\n",
      "Epoch [3/10], Step [94/147], Loss: 0.7186\n",
      "Epoch [3/10], Step [95/147], Loss: 0.7515\n",
      "Epoch [3/10], Step [96/147], Loss: 0.7686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [97/147], Loss: 0.7159\n",
      "Epoch [3/10], Step [98/147], Loss: 0.8175\n",
      "Epoch [3/10], Step [99/147], Loss: 0.7270\n",
      "Epoch [3/10], Step [100/147], Loss: 0.7534\n",
      "Epoch [3/10], Step [101/147], Loss: 0.8395\n",
      "Epoch [3/10], Step [102/147], Loss: 0.7757\n",
      "Epoch [3/10], Step [103/147], Loss: 0.8174\n",
      "Epoch [3/10], Step [104/147], Loss: 0.8018\n",
      "Epoch [3/10], Step [105/147], Loss: 0.7683\n",
      "Epoch [3/10], Step [106/147], Loss: 0.7837\n",
      "Epoch [3/10], Step [107/147], Loss: 0.8021\n",
      "Epoch [3/10], Step [108/147], Loss: 0.7498\n",
      "Epoch [3/10], Step [109/147], Loss: 0.7340\n",
      "Epoch [3/10], Step [110/147], Loss: 0.7924\n",
      "Epoch [3/10], Step [111/147], Loss: 0.7120\n",
      "Epoch [3/10], Step [112/147], Loss: 0.7186\n",
      "Epoch [3/10], Step [113/147], Loss: 0.7684\n",
      "Epoch [3/10], Step [114/147], Loss: 0.7198\n",
      "Epoch [3/10], Step [115/147], Loss: 0.7383\n",
      "Epoch [3/10], Step [116/147], Loss: 0.7812\n",
      "Epoch [3/10], Step [117/147], Loss: 0.7600\n",
      "Epoch [3/10], Step [118/147], Loss: 0.7070\n",
      "Epoch [3/10], Step [119/147], Loss: 0.7478\n",
      "Epoch [3/10], Step [120/147], Loss: 0.7397\n",
      "Epoch [3/10], Step [121/147], Loss: 0.7660\n",
      "Epoch [3/10], Step [122/147], Loss: 0.7313\n",
      "Epoch [3/10], Step [123/147], Loss: 0.8042\n",
      "Epoch [3/10], Step [124/147], Loss: 0.7375\n",
      "Epoch [3/10], Step [125/147], Loss: 0.7924\n",
      "Epoch [3/10], Step [126/147], Loss: 0.7063\n",
      "Epoch [3/10], Step [127/147], Loss: 0.6815\n",
      "Epoch [3/10], Step [128/147], Loss: 0.8141\n",
      "Epoch [3/10], Step [129/147], Loss: 0.6636\n",
      "Epoch [3/10], Step [130/147], Loss: 0.6892\n",
      "Epoch [3/10], Step [131/147], Loss: 0.7699\n",
      "Epoch [3/10], Step [132/147], Loss: 0.8114\n",
      "Epoch [3/10], Step [133/147], Loss: 0.7844\n",
      "Epoch [3/10], Step [134/147], Loss: 0.7895\n",
      "Epoch [3/10], Step [135/147], Loss: 0.7190\n",
      "Epoch [3/10], Step [136/147], Loss: 0.7304\n",
      "Epoch [3/10], Step [137/147], Loss: 0.7622\n",
      "Epoch [3/10], Step [138/147], Loss: 0.7570\n",
      "Epoch [3/10], Step [139/147], Loss: 0.7198\n",
      "Epoch [3/10], Step [140/147], Loss: 0.6879\n",
      "Epoch [3/10], Step [141/147], Loss: 0.7130\n",
      "Epoch [3/10], Step [142/147], Loss: 0.7962\n",
      "Epoch [3/10], Step [143/147], Loss: 0.7531\n",
      "Epoch [3/10], Step [144/147], Loss: 0.7393\n",
      "Epoch [3/10], Step [145/147], Loss: 0.7559\n",
      "Epoch [3/10], Step [146/147], Loss: 0.7194\n",
      "Epoch [3/10], Step [147/147], Loss: 0.7014\n",
      "Epoch [4/10], Step [1/147], Loss: 0.7141\n",
      "Epoch [4/10], Step [2/147], Loss: 0.6863\n",
      "Epoch [4/10], Step [3/147], Loss: 0.7448\n",
      "Epoch [4/10], Step [4/147], Loss: 0.6594\n",
      "Epoch [4/10], Step [5/147], Loss: 0.6615\n",
      "Epoch [4/10], Step [6/147], Loss: 0.7274\n",
      "Epoch [4/10], Step [7/147], Loss: 0.7851\n",
      "Epoch [4/10], Step [8/147], Loss: 0.7030\n",
      "Epoch [4/10], Step [9/147], Loss: 0.7458\n",
      "Epoch [4/10], Step [10/147], Loss: 0.8081\n",
      "Epoch [4/10], Step [11/147], Loss: 0.7189\n",
      "Epoch [4/10], Step [12/147], Loss: 0.7486\n",
      "Epoch [4/10], Step [13/147], Loss: 0.6735\n",
      "Epoch [4/10], Step [14/147], Loss: 0.7371\n",
      "Epoch [4/10], Step [15/147], Loss: 0.8288\n",
      "Epoch [4/10], Step [16/147], Loss: 0.7314\n",
      "Epoch [4/10], Step [17/147], Loss: 0.7381\n",
      "Epoch [4/10], Step [18/147], Loss: 0.8100\n",
      "Epoch [4/10], Step [19/147], Loss: 0.7064\n",
      "Epoch [4/10], Step [20/147], Loss: 0.7392\n",
      "Epoch [4/10], Step [21/147], Loss: 0.7792\n",
      "Epoch [4/10], Step [22/147], Loss: 0.6771\n",
      "Epoch [4/10], Step [23/147], Loss: 0.7855\n",
      "Epoch [4/10], Step [24/147], Loss: 0.6433\n",
      "Epoch [4/10], Step [25/147], Loss: 0.6733\n",
      "Epoch [4/10], Step [26/147], Loss: 0.7454\n",
      "Epoch [4/10], Step [27/147], Loss: 0.7284\n",
      "Epoch [4/10], Step [28/147], Loss: 0.7165\n",
      "Epoch [4/10], Step [29/147], Loss: 0.7505\n",
      "Epoch [4/10], Step [30/147], Loss: 0.7522\n",
      "Epoch [4/10], Step [31/147], Loss: 0.7030\n",
      "Epoch [4/10], Step [32/147], Loss: 0.6591\n",
      "Epoch [4/10], Step [33/147], Loss: 0.7676\n",
      "Epoch [4/10], Step [34/147], Loss: 0.7623\n",
      "Epoch [4/10], Step [35/147], Loss: 0.7364\n",
      "Epoch [4/10], Step [36/147], Loss: 0.6544\n",
      "Epoch [4/10], Step [37/147], Loss: 0.7192\n",
      "Epoch [4/10], Step [38/147], Loss: 0.7512\n",
      "Epoch [4/10], Step [39/147], Loss: 0.6756\n",
      "Epoch [4/10], Step [40/147], Loss: 0.6790\n",
      "Epoch [4/10], Step [41/147], Loss: 0.6642\n",
      "Epoch [4/10], Step [42/147], Loss: 0.6887\n",
      "Epoch [4/10], Step [43/147], Loss: 0.6553\n",
      "Epoch [4/10], Step [44/147], Loss: 0.6504\n",
      "Epoch [4/10], Step [45/147], Loss: 0.6727\n",
      "Epoch [4/10], Step [46/147], Loss: 0.6966\n",
      "Epoch [4/10], Step [47/147], Loss: 0.7396\n",
      "Epoch [4/10], Step [48/147], Loss: 0.7667\n",
      "Epoch [4/10], Step [49/147], Loss: 0.6720\n",
      "Epoch [4/10], Step [50/147], Loss: 0.6883\n",
      "Epoch [4/10], Step [51/147], Loss: 0.6711\n",
      "Epoch [4/10], Step [52/147], Loss: 0.6632\n",
      "Epoch [4/10], Step [53/147], Loss: 0.7488\n",
      "Epoch [4/10], Step [54/147], Loss: 0.6193\n",
      "Epoch [4/10], Step [55/147], Loss: 0.7700\n",
      "Epoch [4/10], Step [56/147], Loss: 0.7125\n",
      "Epoch [4/10], Step [57/147], Loss: 0.6749\n",
      "Epoch [4/10], Step [58/147], Loss: 0.7147\n",
      "Epoch [4/10], Step [59/147], Loss: 0.6797\n",
      "Epoch [4/10], Step [60/147], Loss: 0.7052\n",
      "Epoch [4/10], Step [61/147], Loss: 0.6764\n",
      "Epoch [4/10], Step [62/147], Loss: 0.6976\n",
      "Epoch [4/10], Step [63/147], Loss: 0.6811\n",
      "Epoch [4/10], Step [64/147], Loss: 0.6657\n",
      "Epoch [4/10], Step [65/147], Loss: 0.6790\n",
      "Epoch [4/10], Step [66/147], Loss: 0.6449\n",
      "Epoch [4/10], Step [67/147], Loss: 0.7919\n",
      "Epoch [4/10], Step [68/147], Loss: 0.6656\n",
      "Epoch [4/10], Step [69/147], Loss: 0.6920\n",
      "Epoch [4/10], Step [70/147], Loss: 0.7073\n",
      "Epoch [4/10], Step [71/147], Loss: 0.7039\n",
      "Epoch [4/10], Step [72/147], Loss: 0.6428\n",
      "Epoch [4/10], Step [73/147], Loss: 0.6786\n",
      "Epoch [4/10], Step [74/147], Loss: 0.6744\n",
      "Epoch [4/10], Step [75/147], Loss: 0.6900\n",
      "Epoch [4/10], Step [76/147], Loss: 0.7236\n",
      "Epoch [4/10], Step [77/147], Loss: 0.7029\n",
      "Epoch [4/10], Step [78/147], Loss: 0.6637\n",
      "Epoch [4/10], Step [79/147], Loss: 0.5715\n",
      "Epoch [4/10], Step [80/147], Loss: 0.6731\n",
      "Epoch [4/10], Step [81/147], Loss: 0.6846\n",
      "Epoch [4/10], Step [82/147], Loss: 0.6808\n",
      "Epoch [4/10], Step [83/147], Loss: 0.6785\n",
      "Epoch [4/10], Step [84/147], Loss: 0.7012\n",
      "Epoch [4/10], Step [85/147], Loss: 0.6522\n",
      "Epoch [4/10], Step [86/147], Loss: 0.6445\n",
      "Epoch [4/10], Step [87/147], Loss: 0.6593\n",
      "Epoch [4/10], Step [88/147], Loss: 0.7926\n",
      "Epoch [4/10], Step [89/147], Loss: 0.6041\n",
      "Epoch [4/10], Step [90/147], Loss: 0.7255\n",
      "Epoch [4/10], Step [91/147], Loss: 0.7220\n",
      "Epoch [4/10], Step [92/147], Loss: 0.6145\n",
      "Epoch [4/10], Step [93/147], Loss: 0.6639\n",
      "Epoch [4/10], Step [94/147], Loss: 0.6304\n",
      "Epoch [4/10], Step [95/147], Loss: 0.7457\n",
      "Epoch [4/10], Step [96/147], Loss: 0.6031\n",
      "Epoch [4/10], Step [97/147], Loss: 0.5967\n",
      "Epoch [4/10], Step [98/147], Loss: 0.6334\n",
      "Epoch [4/10], Step [99/147], Loss: 0.7141\n",
      "Epoch [4/10], Step [100/147], Loss: 0.7064\n",
      "Epoch [4/10], Step [101/147], Loss: 0.6465\n",
      "Epoch [4/10], Step [102/147], Loss: 0.6571\n",
      "Epoch [4/10], Step [103/147], Loss: 0.7124\n",
      "Epoch [4/10], Step [104/147], Loss: 0.5770\n",
      "Epoch [4/10], Step [105/147], Loss: 0.7000\n",
      "Epoch [4/10], Step [106/147], Loss: 0.6215\n",
      "Epoch [4/10], Step [107/147], Loss: 0.7028\n",
      "Epoch [4/10], Step [108/147], Loss: 0.6447\n",
      "Epoch [4/10], Step [109/147], Loss: 0.6082\n",
      "Epoch [4/10], Step [110/147], Loss: 0.6623\n",
      "Epoch [4/10], Step [111/147], Loss: 0.6313\n",
      "Epoch [4/10], Step [112/147], Loss: 0.6608\n",
      "Epoch [4/10], Step [113/147], Loss: 0.6336\n",
      "Epoch [4/10], Step [114/147], Loss: 0.7623\n",
      "Epoch [4/10], Step [115/147], Loss: 0.5838\n",
      "Epoch [4/10], Step [116/147], Loss: 0.6061\n",
      "Epoch [4/10], Step [117/147], Loss: 0.6634\n",
      "Epoch [4/10], Step [118/147], Loss: 0.6104\n",
      "Epoch [4/10], Step [119/147], Loss: 0.6811\n",
      "Epoch [4/10], Step [120/147], Loss: 0.6282\n",
      "Epoch [4/10], Step [121/147], Loss: 0.6355\n",
      "Epoch [4/10], Step [122/147], Loss: 0.6093\n",
      "Epoch [4/10], Step [123/147], Loss: 0.7150\n",
      "Epoch [4/10], Step [124/147], Loss: 0.6596\n",
      "Epoch [4/10], Step [125/147], Loss: 0.6273\n",
      "Epoch [4/10], Step [126/147], Loss: 0.6675\n",
      "Epoch [4/10], Step [127/147], Loss: 0.6439\n",
      "Epoch [4/10], Step [128/147], Loss: 0.6312\n",
      "Epoch [4/10], Step [129/147], Loss: 0.6640\n",
      "Epoch [4/10], Step [130/147], Loss: 0.6154\n",
      "Epoch [4/10], Step [131/147], Loss: 0.6232\n",
      "Epoch [4/10], Step [132/147], Loss: 0.6564\n",
      "Epoch [4/10], Step [133/147], Loss: 0.6681\n",
      "Epoch [4/10], Step [134/147], Loss: 0.6429\n",
      "Epoch [4/10], Step [135/147], Loss: 0.7241\n",
      "Epoch [4/10], Step [136/147], Loss: 0.6487\n",
      "Epoch [4/10], Step [137/147], Loss: 0.6376\n",
      "Epoch [4/10], Step [138/147], Loss: 0.5864\n",
      "Epoch [4/10], Step [139/147], Loss: 0.6628\n",
      "Epoch [4/10], Step [140/147], Loss: 0.6406\n",
      "Epoch [4/10], Step [141/147], Loss: 0.6705\n",
      "Epoch [4/10], Step [142/147], Loss: 0.6076\n",
      "Epoch [4/10], Step [143/147], Loss: 0.6556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [144/147], Loss: 0.5729\n",
      "Epoch [4/10], Step [145/147], Loss: 0.6556\n",
      "Epoch [4/10], Step [146/147], Loss: 0.6266\n",
      "Epoch [4/10], Step [147/147], Loss: 0.6261\n",
      "Epoch [5/10], Step [1/147], Loss: 0.5992\n",
      "Epoch [5/10], Step [2/147], Loss: 0.6188\n",
      "Epoch [5/10], Step [3/147], Loss: 0.6318\n",
      "Epoch [5/10], Step [4/147], Loss: 0.7010\n",
      "Epoch [5/10], Step [5/147], Loss: 0.5684\n",
      "Epoch [5/10], Step [6/147], Loss: 0.5896\n",
      "Epoch [5/10], Step [7/147], Loss: 0.6616\n",
      "Epoch [5/10], Step [8/147], Loss: 0.6230\n",
      "Epoch [5/10], Step [9/147], Loss: 0.7149\n",
      "Epoch [5/10], Step [10/147], Loss: 0.6198\n",
      "Epoch [5/10], Step [11/147], Loss: 0.6516\n",
      "Epoch [5/10], Step [12/147], Loss: 0.5825\n",
      "Epoch [5/10], Step [13/147], Loss: 0.6128\n",
      "Epoch [5/10], Step [14/147], Loss: 0.6053\n",
      "Epoch [5/10], Step [15/147], Loss: 0.6968\n",
      "Epoch [5/10], Step [16/147], Loss: 0.6387\n",
      "Epoch [5/10], Step [17/147], Loss: 0.6506\n",
      "Epoch [5/10], Step [18/147], Loss: 0.5471\n",
      "Epoch [5/10], Step [19/147], Loss: 0.6372\n",
      "Epoch [5/10], Step [20/147], Loss: 0.6605\n",
      "Epoch [5/10], Step [21/147], Loss: 0.6469\n",
      "Epoch [5/10], Step [22/147], Loss: 0.5876\n",
      "Epoch [5/10], Step [23/147], Loss: 0.5873\n",
      "Epoch [5/10], Step [24/147], Loss: 0.6444\n",
      "Epoch [5/10], Step [25/147], Loss: 0.7763\n",
      "Epoch [5/10], Step [26/147], Loss: 0.5994\n",
      "Epoch [5/10], Step [27/147], Loss: 0.6082\n",
      "Epoch [5/10], Step [28/147], Loss: 0.6338\n",
      "Epoch [5/10], Step [29/147], Loss: 0.5810\n",
      "Epoch [5/10], Step [30/147], Loss: 0.6625\n",
      "Epoch [5/10], Step [31/147], Loss: 0.6052\n",
      "Epoch [5/10], Step [32/147], Loss: 0.6427\n",
      "Epoch [5/10], Step [33/147], Loss: 0.5923\n",
      "Epoch [5/10], Step [34/147], Loss: 0.5224\n",
      "Epoch [5/10], Step [35/147], Loss: 0.6161\n",
      "Epoch [5/10], Step [36/147], Loss: 0.5890\n",
      "Epoch [5/10], Step [37/147], Loss: 0.6397\n",
      "Epoch [5/10], Step [38/147], Loss: 0.6244\n",
      "Epoch [5/10], Step [39/147], Loss: 0.5725\n",
      "Epoch [5/10], Step [40/147], Loss: 0.6092\n",
      "Epoch [5/10], Step [41/147], Loss: 0.6481\n",
      "Epoch [5/10], Step [42/147], Loss: 0.6194\n",
      "Epoch [5/10], Step [43/147], Loss: 0.6273\n",
      "Epoch [5/10], Step [44/147], Loss: 0.6322\n",
      "Epoch [5/10], Step [45/147], Loss: 0.6762\n",
      "Epoch [5/10], Step [46/147], Loss: 0.5693\n",
      "Epoch [5/10], Step [47/147], Loss: 0.6487\n",
      "Epoch [5/10], Step [48/147], Loss: 0.6471\n",
      "Epoch [5/10], Step [49/147], Loss: 0.6097\n",
      "Epoch [5/10], Step [50/147], Loss: 0.6827\n",
      "Epoch [5/10], Step [51/147], Loss: 0.5074\n",
      "Epoch [5/10], Step [52/147], Loss: 0.6045\n",
      "Epoch [5/10], Step [53/147], Loss: 0.6673\n",
      "Epoch [5/10], Step [54/147], Loss: 0.5193\n",
      "Epoch [5/10], Step [55/147], Loss: 0.6060\n",
      "Epoch [5/10], Step [56/147], Loss: 0.6116\n",
      "Epoch [5/10], Step [57/147], Loss: 0.6949\n",
      "Epoch [5/10], Step [58/147], Loss: 0.6250\n",
      "Epoch [5/10], Step [59/147], Loss: 0.5930\n",
      "Epoch [5/10], Step [60/147], Loss: 0.6259\n",
      "Epoch [5/10], Step [61/147], Loss: 0.6173\n",
      "Epoch [5/10], Step [62/147], Loss: 0.6137\n",
      "Epoch [5/10], Step [63/147], Loss: 0.6413\n",
      "Epoch [5/10], Step [64/147], Loss: 0.5881\n",
      "Epoch [5/10], Step [65/147], Loss: 0.6542\n",
      "Epoch [5/10], Step [66/147], Loss: 0.5673\n",
      "Epoch [5/10], Step [67/147], Loss: 0.5948\n",
      "Epoch [5/10], Step [68/147], Loss: 0.6869\n",
      "Epoch [5/10], Step [69/147], Loss: 0.6179\n",
      "Epoch [5/10], Step [70/147], Loss: 0.6237\n",
      "Epoch [5/10], Step [71/147], Loss: 0.6345\n",
      "Epoch [5/10], Step [72/147], Loss: 0.5496\n",
      "Epoch [5/10], Step [73/147], Loss: 0.5136\n",
      "Epoch [5/10], Step [74/147], Loss: 0.6135\n",
      "Epoch [5/10], Step [75/147], Loss: 0.6113\n",
      "Epoch [5/10], Step [76/147], Loss: 0.6126\n",
      "Epoch [5/10], Step [77/147], Loss: 0.6204\n",
      "Epoch [5/10], Step [78/147], Loss: 0.5674\n",
      "Epoch [5/10], Step [79/147], Loss: 0.6545\n",
      "Epoch [5/10], Step [80/147], Loss: 0.5878\n",
      "Epoch [5/10], Step [81/147], Loss: 0.6436\n",
      "Epoch [5/10], Step [82/147], Loss: 0.6314\n",
      "Epoch [5/10], Step [83/147], Loss: 0.5800\n",
      "Epoch [5/10], Step [84/147], Loss: 0.5151\n",
      "Epoch [5/10], Step [85/147], Loss: 0.6844\n",
      "Epoch [5/10], Step [86/147], Loss: 0.5905\n",
      "Epoch [5/10], Step [87/147], Loss: 0.6146\n",
      "Epoch [5/10], Step [88/147], Loss: 0.5720\n",
      "Epoch [5/10], Step [89/147], Loss: 0.5333\n",
      "Epoch [5/10], Step [90/147], Loss: 0.5654\n",
      "Epoch [5/10], Step [91/147], Loss: 0.5779\n",
      "Epoch [5/10], Step [92/147], Loss: 0.5457\n",
      "Epoch [5/10], Step [93/147], Loss: 0.6154\n",
      "Epoch [5/10], Step [94/147], Loss: 0.5645\n",
      "Epoch [5/10], Step [95/147], Loss: 0.6517\n",
      "Epoch [5/10], Step [96/147], Loss: 0.6503\n",
      "Epoch [5/10], Step [97/147], Loss: 0.5803\n",
      "Epoch [5/10], Step [98/147], Loss: 0.6388\n",
      "Epoch [5/10], Step [99/147], Loss: 0.5688\n",
      "Epoch [5/10], Step [100/147], Loss: 0.5381\n",
      "Epoch [5/10], Step [101/147], Loss: 0.6531\n",
      "Epoch [5/10], Step [102/147], Loss: 0.5970\n",
      "Epoch [5/10], Step [103/147], Loss: 0.6161\n",
      "Epoch [5/10], Step [104/147], Loss: 0.5676\n",
      "Epoch [5/10], Step [105/147], Loss: 0.5658\n",
      "Epoch [5/10], Step [106/147], Loss: 0.5535\n",
      "Epoch [5/10], Step [107/147], Loss: 0.6370\n",
      "Epoch [5/10], Step [108/147], Loss: 0.6073\n",
      "Epoch [5/10], Step [109/147], Loss: 0.6022\n",
      "Epoch [5/10], Step [110/147], Loss: 0.5650\n",
      "Epoch [5/10], Step [111/147], Loss: 0.6148\n",
      "Epoch [5/10], Step [112/147], Loss: 0.5760\n",
      "Epoch [5/10], Step [113/147], Loss: 0.5527\n",
      "Epoch [5/10], Step [114/147], Loss: 0.5992\n",
      "Epoch [5/10], Step [115/147], Loss: 0.5884\n",
      "Epoch [5/10], Step [116/147], Loss: 0.5236\n",
      "Epoch [5/10], Step [117/147], Loss: 0.5935\n",
      "Epoch [5/10], Step [118/147], Loss: 0.5576\n",
      "Epoch [5/10], Step [119/147], Loss: 0.6124\n",
      "Epoch [5/10], Step [120/147], Loss: 0.5003\n",
      "Epoch [5/10], Step [121/147], Loss: 0.5439\n",
      "Epoch [5/10], Step [122/147], Loss: 0.6144\n",
      "Epoch [5/10], Step [123/147], Loss: 0.6260\n",
      "Epoch [5/10], Step [124/147], Loss: 0.6042\n",
      "Epoch [5/10], Step [125/147], Loss: 0.5600\n",
      "Epoch [5/10], Step [126/147], Loss: 0.5631\n",
      "Epoch [5/10], Step [127/147], Loss: 0.5983\n",
      "Epoch [5/10], Step [128/147], Loss: 0.5722\n",
      "Epoch [5/10], Step [129/147], Loss: 0.5775\n",
      "Epoch [5/10], Step [130/147], Loss: 0.5618\n",
      "Epoch [5/10], Step [131/147], Loss: 0.5752\n",
      "Epoch [5/10], Step [132/147], Loss: 0.5496\n",
      "Epoch [5/10], Step [133/147], Loss: 0.5554\n",
      "Epoch [5/10], Step [134/147], Loss: 0.5036\n",
      "Epoch [5/10], Step [135/147], Loss: 0.5482\n",
      "Epoch [5/10], Step [136/147], Loss: 0.5802\n",
      "Epoch [5/10], Step [137/147], Loss: 0.5977\n",
      "Epoch [5/10], Step [138/147], Loss: 0.5556\n",
      "Epoch [5/10], Step [139/147], Loss: 0.6333\n",
      "Epoch [5/10], Step [140/147], Loss: 0.6507\n",
      "Epoch [5/10], Step [141/147], Loss: 0.5922\n",
      "Epoch [5/10], Step [142/147], Loss: 0.5674\n",
      "Epoch [5/10], Step [143/147], Loss: 0.5288\n",
      "Epoch [5/10], Step [144/147], Loss: 0.5756\n",
      "Epoch [5/10], Step [145/147], Loss: 0.5581\n",
      "Epoch [5/10], Step [146/147], Loss: 0.5321\n",
      "Epoch [5/10], Step [147/147], Loss: 0.5247\n",
      "Epoch [6/10], Step [1/147], Loss: 0.5148\n",
      "Epoch [6/10], Step [2/147], Loss: 0.5730\n",
      "Epoch [6/10], Step [3/147], Loss: 0.5833\n",
      "Epoch [6/10], Step [4/147], Loss: 0.6084\n",
      "Epoch [6/10], Step [5/147], Loss: 0.5844\n",
      "Epoch [6/10], Step [6/147], Loss: 0.5876\n",
      "Epoch [6/10], Step [7/147], Loss: 0.5550\n",
      "Epoch [6/10], Step [8/147], Loss: 0.5924\n",
      "Epoch [6/10], Step [9/147], Loss: 0.5209\n",
      "Epoch [6/10], Step [10/147], Loss: 0.5000\n",
      "Epoch [6/10], Step [11/147], Loss: 0.6036\n",
      "Epoch [6/10], Step [12/147], Loss: 0.5642\n",
      "Epoch [6/10], Step [13/147], Loss: 0.6224\n",
      "Epoch [6/10], Step [14/147], Loss: 0.4550\n",
      "Epoch [6/10], Step [15/147], Loss: 0.5987\n",
      "Epoch [6/10], Step [16/147], Loss: 0.6277\n",
      "Epoch [6/10], Step [17/147], Loss: 0.5856\n",
      "Epoch [6/10], Step [18/147], Loss: 0.6606\n",
      "Epoch [6/10], Step [19/147], Loss: 0.5985\n",
      "Epoch [6/10], Step [20/147], Loss: 0.6135\n",
      "Epoch [6/10], Step [21/147], Loss: 0.6186\n",
      "Epoch [6/10], Step [22/147], Loss: 0.5617\n",
      "Epoch [6/10], Step [23/147], Loss: 0.6406\n",
      "Epoch [6/10], Step [24/147], Loss: 0.5941\n",
      "Epoch [6/10], Step [25/147], Loss: 0.5297\n",
      "Epoch [6/10], Step [26/147], Loss: 0.5183\n",
      "Epoch [6/10], Step [27/147], Loss: 0.5442\n",
      "Epoch [6/10], Step [28/147], Loss: 0.5983\n",
      "Epoch [6/10], Step [29/147], Loss: 0.5456\n",
      "Epoch [6/10], Step [30/147], Loss: 0.5649\n",
      "Epoch [6/10], Step [31/147], Loss: 0.6520\n",
      "Epoch [6/10], Step [32/147], Loss: 0.5968\n",
      "Epoch [6/10], Step [33/147], Loss: 0.5585\n",
      "Epoch [6/10], Step [34/147], Loss: 0.4726\n",
      "Epoch [6/10], Step [35/147], Loss: 0.5915\n",
      "Epoch [6/10], Step [36/147], Loss: 0.5740\n",
      "Epoch [6/10], Step [37/147], Loss: 0.5954\n",
      "Epoch [6/10], Step [38/147], Loss: 0.5592\n",
      "Epoch [6/10], Step [39/147], Loss: 0.5522\n",
      "Epoch [6/10], Step [40/147], Loss: 0.4991\n",
      "Epoch [6/10], Step [41/147], Loss: 0.5513\n",
      "Epoch [6/10], Step [42/147], Loss: 0.5596\n",
      "Epoch [6/10], Step [43/147], Loss: 0.5412\n",
      "Epoch [6/10], Step [44/147], Loss: 0.5391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [45/147], Loss: 0.5163\n",
      "Epoch [6/10], Step [46/147], Loss: 0.5254\n",
      "Epoch [6/10], Step [47/147], Loss: 0.6125\n",
      "Epoch [6/10], Step [48/147], Loss: 0.4944\n",
      "Epoch [6/10], Step [49/147], Loss: 0.4770\n",
      "Epoch [6/10], Step [50/147], Loss: 0.5447\n",
      "Epoch [6/10], Step [51/147], Loss: 0.5503\n",
      "Epoch [6/10], Step [52/147], Loss: 0.5275\n",
      "Epoch [6/10], Step [53/147], Loss: 0.5525\n",
      "Epoch [6/10], Step [54/147], Loss: 0.5633\n",
      "Epoch [6/10], Step [55/147], Loss: 0.5361\n",
      "Epoch [6/10], Step [56/147], Loss: 0.5613\n",
      "Epoch [6/10], Step [57/147], Loss: 0.5237\n",
      "Epoch [6/10], Step [58/147], Loss: 0.5662\n",
      "Epoch [6/10], Step [59/147], Loss: 0.4950\n",
      "Epoch [6/10], Step [60/147], Loss: 0.5613\n",
      "Epoch [6/10], Step [61/147], Loss: 0.5448\n",
      "Epoch [6/10], Step [62/147], Loss: 0.4895\n",
      "Epoch [6/10], Step [63/147], Loss: 0.5250\n",
      "Epoch [6/10], Step [64/147], Loss: 0.5757\n",
      "Epoch [6/10], Step [65/147], Loss: 0.5152\n",
      "Epoch [6/10], Step [66/147], Loss: 0.5270\n",
      "Epoch [6/10], Step [67/147], Loss: 0.4669\n",
      "Epoch [6/10], Step [68/147], Loss: 0.5177\n",
      "Epoch [6/10], Step [69/147], Loss: 0.5136\n",
      "Epoch [6/10], Step [70/147], Loss: 0.5348\n",
      "Epoch [6/10], Step [71/147], Loss: 0.4888\n",
      "Epoch [6/10], Step [72/147], Loss: 0.4733\n",
      "Epoch [6/10], Step [73/147], Loss: 0.5317\n",
      "Epoch [6/10], Step [74/147], Loss: 0.5060\n",
      "Epoch [6/10], Step [75/147], Loss: 0.5900\n",
      "Epoch [6/10], Step [76/147], Loss: 0.5599\n",
      "Epoch [6/10], Step [77/147], Loss: 0.5389\n",
      "Epoch [6/10], Step [78/147], Loss: 0.4935\n",
      "Epoch [6/10], Step [79/147], Loss: 0.6658\n",
      "Epoch [6/10], Step [80/147], Loss: 0.4759\n",
      "Epoch [6/10], Step [81/147], Loss: 0.4975\n",
      "Epoch [6/10], Step [82/147], Loss: 0.5275\n",
      "Epoch [6/10], Step [83/147], Loss: 0.5552\n",
      "Epoch [6/10], Step [84/147], Loss: 0.5444\n",
      "Epoch [6/10], Step [85/147], Loss: 0.4963\n",
      "Epoch [6/10], Step [86/147], Loss: 0.5191\n",
      "Epoch [6/10], Step [87/147], Loss: 0.5127\n",
      "Epoch [6/10], Step [88/147], Loss: 0.4991\n",
      "Epoch [6/10], Step [89/147], Loss: 0.5750\n",
      "Epoch [6/10], Step [90/147], Loss: 0.5260\n",
      "Epoch [6/10], Step [91/147], Loss: 0.5219\n",
      "Epoch [6/10], Step [92/147], Loss: 0.5398\n",
      "Epoch [6/10], Step [93/147], Loss: 0.5116\n",
      "Epoch [6/10], Step [94/147], Loss: 0.6203\n",
      "Epoch [6/10], Step [95/147], Loss: 0.4875\n",
      "Epoch [6/10], Step [96/147], Loss: 0.5521\n",
      "Epoch [6/10], Step [97/147], Loss: 0.5965\n",
      "Epoch [6/10], Step [98/147], Loss: 0.5614\n",
      "Epoch [6/10], Step [99/147], Loss: 0.5310\n",
      "Epoch [6/10], Step [100/147], Loss: 0.4362\n",
      "Epoch [6/10], Step [101/147], Loss: 0.4882\n",
      "Epoch [6/10], Step [102/147], Loss: 0.5154\n",
      "Epoch [6/10], Step [103/147], Loss: 0.4881\n",
      "Epoch [6/10], Step [104/147], Loss: 0.5543\n",
      "Epoch [6/10], Step [105/147], Loss: 0.5872\n",
      "Epoch [6/10], Step [106/147], Loss: 0.6408\n",
      "Epoch [6/10], Step [107/147], Loss: 0.4607\n",
      "Epoch [6/10], Step [108/147], Loss: 0.5213\n",
      "Epoch [6/10], Step [109/147], Loss: 0.6399\n",
      "Epoch [6/10], Step [110/147], Loss: 0.5514\n",
      "Epoch [6/10], Step [111/147], Loss: 0.4334\n",
      "Epoch [6/10], Step [112/147], Loss: 0.5568\n",
      "Epoch [6/10], Step [113/147], Loss: 0.4993\n",
      "Epoch [6/10], Step [114/147], Loss: 0.5906\n",
      "Epoch [6/10], Step [115/147], Loss: 0.5639\n",
      "Epoch [6/10], Step [116/147], Loss: 0.4923\n",
      "Epoch [6/10], Step [117/147], Loss: 0.5822\n",
      "Epoch [6/10], Step [118/147], Loss: 0.5525\n",
      "Epoch [6/10], Step [119/147], Loss: 0.5916\n",
      "Epoch [6/10], Step [120/147], Loss: 0.5577\n",
      "Epoch [6/10], Step [121/147], Loss: 0.5498\n",
      "Epoch [6/10], Step [122/147], Loss: 0.4351\n",
      "Epoch [6/10], Step [123/147], Loss: 0.6342\n",
      "Epoch [6/10], Step [124/147], Loss: 0.5831\n",
      "Epoch [6/10], Step [125/147], Loss: 0.5325\n",
      "Epoch [6/10], Step [126/147], Loss: 0.5302\n",
      "Epoch [6/10], Step [127/147], Loss: 0.5612\n",
      "Epoch [6/10], Step [128/147], Loss: 0.4872\n",
      "Epoch [6/10], Step [129/147], Loss: 0.4793\n",
      "Epoch [6/10], Step [130/147], Loss: 0.5036\n",
      "Epoch [6/10], Step [131/147], Loss: 0.4856\n",
      "Epoch [6/10], Step [132/147], Loss: 0.5433\n",
      "Epoch [6/10], Step [133/147], Loss: 0.4970\n",
      "Epoch [6/10], Step [134/147], Loss: 0.6272\n",
      "Epoch [6/10], Step [135/147], Loss: 0.5299\n",
      "Epoch [6/10], Step [136/147], Loss: 0.5375\n",
      "Epoch [6/10], Step [137/147], Loss: 0.4310\n",
      "Epoch [6/10], Step [138/147], Loss: 0.4962\n",
      "Epoch [6/10], Step [139/147], Loss: 0.5419\n",
      "Epoch [6/10], Step [140/147], Loss: 0.4617\n",
      "Epoch [6/10], Step [141/147], Loss: 0.4653\n",
      "Epoch [6/10], Step [142/147], Loss: 0.5085\n",
      "Epoch [6/10], Step [143/147], Loss: 0.5480\n",
      "Epoch [6/10], Step [144/147], Loss: 0.5586\n",
      "Epoch [6/10], Step [145/147], Loss: 0.4573\n",
      "Epoch [6/10], Step [146/147], Loss: 0.5854\n",
      "Epoch [6/10], Step [147/147], Loss: 0.6386\n",
      "Epoch [7/10], Step [1/147], Loss: 0.5944\n",
      "Epoch [7/10], Step [2/147], Loss: 0.5095\n",
      "Epoch [7/10], Step [3/147], Loss: 0.5392\n",
      "Epoch [7/10], Step [4/147], Loss: 0.5314\n",
      "Epoch [7/10], Step [5/147], Loss: 0.5638\n",
      "Epoch [7/10], Step [6/147], Loss: 0.5345\n",
      "Epoch [7/10], Step [7/147], Loss: 0.5188\n",
      "Epoch [7/10], Step [8/147], Loss: 0.4954\n",
      "Epoch [7/10], Step [9/147], Loss: 0.4892\n",
      "Epoch [7/10], Step [10/147], Loss: 0.5355\n",
      "Epoch [7/10], Step [11/147], Loss: 0.5100\n",
      "Epoch [7/10], Step [12/147], Loss: 0.5257\n",
      "Epoch [7/10], Step [13/147], Loss: 0.5662\n",
      "Epoch [7/10], Step [14/147], Loss: 0.5021\n",
      "Epoch [7/10], Step [15/147], Loss: 0.5311\n",
      "Epoch [7/10], Step [16/147], Loss: 0.5049\n",
      "Epoch [7/10], Step [17/147], Loss: 0.4964\n",
      "Epoch [7/10], Step [18/147], Loss: 0.5081\n",
      "Epoch [7/10], Step [19/147], Loss: 0.5899\n",
      "Epoch [7/10], Step [20/147], Loss: 0.5649\n",
      "Epoch [7/10], Step [21/147], Loss: 0.6363\n",
      "Epoch [7/10], Step [22/147], Loss: 0.5416\n",
      "Epoch [7/10], Step [23/147], Loss: 0.5427\n",
      "Epoch [7/10], Step [24/147], Loss: 0.4630\n",
      "Epoch [7/10], Step [25/147], Loss: 0.4954\n",
      "Epoch [7/10], Step [26/147], Loss: 0.5762\n",
      "Epoch [7/10], Step [27/147], Loss: 0.5220\n",
      "Epoch [7/10], Step [28/147], Loss: 0.5043\n",
      "Epoch [7/10], Step [29/147], Loss: 0.5721\n",
      "Epoch [7/10], Step [30/147], Loss: 0.4448\n",
      "Epoch [7/10], Step [31/147], Loss: 0.5266\n",
      "Epoch [7/10], Step [32/147], Loss: 0.5519\n",
      "Epoch [7/10], Step [33/147], Loss: 0.4980\n",
      "Epoch [7/10], Step [34/147], Loss: 0.4891\n",
      "Epoch [7/10], Step [35/147], Loss: 0.4874\n",
      "Epoch [7/10], Step [36/147], Loss: 0.5059\n",
      "Epoch [7/10], Step [37/147], Loss: 0.4962\n",
      "Epoch [7/10], Step [38/147], Loss: 0.4167\n",
      "Epoch [7/10], Step [39/147], Loss: 0.4779\n",
      "Epoch [7/10], Step [40/147], Loss: 0.4901\n",
      "Epoch [7/10], Step [41/147], Loss: 0.5191\n",
      "Epoch [7/10], Step [42/147], Loss: 0.4374\n",
      "Epoch [7/10], Step [43/147], Loss: 0.4052\n",
      "Epoch [7/10], Step [44/147], Loss: 0.4658\n",
      "Epoch [7/10], Step [45/147], Loss: 0.5065\n",
      "Epoch [7/10], Step [46/147], Loss: 0.5419\n",
      "Epoch [7/10], Step [47/147], Loss: 0.4132\n",
      "Epoch [7/10], Step [48/147], Loss: 0.5529\n",
      "Epoch [7/10], Step [49/147], Loss: 0.5017\n",
      "Epoch [7/10], Step [50/147], Loss: 0.5453\n",
      "Epoch [7/10], Step [51/147], Loss: 0.4327\n",
      "Epoch [7/10], Step [52/147], Loss: 0.4565\n",
      "Epoch [7/10], Step [53/147], Loss: 0.5246\n",
      "Epoch [7/10], Step [54/147], Loss: 0.4467\n",
      "Epoch [7/10], Step [55/147], Loss: 0.4663\n",
      "Epoch [7/10], Step [56/147], Loss: 0.4715\n",
      "Epoch [7/10], Step [57/147], Loss: 0.4809\n",
      "Epoch [7/10], Step [58/147], Loss: 0.4527\n",
      "Epoch [7/10], Step [59/147], Loss: 0.4206\n",
      "Epoch [7/10], Step [60/147], Loss: 0.5019\n",
      "Epoch [7/10], Step [61/147], Loss: 0.5530\n",
      "Epoch [7/10], Step [62/147], Loss: 0.4707\n",
      "Epoch [7/10], Step [63/147], Loss: 0.5441\n",
      "Epoch [7/10], Step [64/147], Loss: 0.6026\n",
      "Epoch [7/10], Step [65/147], Loss: 0.4963\n",
      "Epoch [7/10], Step [66/147], Loss: 0.4853\n",
      "Epoch [7/10], Step [67/147], Loss: 0.5639\n",
      "Epoch [7/10], Step [68/147], Loss: 0.5175\n",
      "Epoch [7/10], Step [69/147], Loss: 0.4920\n",
      "Epoch [7/10], Step [70/147], Loss: 0.4755\n",
      "Epoch [7/10], Step [71/147], Loss: 0.5169\n",
      "Epoch [7/10], Step [72/147], Loss: 0.5144\n",
      "Epoch [7/10], Step [73/147], Loss: 0.5143\n",
      "Epoch [7/10], Step [74/147], Loss: 0.4914\n",
      "Epoch [7/10], Step [75/147], Loss: 0.5177\n",
      "Epoch [7/10], Step [76/147], Loss: 0.5428\n",
      "Epoch [7/10], Step [77/147], Loss: 0.5260\n",
      "Epoch [7/10], Step [78/147], Loss: 0.4161\n",
      "Epoch [7/10], Step [79/147], Loss: 0.4359\n",
      "Epoch [7/10], Step [80/147], Loss: 0.4973\n",
      "Epoch [7/10], Step [81/147], Loss: 0.5702\n",
      "Epoch [7/10], Step [82/147], Loss: 0.4856\n",
      "Epoch [7/10], Step [83/147], Loss: 0.4630\n",
      "Epoch [7/10], Step [84/147], Loss: 0.4793\n",
      "Epoch [7/10], Step [85/147], Loss: 0.4625\n",
      "Epoch [7/10], Step [86/147], Loss: 0.4798\n",
      "Epoch [7/10], Step [87/147], Loss: 0.5069\n",
      "Epoch [7/10], Step [88/147], Loss: 0.5014\n",
      "Epoch [7/10], Step [89/147], Loss: 0.4843\n",
      "Epoch [7/10], Step [90/147], Loss: 0.4081\n",
      "Epoch [7/10], Step [91/147], Loss: 0.5694\n",
      "Epoch [7/10], Step [92/147], Loss: 0.4849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [93/147], Loss: 0.5400\n",
      "Epoch [7/10], Step [94/147], Loss: 0.4900\n",
      "Epoch [7/10], Step [95/147], Loss: 0.4505\n",
      "Epoch [7/10], Step [96/147], Loss: 0.4700\n",
      "Epoch [7/10], Step [97/147], Loss: 0.5066\n",
      "Epoch [7/10], Step [98/147], Loss: 0.4405\n",
      "Epoch [7/10], Step [99/147], Loss: 0.4664\n",
      "Epoch [7/10], Step [100/147], Loss: 0.5775\n",
      "Epoch [7/10], Step [101/147], Loss: 0.5487\n",
      "Epoch [7/10], Step [102/147], Loss: 0.5191\n",
      "Epoch [7/10], Step [103/147], Loss: 0.4834\n",
      "Epoch [7/10], Step [104/147], Loss: 0.4370\n",
      "Epoch [7/10], Step [105/147], Loss: 0.4658\n",
      "Epoch [7/10], Step [106/147], Loss: 0.5119\n",
      "Epoch [7/10], Step [107/147], Loss: 0.3981\n",
      "Epoch [7/10], Step [108/147], Loss: 0.4736\n",
      "Epoch [7/10], Step [109/147], Loss: 0.5465\n",
      "Epoch [7/10], Step [110/147], Loss: 0.5729\n",
      "Epoch [7/10], Step [111/147], Loss: 0.4893\n",
      "Epoch [7/10], Step [112/147], Loss: 0.4827\n",
      "Epoch [7/10], Step [113/147], Loss: 0.4367\n",
      "Epoch [7/10], Step [114/147], Loss: 0.4841\n",
      "Epoch [7/10], Step [115/147], Loss: 0.4643\n",
      "Epoch [7/10], Step [116/147], Loss: 0.4486\n",
      "Epoch [7/10], Step [117/147], Loss: 0.4910\n",
      "Epoch [7/10], Step [118/147], Loss: 0.4362\n",
      "Epoch [7/10], Step [119/147], Loss: 0.4956\n",
      "Epoch [7/10], Step [120/147], Loss: 0.4753\n",
      "Epoch [7/10], Step [121/147], Loss: 0.5128\n",
      "Epoch [7/10], Step [122/147], Loss: 0.5274\n",
      "Epoch [7/10], Step [123/147], Loss: 0.4744\n",
      "Epoch [7/10], Step [124/147], Loss: 0.6008\n",
      "Epoch [7/10], Step [125/147], Loss: 0.5123\n",
      "Epoch [7/10], Step [126/147], Loss: 0.4899\n",
      "Epoch [7/10], Step [127/147], Loss: 0.4783\n",
      "Epoch [7/10], Step [128/147], Loss: 0.4545\n",
      "Epoch [7/10], Step [129/147], Loss: 0.5439\n",
      "Epoch [7/10], Step [130/147], Loss: 0.4733\n",
      "Epoch [7/10], Step [131/147], Loss: 0.5531\n",
      "Epoch [7/10], Step [132/147], Loss: 0.6011\n",
      "Epoch [7/10], Step [133/147], Loss: 0.4379\n",
      "Epoch [7/10], Step [134/147], Loss: 0.4669\n",
      "Epoch [7/10], Step [135/147], Loss: 0.5048\n",
      "Epoch [7/10], Step [136/147], Loss: 0.5193\n",
      "Epoch [7/10], Step [137/147], Loss: 0.4448\n",
      "Epoch [7/10], Step [138/147], Loss: 0.4138\n",
      "Epoch [7/10], Step [139/147], Loss: 0.4692\n",
      "Epoch [7/10], Step [140/147], Loss: 0.4620\n",
      "Epoch [7/10], Step [141/147], Loss: 0.4508\n",
      "Epoch [7/10], Step [142/147], Loss: 0.4474\n",
      "Epoch [7/10], Step [143/147], Loss: 0.4125\n",
      "Epoch [7/10], Step [144/147], Loss: 0.4798\n",
      "Epoch [7/10], Step [145/147], Loss: 0.4629\n",
      "Epoch [7/10], Step [146/147], Loss: 0.4709\n",
      "Epoch [7/10], Step [147/147], Loss: 0.5213\n",
      "Epoch [8/10], Step [1/147], Loss: 0.3724\n",
      "Epoch [8/10], Step [2/147], Loss: 0.4797\n",
      "Epoch [8/10], Step [3/147], Loss: 0.4706\n",
      "Epoch [8/10], Step [4/147], Loss: 0.5181\n",
      "Epoch [8/10], Step [5/147], Loss: 0.4091\n",
      "Epoch [8/10], Step [6/147], Loss: 0.4081\n",
      "Epoch [8/10], Step [7/147], Loss: 0.4324\n",
      "Epoch [8/10], Step [8/147], Loss: 0.5240\n",
      "Epoch [8/10], Step [9/147], Loss: 0.4893\n",
      "Epoch [8/10], Step [10/147], Loss: 0.4435\n",
      "Epoch [8/10], Step [11/147], Loss: 0.6013\n",
      "Epoch [8/10], Step [12/147], Loss: 0.5343\n",
      "Epoch [8/10], Step [13/147], Loss: 0.5501\n",
      "Epoch [8/10], Step [14/147], Loss: 0.5127\n",
      "Epoch [8/10], Step [15/147], Loss: 0.4075\n",
      "Epoch [8/10], Step [16/147], Loss: 0.4755\n",
      "Epoch [8/10], Step [17/147], Loss: 0.4899\n",
      "Epoch [8/10], Step [18/147], Loss: 0.5737\n",
      "Epoch [8/10], Step [19/147], Loss: 0.5177\n",
      "Epoch [8/10], Step [20/147], Loss: 0.5083\n",
      "Epoch [8/10], Step [21/147], Loss: 0.4563\n",
      "Epoch [8/10], Step [22/147], Loss: 0.5274\n",
      "Epoch [8/10], Step [23/147], Loss: 0.4047\n",
      "Epoch [8/10], Step [24/147], Loss: 0.5395\n",
      "Epoch [8/10], Step [25/147], Loss: 0.4947\n",
      "Epoch [8/10], Step [26/147], Loss: 0.4826\n",
      "Epoch [8/10], Step [27/147], Loss: 0.4770\n",
      "Epoch [8/10], Step [28/147], Loss: 0.5181\n",
      "Epoch [8/10], Step [29/147], Loss: 0.4760\n",
      "Epoch [8/10], Step [30/147], Loss: 0.4779\n",
      "Epoch [8/10], Step [31/147], Loss: 0.5550\n",
      "Epoch [8/10], Step [32/147], Loss: 0.4791\n",
      "Epoch [8/10], Step [33/147], Loss: 0.4935\n",
      "Epoch [8/10], Step [34/147], Loss: 0.4193\n",
      "Epoch [8/10], Step [35/147], Loss: 0.5312\n",
      "Epoch [8/10], Step [36/147], Loss: 0.4871\n",
      "Epoch [8/10], Step [37/147], Loss: 0.5034\n",
      "Epoch [8/10], Step [38/147], Loss: 0.5556\n",
      "Epoch [8/10], Step [39/147], Loss: 0.4489\n",
      "Epoch [8/10], Step [40/147], Loss: 0.4500\n",
      "Epoch [8/10], Step [41/147], Loss: 0.4676\n",
      "Epoch [8/10], Step [42/147], Loss: 0.3935\n",
      "Epoch [8/10], Step [43/147], Loss: 0.4621\n",
      "Epoch [8/10], Step [44/147], Loss: 0.4100\n",
      "Epoch [8/10], Step [45/147], Loss: 0.5256\n",
      "Epoch [8/10], Step [46/147], Loss: 0.4267\n",
      "Epoch [8/10], Step [47/147], Loss: 0.4801\n",
      "Epoch [8/10], Step [48/147], Loss: 0.5206\n",
      "Epoch [8/10], Step [49/147], Loss: 0.4382\n",
      "Epoch [8/10], Step [50/147], Loss: 0.4029\n",
      "Epoch [8/10], Step [51/147], Loss: 0.4729\n",
      "Epoch [8/10], Step [52/147], Loss: 0.4788\n",
      "Epoch [8/10], Step [53/147], Loss: 0.4402\n",
      "Epoch [8/10], Step [54/147], Loss: 0.4006\n",
      "Epoch [8/10], Step [55/147], Loss: 0.4364\n",
      "Epoch [8/10], Step [56/147], Loss: 0.4604\n",
      "Epoch [8/10], Step [57/147], Loss: 0.5468\n",
      "Epoch [8/10], Step [58/147], Loss: 0.4483\n",
      "Epoch [8/10], Step [59/147], Loss: 0.4447\n",
      "Epoch [8/10], Step [60/147], Loss: 0.5168\n",
      "Epoch [8/10], Step [61/147], Loss: 0.4884\n",
      "Epoch [8/10], Step [62/147], Loss: 0.4800\n",
      "Epoch [8/10], Step [63/147], Loss: 0.5338\n",
      "Epoch [8/10], Step [64/147], Loss: 0.4982\n",
      "Epoch [8/10], Step [65/147], Loss: 0.5377\n",
      "Epoch [8/10], Step [66/147], Loss: 0.5220\n",
      "Epoch [8/10], Step [67/147], Loss: 0.4806\n",
      "Epoch [8/10], Step [68/147], Loss: 0.4171\n",
      "Epoch [8/10], Step [69/147], Loss: 0.5062\n",
      "Epoch [8/10], Step [70/147], Loss: 0.4693\n",
      "Epoch [8/10], Step [71/147], Loss: 0.4115\n",
      "Epoch [8/10], Step [72/147], Loss: 0.4854\n",
      "Epoch [8/10], Step [73/147], Loss: 0.4058\n",
      "Epoch [8/10], Step [74/147], Loss: 0.4130\n",
      "Epoch [8/10], Step [75/147], Loss: 0.4463\n",
      "Epoch [8/10], Step [76/147], Loss: 0.4442\n",
      "Epoch [8/10], Step [77/147], Loss: 0.4158\n",
      "Epoch [8/10], Step [78/147], Loss: 0.4195\n",
      "Epoch [8/10], Step [79/147], Loss: 0.5062\n",
      "Epoch [8/10], Step [80/147], Loss: 0.4405\n",
      "Epoch [8/10], Step [81/147], Loss: 0.4409\n",
      "Epoch [8/10], Step [82/147], Loss: 0.4814\n",
      "Epoch [8/10], Step [83/147], Loss: 0.5156\n",
      "Epoch [8/10], Step [84/147], Loss: 0.4472\n",
      "Epoch [8/10], Step [85/147], Loss: 0.4104\n",
      "Epoch [8/10], Step [86/147], Loss: 0.4312\n",
      "Epoch [8/10], Step [87/147], Loss: 0.4172\n",
      "Epoch [8/10], Step [88/147], Loss: 0.4376\n",
      "Epoch [8/10], Step [89/147], Loss: 0.4866\n",
      "Epoch [8/10], Step [90/147], Loss: 0.4598\n",
      "Epoch [8/10], Step [91/147], Loss: 0.5040\n",
      "Epoch [8/10], Step [92/147], Loss: 0.4614\n",
      "Epoch [8/10], Step [93/147], Loss: 0.4285\n",
      "Epoch [8/10], Step [94/147], Loss: 0.4272\n",
      "Epoch [8/10], Step [95/147], Loss: 0.4035\n",
      "Epoch [8/10], Step [96/147], Loss: 0.4628\n",
      "Epoch [8/10], Step [97/147], Loss: 0.4700\n",
      "Epoch [8/10], Step [98/147], Loss: 0.3914\n",
      "Epoch [8/10], Step [99/147], Loss: 0.4978\n",
      "Epoch [8/10], Step [100/147], Loss: 0.3691\n",
      "Epoch [8/10], Step [101/147], Loss: 0.4278\n",
      "Epoch [8/10], Step [102/147], Loss: 0.4825\n",
      "Epoch [8/10], Step [103/147], Loss: 0.4218\n",
      "Epoch [8/10], Step [104/147], Loss: 0.3804\n",
      "Epoch [8/10], Step [105/147], Loss: 0.6057\n",
      "Epoch [8/10], Step [106/147], Loss: 0.4012\n",
      "Epoch [8/10], Step [107/147], Loss: 0.4007\n",
      "Epoch [8/10], Step [108/147], Loss: 0.4749\n",
      "Epoch [8/10], Step [109/147], Loss: 0.4654\n",
      "Epoch [8/10], Step [110/147], Loss: 0.4990\n",
      "Epoch [8/10], Step [111/147], Loss: 0.3786\n",
      "Epoch [8/10], Step [112/147], Loss: 0.5586\n",
      "Epoch [8/10], Step [113/147], Loss: 0.4198\n",
      "Epoch [8/10], Step [114/147], Loss: 0.3780\n",
      "Epoch [8/10], Step [115/147], Loss: 0.4625\n",
      "Epoch [8/10], Step [116/147], Loss: 0.5115\n",
      "Epoch [8/10], Step [117/147], Loss: 0.5875\n",
      "Epoch [8/10], Step [118/147], Loss: 0.4062\n",
      "Epoch [8/10], Step [119/147], Loss: 0.4633\n",
      "Epoch [8/10], Step [120/147], Loss: 0.4749\n",
      "Epoch [8/10], Step [121/147], Loss: 0.4180\n",
      "Epoch [8/10], Step [122/147], Loss: 0.4941\n",
      "Epoch [8/10], Step [123/147], Loss: 0.4559\n",
      "Epoch [8/10], Step [124/147], Loss: 0.3942\n",
      "Epoch [8/10], Step [125/147], Loss: 0.5697\n",
      "Epoch [8/10], Step [126/147], Loss: 0.4706\n",
      "Epoch [8/10], Step [127/147], Loss: 0.4515\n",
      "Epoch [8/10], Step [128/147], Loss: 0.4940\n",
      "Epoch [8/10], Step [129/147], Loss: 0.4162\n",
      "Epoch [8/10], Step [130/147], Loss: 0.3262\n",
      "Epoch [8/10], Step [131/147], Loss: 0.4121\n",
      "Epoch [8/10], Step [132/147], Loss: 0.4299\n",
      "Epoch [8/10], Step [133/147], Loss: 0.4605\n",
      "Epoch [8/10], Step [134/147], Loss: 0.4877\n",
      "Epoch [8/10], Step [135/147], Loss: 0.4711\n",
      "Epoch [8/10], Step [136/147], Loss: 0.4699\n",
      "Epoch [8/10], Step [137/147], Loss: 0.5045\n",
      "Epoch [8/10], Step [138/147], Loss: 0.4073\n",
      "Epoch [8/10], Step [139/147], Loss: 0.5193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [140/147], Loss: 0.4263\n",
      "Epoch [8/10], Step [141/147], Loss: 0.5218\n",
      "Epoch [8/10], Step [142/147], Loss: 0.5195\n",
      "Epoch [8/10], Step [143/147], Loss: 0.3929\n",
      "Epoch [8/10], Step [144/147], Loss: 0.4711\n",
      "Epoch [8/10], Step [145/147], Loss: 0.5642\n",
      "Epoch [8/10], Step [146/147], Loss: 0.4626\n",
      "Epoch [8/10], Step [147/147], Loss: 0.4746\n",
      "Epoch [9/10], Step [1/147], Loss: 0.4528\n",
      "Epoch [9/10], Step [2/147], Loss: 0.4489\n",
      "Epoch [9/10], Step [3/147], Loss: 0.4992\n",
      "Epoch [9/10], Step [4/147], Loss: 0.5097\n",
      "Epoch [9/10], Step [5/147], Loss: 0.4784\n",
      "Epoch [9/10], Step [6/147], Loss: 0.4873\n",
      "Epoch [9/10], Step [7/147], Loss: 0.4611\n",
      "Epoch [9/10], Step [8/147], Loss: 0.4109\n",
      "Epoch [9/10], Step [9/147], Loss: 0.4601\n",
      "Epoch [9/10], Step [10/147], Loss: 0.4748\n",
      "Epoch [9/10], Step [11/147], Loss: 0.4046\n",
      "Epoch [9/10], Step [12/147], Loss: 0.4887\n",
      "Epoch [9/10], Step [13/147], Loss: 0.3956\n",
      "Epoch [9/10], Step [14/147], Loss: 0.4542\n",
      "Epoch [9/10], Step [15/147], Loss: 0.4489\n",
      "Epoch [9/10], Step [16/147], Loss: 0.4366\n",
      "Epoch [9/10], Step [17/147], Loss: 0.5329\n",
      "Epoch [9/10], Step [18/147], Loss: 0.3753\n",
      "Epoch [9/10], Step [19/147], Loss: 0.5001\n",
      "Epoch [9/10], Step [20/147], Loss: 0.4563\n",
      "Epoch [9/10], Step [21/147], Loss: 0.4464\n",
      "Epoch [9/10], Step [22/147], Loss: 0.4734\n",
      "Epoch [9/10], Step [23/147], Loss: 0.4157\n",
      "Epoch [9/10], Step [24/147], Loss: 0.5087\n",
      "Epoch [9/10], Step [25/147], Loss: 0.4077\n",
      "Epoch [9/10], Step [26/147], Loss: 0.3651\n",
      "Epoch [9/10], Step [27/147], Loss: 0.4435\n",
      "Epoch [9/10], Step [28/147], Loss: 0.4474\n",
      "Epoch [9/10], Step [29/147], Loss: 0.4290\n",
      "Epoch [9/10], Step [30/147], Loss: 0.3990\n",
      "Epoch [9/10], Step [31/147], Loss: 0.4877\n",
      "Epoch [9/10], Step [32/147], Loss: 0.3049\n",
      "Epoch [9/10], Step [33/147], Loss: 0.5086\n",
      "Epoch [9/10], Step [34/147], Loss: 0.4185\n",
      "Epoch [9/10], Step [35/147], Loss: 0.4642\n",
      "Epoch [9/10], Step [36/147], Loss: 0.3984\n",
      "Epoch [9/10], Step [37/147], Loss: 0.5202\n",
      "Epoch [9/10], Step [38/147], Loss: 0.4993\n",
      "Epoch [9/10], Step [39/147], Loss: 0.4931\n",
      "Epoch [9/10], Step [40/147], Loss: 0.3542\n",
      "Epoch [9/10], Step [41/147], Loss: 0.5275\n",
      "Epoch [9/10], Step [42/147], Loss: 0.4080\n",
      "Epoch [9/10], Step [43/147], Loss: 0.4326\n",
      "Epoch [9/10], Step [44/147], Loss: 0.4589\n",
      "Epoch [9/10], Step [45/147], Loss: 0.4362\n",
      "Epoch [9/10], Step [46/147], Loss: 0.4100\n",
      "Epoch [9/10], Step [47/147], Loss: 0.4216\n",
      "Epoch [9/10], Step [48/147], Loss: 0.3192\n",
      "Epoch [9/10], Step [49/147], Loss: 0.4536\n",
      "Epoch [9/10], Step [50/147], Loss: 0.4785\n",
      "Epoch [9/10], Step [51/147], Loss: 0.4512\n",
      "Epoch [9/10], Step [52/147], Loss: 0.4945\n",
      "Epoch [9/10], Step [53/147], Loss: 0.4293\n",
      "Epoch [9/10], Step [54/147], Loss: 0.5472\n",
      "Epoch [9/10], Step [55/147], Loss: 0.4381\n",
      "Epoch [9/10], Step [56/147], Loss: 0.4423\n",
      "Epoch [9/10], Step [57/147], Loss: 0.4317\n",
      "Epoch [9/10], Step [58/147], Loss: 0.4438\n",
      "Epoch [9/10], Step [59/147], Loss: 0.4570\n",
      "Epoch [9/10], Step [60/147], Loss: 0.4467\n",
      "Epoch [9/10], Step [61/147], Loss: 0.4277\n",
      "Epoch [9/10], Step [62/147], Loss: 0.3717\n",
      "Epoch [9/10], Step [63/147], Loss: 0.4671\n",
      "Epoch [9/10], Step [64/147], Loss: 0.3660\n",
      "Epoch [9/10], Step [65/147], Loss: 0.4367\n",
      "Epoch [9/10], Step [66/147], Loss: 0.4296\n",
      "Epoch [9/10], Step [67/147], Loss: 0.3782\n",
      "Epoch [9/10], Step [68/147], Loss: 0.5132\n",
      "Epoch [9/10], Step [69/147], Loss: 0.5242\n",
      "Epoch [9/10], Step [70/147], Loss: 0.4611\n",
      "Epoch [9/10], Step [71/147], Loss: 0.4125\n",
      "Epoch [9/10], Step [72/147], Loss: 0.4507\n",
      "Epoch [9/10], Step [73/147], Loss: 0.4652\n",
      "Epoch [9/10], Step [74/147], Loss: 0.4715\n",
      "Epoch [9/10], Step [75/147], Loss: 0.4023\n",
      "Epoch [9/10], Step [76/147], Loss: 0.3434\n",
      "Epoch [9/10], Step [77/147], Loss: 0.3985\n",
      "Epoch [9/10], Step [78/147], Loss: 0.4771\n",
      "Epoch [9/10], Step [79/147], Loss: 0.4821\n",
      "Epoch [9/10], Step [80/147], Loss: 0.5016\n",
      "Epoch [9/10], Step [81/147], Loss: 0.4608\n",
      "Epoch [9/10], Step [82/147], Loss: 0.4798\n",
      "Epoch [9/10], Step [83/147], Loss: 0.4331\n",
      "Epoch [9/10], Step [84/147], Loss: 0.4231\n",
      "Epoch [9/10], Step [85/147], Loss: 0.4336\n",
      "Epoch [9/10], Step [86/147], Loss: 0.4765\n",
      "Epoch [9/10], Step [87/147], Loss: 0.4045\n",
      "Epoch [9/10], Step [88/147], Loss: 0.4695\n",
      "Epoch [9/10], Step [89/147], Loss: 0.5846\n",
      "Epoch [9/10], Step [90/147], Loss: 0.4311\n",
      "Epoch [9/10], Step [91/147], Loss: 0.3753\n",
      "Epoch [9/10], Step [92/147], Loss: 0.4319\n",
      "Epoch [9/10], Step [93/147], Loss: 0.4248\n",
      "Epoch [9/10], Step [94/147], Loss: 0.4230\n",
      "Epoch [9/10], Step [95/147], Loss: 0.3373\n",
      "Epoch [9/10], Step [96/147], Loss: 0.4175\n",
      "Epoch [9/10], Step [97/147], Loss: 0.4821\n",
      "Epoch [9/10], Step [98/147], Loss: 0.4349\n",
      "Epoch [9/10], Step [99/147], Loss: 0.4670\n",
      "Epoch [9/10], Step [100/147], Loss: 0.4512\n",
      "Epoch [9/10], Step [101/147], Loss: 0.4296\n",
      "Epoch [9/10], Step [102/147], Loss: 0.4139\n",
      "Epoch [9/10], Step [103/147], Loss: 0.4046\n",
      "Epoch [9/10], Step [104/147], Loss: 0.4354\n",
      "Epoch [9/10], Step [105/147], Loss: 0.4150\n",
      "Epoch [9/10], Step [106/147], Loss: 0.4786\n",
      "Epoch [9/10], Step [107/147], Loss: 0.4172\n",
      "Epoch [9/10], Step [108/147], Loss: 0.3882\n",
      "Epoch [9/10], Step [109/147], Loss: 0.4024\n",
      "Epoch [9/10], Step [110/147], Loss: 0.4219\n",
      "Epoch [9/10], Step [111/147], Loss: 0.3861\n",
      "Epoch [9/10], Step [112/147], Loss: 0.4126\n",
      "Epoch [9/10], Step [113/147], Loss: 0.3940\n",
      "Epoch [9/10], Step [114/147], Loss: 0.4308\n",
      "Epoch [9/10], Step [115/147], Loss: 0.4362\n",
      "Epoch [9/10], Step [116/147], Loss: 0.4238\n",
      "Epoch [9/10], Step [117/147], Loss: 0.5388\n",
      "Epoch [9/10], Step [118/147], Loss: 0.4290\n",
      "Epoch [9/10], Step [119/147], Loss: 0.4291\n",
      "Epoch [9/10], Step [120/147], Loss: 0.4913\n",
      "Epoch [9/10], Step [121/147], Loss: 0.4293\n",
      "Epoch [9/10], Step [122/147], Loss: 0.4884\n",
      "Epoch [9/10], Step [123/147], Loss: 0.4092\n",
      "Epoch [9/10], Step [124/147], Loss: 0.4025\n",
      "Epoch [9/10], Step [125/147], Loss: 0.4208\n",
      "Epoch [9/10], Step [126/147], Loss: 0.3957\n",
      "Epoch [9/10], Step [127/147], Loss: 0.4431\n",
      "Epoch [9/10], Step [128/147], Loss: 0.4708\n",
      "Epoch [9/10], Step [129/147], Loss: 0.3941\n",
      "Epoch [9/10], Step [130/147], Loss: 0.4426\n",
      "Epoch [9/10], Step [131/147], Loss: 0.5366\n",
      "Epoch [9/10], Step [132/147], Loss: 0.3745\n",
      "Epoch [9/10], Step [133/147], Loss: 0.4776\n",
      "Epoch [9/10], Step [134/147], Loss: 0.5099\n",
      "Epoch [9/10], Step [135/147], Loss: 0.3845\n",
      "Epoch [9/10], Step [136/147], Loss: 0.4160\n",
      "Epoch [9/10], Step [137/147], Loss: 0.4352\n",
      "Epoch [9/10], Step [138/147], Loss: 0.4024\n",
      "Epoch [9/10], Step [139/147], Loss: 0.4428\n",
      "Epoch [9/10], Step [140/147], Loss: 0.4149\n",
      "Epoch [9/10], Step [141/147], Loss: 0.4408\n",
      "Epoch [9/10], Step [142/147], Loss: 0.3814\n",
      "Epoch [9/10], Step [143/147], Loss: 0.5107\n",
      "Epoch [9/10], Step [144/147], Loss: 0.3685\n",
      "Epoch [9/10], Step [145/147], Loss: 0.4110\n",
      "Epoch [9/10], Step [146/147], Loss: 0.4355\n",
      "Epoch [9/10], Step [147/147], Loss: 0.4678\n",
      "Epoch [10/10], Step [1/147], Loss: 0.5730\n",
      "Epoch [10/10], Step [2/147], Loss: 0.4688\n",
      "Epoch [10/10], Step [3/147], Loss: 0.4213\n",
      "Epoch [10/10], Step [4/147], Loss: 0.4371\n",
      "Epoch [10/10], Step [5/147], Loss: 0.4979\n",
      "Epoch [10/10], Step [6/147], Loss: 0.3876\n",
      "Epoch [10/10], Step [7/147], Loss: 0.4461\n",
      "Epoch [10/10], Step [8/147], Loss: 0.5102\n",
      "Epoch [10/10], Step [9/147], Loss: 0.4771\n",
      "Epoch [10/10], Step [10/147], Loss: 0.3238\n",
      "Epoch [10/10], Step [11/147], Loss: 0.4950\n",
      "Epoch [10/10], Step [12/147], Loss: 0.4070\n",
      "Epoch [10/10], Step [13/147], Loss: 0.5062\n",
      "Epoch [10/10], Step [14/147], Loss: 0.4168\n",
      "Epoch [10/10], Step [15/147], Loss: 0.3871\n",
      "Epoch [10/10], Step [16/147], Loss: 0.3584\n",
      "Epoch [10/10], Step [17/147], Loss: 0.4511\n",
      "Epoch [10/10], Step [18/147], Loss: 0.5200\n",
      "Epoch [10/10], Step [19/147], Loss: 0.4686\n",
      "Epoch [10/10], Step [20/147], Loss: 0.3849\n",
      "Epoch [10/10], Step [21/147], Loss: 0.3989\n",
      "Epoch [10/10], Step [22/147], Loss: 0.4111\n",
      "Epoch [10/10], Step [23/147], Loss: 0.4357\n",
      "Epoch [10/10], Step [24/147], Loss: 0.3845\n",
      "Epoch [10/10], Step [25/147], Loss: 0.4429\n",
      "Epoch [10/10], Step [26/147], Loss: 0.4361\n",
      "Epoch [10/10], Step [27/147], Loss: 0.4031\n",
      "Epoch [10/10], Step [28/147], Loss: 0.4263\n",
      "Epoch [10/10], Step [29/147], Loss: 0.3658\n",
      "Epoch [10/10], Step [30/147], Loss: 0.4544\n",
      "Epoch [10/10], Step [31/147], Loss: 0.5342\n",
      "Epoch [10/10], Step [32/147], Loss: 0.4162\n",
      "Epoch [10/10], Step [33/147], Loss: 0.3765\n",
      "Epoch [10/10], Step [34/147], Loss: 0.3434\n",
      "Epoch [10/10], Step [35/147], Loss: 0.3520\n",
      "Epoch [10/10], Step [36/147], Loss: 0.4604\n",
      "Epoch [10/10], Step [37/147], Loss: 0.4700\n",
      "Epoch [10/10], Step [38/147], Loss: 0.3596\n",
      "Epoch [10/10], Step [39/147], Loss: 0.3858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [40/147], Loss: 0.3903\n",
      "Epoch [10/10], Step [41/147], Loss: 0.4243\n",
      "Epoch [10/10], Step [42/147], Loss: 0.3611\n",
      "Epoch [10/10], Step [43/147], Loss: 0.3937\n",
      "Epoch [10/10], Step [44/147], Loss: 0.4596\n",
      "Epoch [10/10], Step [45/147], Loss: 0.5074\n",
      "Epoch [10/10], Step [46/147], Loss: 0.5016\n",
      "Epoch [10/10], Step [47/147], Loss: 0.4356\n",
      "Epoch [10/10], Step [48/147], Loss: 0.3956\n",
      "Epoch [10/10], Step [49/147], Loss: 0.5199\n",
      "Epoch [10/10], Step [50/147], Loss: 0.4524\n",
      "Epoch [10/10], Step [51/147], Loss: 0.3890\n",
      "Epoch [10/10], Step [52/147], Loss: 0.4338\n",
      "Epoch [10/10], Step [53/147], Loss: 0.4400\n",
      "Epoch [10/10], Step [54/147], Loss: 0.4080\n",
      "Epoch [10/10], Step [55/147], Loss: 0.3829\n",
      "Epoch [10/10], Step [56/147], Loss: 0.3927\n",
      "Epoch [10/10], Step [57/147], Loss: 0.3674\n",
      "Epoch [10/10], Step [58/147], Loss: 0.4268\n",
      "Epoch [10/10], Step [59/147], Loss: 0.3717\n",
      "Epoch [10/10], Step [60/147], Loss: 0.4640\n",
      "Epoch [10/10], Step [61/147], Loss: 0.3991\n",
      "Epoch [10/10], Step [62/147], Loss: 0.4137\n",
      "Epoch [10/10], Step [63/147], Loss: 0.3417\n",
      "Epoch [10/10], Step [64/147], Loss: 0.4686\n",
      "Epoch [10/10], Step [65/147], Loss: 0.4709\n",
      "Epoch [10/10], Step [66/147], Loss: 0.4502\n",
      "Epoch [10/10], Step [67/147], Loss: 0.4655\n",
      "Epoch [10/10], Step [68/147], Loss: 0.3531\n",
      "Epoch [10/10], Step [69/147], Loss: 0.5040\n",
      "Epoch [10/10], Step [70/147], Loss: 0.4687\n",
      "Epoch [10/10], Step [71/147], Loss: 0.4753\n",
      "Epoch [10/10], Step [72/147], Loss: 0.4418\n",
      "Epoch [10/10], Step [73/147], Loss: 0.3835\n",
      "Epoch [10/10], Step [74/147], Loss: 0.3411\n",
      "Epoch [10/10], Step [75/147], Loss: 0.3550\n",
      "Epoch [10/10], Step [76/147], Loss: 0.4537\n",
      "Epoch [10/10], Step [77/147], Loss: 0.4197\n",
      "Epoch [10/10], Step [78/147], Loss: 0.3248\n",
      "Epoch [10/10], Step [79/147], Loss: 0.3610\n",
      "Epoch [10/10], Step [80/147], Loss: 0.4036\n",
      "Epoch [10/10], Step [81/147], Loss: 0.3748\n",
      "Epoch [10/10], Step [82/147], Loss: 0.4034\n",
      "Epoch [10/10], Step [83/147], Loss: 0.3815\n",
      "Epoch [10/10], Step [84/147], Loss: 0.4595\n",
      "Epoch [10/10], Step [85/147], Loss: 0.4253\n",
      "Epoch [10/10], Step [86/147], Loss: 0.4462\n",
      "Epoch [10/10], Step [87/147], Loss: 0.4096\n",
      "Epoch [10/10], Step [88/147], Loss: 0.3819\n",
      "Epoch [10/10], Step [89/147], Loss: 0.4522\n",
      "Epoch [10/10], Step [90/147], Loss: 0.4247\n",
      "Epoch [10/10], Step [91/147], Loss: 0.2982\n",
      "Epoch [10/10], Step [92/147], Loss: 0.4200\n",
      "Epoch [10/10], Step [93/147], Loss: 0.4417\n",
      "Epoch [10/10], Step [94/147], Loss: 0.3784\n",
      "Epoch [10/10], Step [95/147], Loss: 0.3396\n",
      "Epoch [10/10], Step [96/147], Loss: 0.4642\n",
      "Epoch [10/10], Step [97/147], Loss: 0.4054\n",
      "Epoch [10/10], Step [98/147], Loss: 0.4812\n",
      "Epoch [10/10], Step [99/147], Loss: 0.3783\n",
      "Epoch [10/10], Step [100/147], Loss: 0.4028\n",
      "Epoch [10/10], Step [101/147], Loss: 0.3860\n",
      "Epoch [10/10], Step [102/147], Loss: 0.3211\n",
      "Epoch [10/10], Step [103/147], Loss: 0.5233\n",
      "Epoch [10/10], Step [104/147], Loss: 0.3395\n",
      "Epoch [10/10], Step [105/147], Loss: 0.4891\n",
      "Epoch [10/10], Step [106/147], Loss: 0.3274\n",
      "Epoch [10/10], Step [107/147], Loss: 0.5251\n",
      "Epoch [10/10], Step [108/147], Loss: 0.4625\n",
      "Epoch [10/10], Step [109/147], Loss: 0.3748\n",
      "Epoch [10/10], Step [110/147], Loss: 0.3906\n",
      "Epoch [10/10], Step [111/147], Loss: 0.3636\n",
      "Epoch [10/10], Step [112/147], Loss: 0.4047\n",
      "Epoch [10/10], Step [113/147], Loss: 0.3654\n",
      "Epoch [10/10], Step [114/147], Loss: 0.3616\n",
      "Epoch [10/10], Step [115/147], Loss: 0.4635\n",
      "Epoch [10/10], Step [116/147], Loss: 0.4738\n",
      "Epoch [10/10], Step [117/147], Loss: 0.4680\n",
      "Epoch [10/10], Step [118/147], Loss: 0.4105\n",
      "Epoch [10/10], Step [119/147], Loss: 0.4207\n",
      "Epoch [10/10], Step [120/147], Loss: 0.4106\n",
      "Epoch [10/10], Step [121/147], Loss: 0.4854\n",
      "Epoch [10/10], Step [122/147], Loss: 0.4802\n",
      "Epoch [10/10], Step [123/147], Loss: 0.4295\n",
      "Epoch [10/10], Step [124/147], Loss: 0.5141\n",
      "Epoch [10/10], Step [125/147], Loss: 0.2763\n",
      "Epoch [10/10], Step [126/147], Loss: 0.3699\n",
      "Epoch [10/10], Step [127/147], Loss: 0.3965\n",
      "Epoch [10/10], Step [128/147], Loss: 0.4046\n",
      "Epoch [10/10], Step [129/147], Loss: 0.4493\n",
      "Epoch [10/10], Step [130/147], Loss: 0.3951\n",
      "Epoch [10/10], Step [131/147], Loss: 0.3671\n",
      "Epoch [10/10], Step [132/147], Loss: 0.3821\n",
      "Epoch [10/10], Step [133/147], Loss: 0.3988\n",
      "Epoch [10/10], Step [134/147], Loss: 0.4774\n",
      "Epoch [10/10], Step [135/147], Loss: 0.4682\n",
      "Epoch [10/10], Step [136/147], Loss: 0.4225\n",
      "Epoch [10/10], Step [137/147], Loss: 0.4347\n",
      "Epoch [10/10], Step [138/147], Loss: 0.3943\n",
      "Epoch [10/10], Step [139/147], Loss: 0.3388\n",
      "Epoch [10/10], Step [140/147], Loss: 0.4220\n",
      "Epoch [10/10], Step [141/147], Loss: 0.4387\n",
      "Epoch [10/10], Step [142/147], Loss: 0.4680\n",
      "Epoch [10/10], Step [143/147], Loss: 0.4581\n",
      "Epoch [10/10], Step [144/147], Loss: 0.4341\n",
      "Epoch [10/10], Step [145/147], Loss: 0.3655\n",
      "Epoch [10/10], Step [146/147], Loss: 0.4300\n",
      "Epoch [10/10], Step [147/147], Loss: 0.5162\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_ft(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer_ft.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        if (i + 1) % 1== 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 87.93363499245852 %\n"
     ]
    }
   ],
   "source": [
    "# Test the modely_test=labels_all\n",
    "\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_all=[]\n",
    "    labels_all=[]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_ft(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels_all=labels_all+labels.cpu().numpy().tolist()\n",
    "        predicted_all=predicted_all+predicted.cpu().numpy().tolist()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tfs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "class cal_cam(nn.Module):\n",
    "    def __init__(self, feature_layer=\"layer4\"):\n",
    "        super(cal_cam, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=False)\n",
    "        self.model.load_state_dict(torch.load(\"C:/Users/DELL/Desktop/class9 transfer learning example2/class9 transfer learning example/resnet18-5c106cde.pth\"))\n",
    "#自己的resnet18的地址\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    " \n",
    "        # 要求梯度的层\n",
    "        self.feature_layer = feature_layer\n",
    "        # 记录梯度\n",
    "        self.gradient = []\n",
    "        # 记录输出的特征图\n",
    "        self.output = []\n",
    "        self.means = [0.485, 0.456, 0.406]\n",
    "        self.stds = [0.229, 0.224, 0.225]\n",
    " \n",
    "        self.trainsform = tfs.Compose([\n",
    "            tfs.ToTensor(),\n",
    "            tfs.Normalize(self.means, self.stds)\n",
    "        ])\n",
    " \n",
    "    def save_grad(self, grad):\n",
    "        self.gradient.append(grad)\n",
    " \n",
    "    def get_grad(self):\n",
    "        return self.gradient[-1].cpu().data\n",
    " \n",
    "    def get_feature(self):\n",
    "        return self.output[-1][0]\n",
    " \n",
    " \n",
    "    def process_img(self, input):\n",
    "        input = self.trainsform(input)\n",
    "        input = input.unsqueeze(0)\n",
    "        return input\n",
    " \n",
    "    # 计算最后一个卷积层的梯度，输出梯度和最后一个卷积层的特征图\n",
    "    def getGrad(self, input_):\n",
    "        input_ = input_.to(self.device).requires_grad_(True)\n",
    "        num = 1\n",
    "        for name, module in self.model._modules.items():\n",
    "            if(num == 1):\n",
    "                input = module(input_)\n",
    "                num = num + 1\n",
    "                continue\n",
    "            # 是待提取特征图的层\n",
    "            if (name == self.feature_layer):\n",
    "                input = module(input)\n",
    "                input.register_hook(self.save_grad)\n",
    "                self.output.append([input])\n",
    "            # 马上要到全连接层了\n",
    "            elif (name == \"avgpool\"):\n",
    "                input = module(input)\n",
    "                input = input.reshape(input.shape[0], -1)\n",
    "            # 普通的层\n",
    "            else:\n",
    "                input = module(input)\n",
    " \n",
    "        # 到这里input就是最后全连接层的输出了\n",
    "        index = torch.max(input, dim=-1)[1]\n",
    "        one_hot = torch.zeros((1, input.shape[-1]), dtype=torch.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        confidenct = one_hot * input.cpu()\n",
    "        confidenct = torch.sum(confidenct, dim=-1).requires_grad_(True)\n",
    "        # print(confidenct)\n",
    "        self.model.zero_grad()\n",
    "        # 反向传播获取梯度\n",
    "        confidenct.backward(retain_graph=True)\n",
    "        # 获取特征图的梯度\n",
    "        grad_val = self.get_grad()\n",
    "        feature = self.get_feature()\n",
    "        return grad_val, feature, input_.grad\n",
    " \n",
    "    # 计算CAM\n",
    "    def getCam(self, grad_val, feature):\n",
    "        # 对特征图的每个通道进行全局池化\n",
    "        alpha = torch.mean(grad_val, dim=(2,3)).cpu()\n",
    "        feature = feature.cpu()\n",
    "        # 将池化后的结果和相应通道特征图相乘\n",
    "        cam = torch.zeros((feature.shape[2],feature.shape[3]), dtype=torch.float32)\n",
    "        for idx in range(alpha.shape[1]):\n",
    "            cam = cam + alpha[0][idx]*feature[0][idx]\n",
    "        # 进行ReLU操作\n",
    "        cam = np.maximum(cam.detach().numpy(), 0)\n",
    " \n",
    " \n",
    " \n",
    "        plt.imshow(cam)\n",
    "        plt.colorbar()\n",
    "        plt.savefig(\"./cam.jpg\")\n",
    " \n",
    "        # 将cam区域放大到输入图片大小\n",
    "        cam_ = cv2.resize(cam, (224,224))\n",
    "        cam_ = cam_ - np.min(cam_)\n",
    "        cam_ = cam_ / np.max(cam_)\n",
    "        plt.imshow(cam_)\n",
    "        plt.savefig(\"./cam_.jpg\")\n",
    "        cam = torch.from_numpy(cam)\n",
    " \n",
    " \n",
    " \n",
    "        return cam, cam_\n",
    " \n",
    "    def show_img(self, cam_, img):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam_), cv2.COLORMAP_JET)\n",
    "        cam_img = 0.3 * heatmap + 0.7 * np.float32(img)\n",
    "        cv2.imwrite(\"img.jpg\", cam_img)\n",
    " \n",
    "    def __call__(self, img_root):\n",
    "        img = Image.open(img_root)\n",
    "        img = img.resize((224,224))\n",
    "        plt.imshow(img)\n",
    "        plt.savefig(\"4.jpg\")\n",
    "        #要热力图的图片\n",
    "        input = self.process_img(img)\n",
    "        grad_val, feature, input_grad = self.getGrad(input)\n",
    "        cam, cam_ = self.getCam(grad_val, feature)\n",
    "        self.show_img(cam_, img)\n",
    "        return cam\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    cam = cal_cam()\n",
    "    #图片地址\n",
    "    img_root = r'C:\\Users\\DELL\\Desktop\\class9 transfer learning example2\\class9 transfer learning example\\data\\aaa\\train\\buildings\\4.jpg'\n",
    "    cam(img_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 train_loss:  0.008 train_accuracy:  0.819 test_loss:  0.004 test_accuracy:  0.901\n",
      "epoch:  1 train_loss:  0.004 train_accuracy:  0.899 test_loss:  0.004 test_accuracy:  0.911\n",
      "epoch:  2 train_loss:  0.004 train_accuracy:  0.904 test_loss:  0.004 test_accuracy:  0.915\n",
      "epoch:  3 train_loss:  0.004 train_accuracy:  0.91 test_loss:  0.004 test_accuracy:  0.912\n",
      "epoch:  4 train_loss:  0.004 train_accuracy:  0.909 test_loss:  0.003 test_accuracy:  0.917\n",
      "1722.7123746871948\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "base_dir = r'C:\\Users\\DELL\\Desktop\\class9 transfer learning example2\\class9 transfer learning example\\data\\aaa'\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "\n",
    "tran_transformation = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    #随机把图片裁剪到192*192像素\n",
    "    transforms.RandomCrop(192),\n",
    "    #随机水平翻转 默认参数0.5\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #图片随机旋转一定角度\n",
    "    transforms.RandomRotation(0.2),\n",
    "    #改变图片亮度、对比度、饱和度、色调\n",
    "    transforms.ColorJitter(brightness=0.5),\n",
    "    transforms.ColorJitter(contrast=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "test_transformation = transforms.Compose([\n",
    "    transforms.Resize((192,192)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "train_ds = torchvision.datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transform=tran_transformation\n",
    ")\n",
    "test_ds = torchvision.datasets.ImageFolder(\n",
    "    test_dir,\n",
    "    transform=test_transformation\n",
    ")\n",
    "\n",
    "BATCHSIZE=64\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCHSIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCHSIZE,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "#print(model)\n",
    "\n",
    "#将卷积部分冻结\n",
    "for param in model.parameters():\n",
    "    #将卷积层部分的参数冻结\n",
    "    param.requires_grad = False\n",
    "\n",
    "#model.fc.会返回仅有一个的线性层\n",
    "#继承原有线性层的输入特征数\n",
    "in_f = model.fc.in_features\n",
    "#自己构造线性层并替换原来线性层\n",
    "model.fc = nn.Linear(in_f,4)\n",
    "#只需要优化最后的Linear层参数\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(),lr=0.001)\n",
    "#对学习速率进行衰减\n",
    "exp_lr_schedule = lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.1)\n",
    "epochs = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "\n",
    "def fit(epoch,model,trainloader,testloder):\n",
    "    #下面三个是个数不是概率\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    #Dropput在训练的时候回随机丢弃神经元(的输出),但预测的时候不会\n",
    "    #model.train()是训练模式,想让Dropout发挥作用,对BN层也有用\n",
    "    model.train()\n",
    "    for x,y in trainloader:\n",
    "        #将训练数据也放到GPU上\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # 梯度置为0\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播求解梯度\n",
    "        loss.backward()\n",
    "        # 优化\n",
    "        optimizer.step()\n",
    "        # 不需要进行梯度计算\n",
    "        with torch.no_grad():\n",
    "            #torch.argmax将数字转换成真正的预测结果\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            #计算个数\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    #一旦exp_lr_schedule.step()运行了5次就调整学习速率\n",
    "    exp_lr_schedule.step()\n",
    "    #除以的是总样本数 trainloader.dataset是形参,实参是train_dl即train_dl.dataset\n",
    "    #train_dl.dataset指向的是train_ds\n",
    "    epoch_loss = running_loss/len(trainloader.dataset)\n",
    "    epoch_acc = correct/total\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_running_loss = 0\n",
    "\n",
    "    #model.eval()是预测模式,不让Dropout发挥作用,对BN层也有用\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in testloder:\n",
    "            #将测试数据也要放到GPU上\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred,y)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            test_correct += (y_pred == y).sum().item()\n",
    "            test_total += y.size(0)\n",
    "            test_running_loss += loss.item()\n",
    "\n",
    "    epoch_test_loss = test_running_loss / len(test_dl.dataset)\n",
    "    epoch_test_acc = test_correct / test_total\n",
    "\n",
    "    print('epoch: ',epoch,\n",
    "          'train_loss: ',round(epoch_loss,3),\n",
    "          'train_accuracy: ',round(epoch_acc,3),\n",
    "          'test_loss: ',round(epoch_test_loss,3),\n",
    "          'test_accuracy: ',round(epoch_test_acc,3)\n",
    "              )\n",
    "\n",
    "    return epoch_loss,epoch_acc,epoch_test_loss,epoch_test_acc\n",
    "\n",
    "\n",
    "#便于随着训练的进行观察数值的变化\n",
    "train_loss=[]\n",
    "train_acc=[]\n",
    "test_loss=[]\n",
    "test_acc=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss,epoch_acc,epoch_test_loss,epoch_test_acc = fit(epoch,model,train_dl,test_dl)\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "    test_loss.append(epoch_test_loss)\n",
    "    test_acc.append(epoch_test_acc)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(1,epochs+1),train_loss,label='train_loss')\n",
    "plt.plot(range(1,epochs+1),test_loss,label='test_loss')\n",
    "plt.plot(range(1,epochs+1),train_acc,label='train_acc')\n",
    "plt.plot(range(1,epochs+1),test_acc,label='test_acc')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train of images 9378\n",
      "Total valid of images 1989\n",
      "训练开始 on cpu\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.model_selection import KFold\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "path = 'data/aaa/train/'\n",
    "files = glob(os.path.join(path, '*/*.jpg'))\n",
    "print(f'Total train of images {len(files)}')\n",
    "path = 'data/aaa/test/'\n",
    "files = glob(os.path.join(path, '*/*.jpg'))\n",
    "print(f'Total valid of images {len(files)}')\n",
    "\n",
    "# imag_size = 224\n",
    "# batch_size = 16\n",
    "imag_size = 200\n",
    "batch_size = 64\n",
    "# 数据增强\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((imag_size, imag_size)),\n",
    "    transforms.RandomHorizontalFlip(),# 随机水平翻转 \n",
    "    transforms.RandomVerticalFlip(), # 随机竖直翻转    \n",
    "    transforms.RandomRotation(45), # 随机角度旋转 \n",
    "    transforms.RandomCrop((imag_size, imag_size)), # 随机位置裁取\n",
    "    transforms.ToTensor(),\n",
    "    # ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((imag_size, imag_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_imgs = ImageFolder('data/aaa/train', transform)\n",
    "test_imgs = ImageFolder('data/aaa/test', transform_test)\n",
    "\n",
    "\n",
    "def kfold(data, k=5): \n",
    "    \"\"\" K折交叉验证 \"\"\"\n",
    "    X = np.arange(len(data))\n",
    "    KF = KFold(n_splits=k,shuffle=True)\n",
    "    for train_idxs, valid_idxs in KF.split(X):\n",
    "        train_iter, valid_iter = [] , []\n",
    "        for i in train_idxs:\n",
    "            train_iter.append(data[i])\n",
    "        for i in valid_idxs:\n",
    "            valid_iter.append(data[i]) \n",
    "        train_data = torch.utils.data.DataLoader(train_iter, shuffle=True, \n",
    "                                                 batch_size = batch_size)\n",
    "        valid_data = torch.utils.data.DataLoader(valid_iter, batch_size = batch_size)\n",
    "        yield train_data, valid_data\n",
    "\n",
    "def train(data, isTrain=True):\n",
    "\n",
    "    if isTrain:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in data:\n",
    "\n",
    "        if isTrain:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        inputs, labels = Variable(\n",
    "            inputs.to(device)), Variable(labels.to(device))\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if isTrain:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.data\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    loss = running_loss / len(data) / batch_size\n",
    "    acc = running_corrects / len(data) / batch_size\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "def test(data):\n",
    "    real_lables,pred_lables = [],[]\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in data:\n",
    "        inputs, labels = Variable(\n",
    "            inputs.to(device)), Variable(labels.to(device))\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        for y in labels:\n",
    "            real_lables.append(y.item())\n",
    "        for y in preds:\n",
    "            pred_lables.append(y.item())\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    acc = running_corrects / len(data) / batch_size\n",
    "    return acc, running_corrects, real_lables, pred_lables\n",
    "\n",
    "# 设置超参数\n",
    "train_iterations, train_loss, test_accuracy =[], [], []\n",
    "model = models.vgg16(pretrained=False)\n",
    "# model_ft.load_state_dict(torch.load('data/resnet50-19c8e357.pth'))\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 8)\n",
    "\n",
    "k = 5\n",
    "lr, num_epochs = 2e-4, 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "device = try_gpu(0)\n",
    "\n",
    "# 开始训练\n",
    "print('训练开始 on', device)\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    loss, acc = 0.0, 0.0\n",
    "    for train_data, valid_data in kfold(train_imgs, k):  # k折交叉验证\n",
    "        train_losses, train_acc = train(train_data)\n",
    "        valid_losses, valid_acc = train(valid_data, False)\n",
    "        loss += valid_losses\n",
    "        acc += valid_acc\n",
    "    if epoch % 1 == 0:  # 每隔10次输出一次结果\n",
    "        train_iterations.append(epoch)\n",
    "        train_loss.append((loss/k).to('cpu').item())\n",
    "        test_accuracy.append((acc/k).to('cpu').item())\n",
    "        print('{},{:.4f},{:.4f}'.format(epoch, loss/k, acc/k))\n",
    "#         print('Epoch {}/{}  avgLoss: {:.4f}  Acc: {:.4f}'.format(epoch + 1, num_epochs, loss/k, acc/k))\n",
    "train_iterations.append(num_epochs - 1)\n",
    "train_loss.append((loss/k).to('cpu').item())\n",
    "test_accuracy.append((acc/k).to('cpu').item())\n",
    "print('{},{:.4f},{:.4f}'.format(num_epochs - 1, loss/k, acc/k))\n",
    "print('训练结束')\n",
    "torch.save(model, 'C:/Users/DELL/Desktop/class9 transfer learning example2/class9 transfer learning example/vgg16-397923af.pth')\n",
    "# self.model.load_state_dict(torch.load(\"C:/Users/DELL/Desktop/class9 transfer learning example2/class9 transfer learning example/resnet18-5c106cde)\n",
    "\n",
    "print('测试 on', device)\n",
    "model.to(device)\n",
    "test_data = torch.utils.data.DataLoader(test_imgs, shuffle=True, batch_size=batch_size)\n",
    "acc, corrects, real, pre = test(test_data)\n",
    "print('准确率: {:.4f}  正确预测个数: {}'.format(acc, corrects))\n",
    "\n",
    "# k = 0\n",
    "# while k < len(real_lables):\n",
    "#     print(real_lables[k], pred_lables[k])\n",
    "#     k = k + 1\n",
    "\n",
    "\n",
    "# 绘制曲线图\n",
    "host = host_subplot(111)\n",
    "plt.subplots_adjust(right=0.8) # ajust the right boundary of the plot window\n",
    "par1 = host.twinx()\n",
    "\n",
    "# 设置类标\n",
    "host.set_xlabel(\"iterations\")\n",
    "host.set_ylabel(\"loss\")\n",
    "par1.set_ylabel(\"validation accuracy\")\n",
    "\n",
    "# 绘制曲线\n",
    "p1, = host.plot(train_iterations, train_loss, \"b-\", label=\"training loss\")\n",
    "p2, = host.plot(train_iterations, train_loss, \".\") #曲线点\n",
    "p3, = par1.plot(train_iterations, test_accuracy, label=\"validation accuracy\")\n",
    "p4, = par1.plot(train_iterations, test_accuracy, \"1\")\n",
    "\n",
    "# 设置图标\n",
    "# 1->rightup corner, 2->leftup corner, 3->leftdown corner\n",
    "# 4->rightdown corner, 5->rightmid ...\n",
    "host.legend(loc=5)\n",
    "\n",
    "# 设置颜色\n",
    "host.axis[\"left\"].label.set_color(p1.get_color())\n",
    "par1.axis[\"right\"].label.set_color(p3.get_color())\n",
    "\n",
    "# 设置范围\n",
    "host.set_xlim([0, num_epochs - 1])\n",
    "\n",
    "plt.draw()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# 第一部分 计算准确率 召回率 F值\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# 计算各类结果 共10类图片\n",
    "# real_8 = list(range(0, 8))   #真实10个类标数量的统计\n",
    "real_8 = list(range(0, 2))\n",
    "pre_8 = list(range(0, 2))    #预测10个类标数量的统计\n",
    "right_8 = list(range(0, 2))  #预测正确的10个类标数量\n",
    "\n",
    "k = 0\n",
    "while k < len(real):\n",
    "    v1 = int(real[k])\n",
    "    v2 = int(pre[k])\n",
    "#     print(v1, v2)\n",
    "    real_8[v1] = real_8[v1] + 1     # 计数\n",
    "    pre_8[v2] = pre_8[v2] + 1       # 计数\n",
    "    if v1==v2:\n",
    "        right_8[v1] = right_8[v1] + 1\n",
    "    k = k + 1\n",
    "# print(\"统计各类数量\")\n",
    "# print(real_10, pre_10, right_10)\n",
    "\n",
    "# 准确率 = 正确数 / 预测数\n",
    "precision = list(range(0, 2))   #8\n",
    "k = 0\n",
    "while k < len(real_8):\n",
    "    value = right_8[k] * 1.0 / pre_8[k] \n",
    "    precision[k] = value\n",
    "    k = k + 1\n",
    "print('准确率: ')\n",
    "print(precision)\n",
    "\n",
    "# 召回率 = 正确数 / 真实数\n",
    "recall = list(range(0, 2))\n",
    "k = 0\n",
    "while k < len(real_8):\n",
    "    value = right_8[k] * 1.0 / real_8[k] \n",
    "    recall[k] = value\n",
    "    k = k + 1\n",
    "print('召回率: ')\n",
    "print(recall)\n",
    "   \n",
    "# F值 = 2*准确率*召回率/(准确率+召回率)\n",
    "f_measure = list(range(0, 2))\n",
    "k = 0\n",
    "while k < len(real_8):\n",
    "    value = (2 * precision[k] * recall[k] * 1.0) / (precision[k] + recall[k])\n",
    "    f_measure[k] = value\n",
    "    k = k + 1\n",
    "print('F值: ')\n",
    "print(f_measure)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# 第二部分 绘制曲线\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# 设置类别\n",
    "n_groups = 2    #8\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.2\n",
    " \n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# 用来正常显示中文标签\n",
    "# plt.rcParams['font.sans-serif']=['SimHei']\n",
    "# 绘制\n",
    "rects1 = ax.bar(index, precision, bar_width,\n",
    "                alpha=opacity, color='b',\n",
    "                error_kw=error_config,\n",
    "                label='precision')\n",
    " \n",
    "rects2 = ax.bar(index + bar_width, recall, bar_width,\n",
    "                alpha=opacity, color='m',\n",
    "                error_kw=error_config,\n",
    "                label='recall')\n",
    " \n",
    "rects3 = ax.bar(index + bar_width + bar_width, f_measure, bar_width,\n",
    "                alpha=opacity, color='r',\n",
    "                error_kw=error_config,\n",
    "                label='f_measure')\n",
    "        \n",
    "# 设置标签\n",
    "ax.set_xticks(index + 3 * bar_width / 3)\n",
    "ax.set_xticklabels(('0-desk', '1-dining table', '2-double bed', '3-sofa', '4-squatting toilet',\n",
    "                    '5-TV cabinet', '6-wardrobe', '7-washbasin'))\n",
    "# 设置类标\n",
    "ax.legend()\n",
    "plt.xlabel(\"lable\")\n",
    "plt.ylabel(\"evaluation\")\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "fig.tight_layout()\n",
    "# plt.savefig('result.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
